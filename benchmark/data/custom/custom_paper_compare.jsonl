{"context": "The first paper begins. \\begin{abstract}\n\nThe core of out-of-distribution (OOD) detection is to learn the in-distribution (ID) representation, which is distinguishable from OOD samples. Previous work applied recognition-based methods to learn the ID features, which tend to learn shortcuts instead of comprehensive representations. In this work, we find surprisingly that simply using reconstruction-based methods could boost the performance of OOD detection significantly. We deeply explore the main contributors of OOD detection and find that reconstruction-based pretext tasks have the potential to provide a generally applicable and efficacious prior, which benefits the model in learning intrinsic data distributions of the ID dataset. Specifically, we take Masked Image Modeling as a pretext task for our OOD detection framework (MOOD). Without bells and whistles, MOOD outperforms previous SOTA of one-class OOD detection by 5.7\\%, multi-class OOD detection by 3.0\\%, and near-distribution OOD detection by 2.1\\%. It even defeats the 10-shot-per-class outlier exposure OOD detection, although we do not include any OOD samples for our detection. Codes are available at \\href{}{https://github.com/JulietLJY/MOOD}.\n\n% achieve new state-of-the-art results on one-class and multi-class tasks with the AUROCs reaching 94.9\\% and 97.6\\% respectively, which are 5.7\\% and 3.0\\% higher than the existing works. \n\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\n\nA reliable visual recognition system not only provides correct predictions on known context (also known as in-distribution data) but also detects unknown out-of-distribution (OOD) samples and rejects (or transfers) them to human intervention for safe handling. This motivates applications of outlier detectors before feeding input to the downstream networks, which is the main task of OOD detection, also referred to as novelty or anomaly detection. OOD detection is the task of identifying whether a test sample is drawn far from the in-distribution (ID) data or not. It is at the cornerstone of various safety-critical applications, including medical diagnosis \\cite{caruana2015intelligible}, fraud detection \\cite{phua2010comprehensive}, autonomous driving \\cite{eykholt2018robust}, etc.\n% When deploying, the OOD network is supposed to solve problems with a large amount of data. \n\n\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=0.99\\linewidth]{figures/performance.pdf}\n    \\caption{Performance of MOOD compared with current SOTA (indicated by `*') on four OOD detection tasks: (a) one-class OOD detection; (b) multi-class detection; (c) near-distribution detection; and (d) few-shot outlier exposure OOD detection.}\n  \\label{fig:performance}\n\\end{figure}\n\n\nMany previous OOD detection approaches depend on outlier exposure \\cite{ssd, oodlimits} to improve the performance of OOD detection, which turns OOD detection into a simple binary classification problem. We claim that the core of OOD detection is, instead, to learn the effective ID representation to discover OOD samples without any known outlier exposure. \n\nIn this paper, we first present our surprising finding -- that is, {\\it simply using reconstruction-based methods can {\\it notably} boost the performance on various OOD detection tasks}. Our pioneer work along this line even outperforms previous few-shot outlier exposure OOD detection, albeit we do not include any OOD samples.\n\nExisting methods perform contrastive learning \\cite{csi, ssd} or pretrain classification on a large dataset \\cite{oodlimits} to detect OOD samples. The former methods classify images according to the pseudo labels while the latter classifies images based on ground truth, whose core tasks are both to fulfill the classification target. However, research on backdoor attack \\cite{backdoor_attack, frog_attack} shows that when learning is represented by classifying data, networks tend to take a shortcut to classify images. % by only learning specific patterns between categories. \n\nIn a typical backdoor attack scene  \\cite{frog_attack}, the attacker adds secret triggers on original training images with the visibly correct label. During the course of testing, the victim model classifies images with secret triggers into the wrong category. Research in this area demonstrates that networks only learn specific distinguishable patterns of different categories because it is a shortcut to fulfill the classification requirement. \n\nNonetheless, learning these patterns is ineffective for OOD detection since the network does not understand the intrinsic data distribution of the ID images. Thus, learning representations by classifying ID data for OOD detection may not be satisfying. For example, when the patterns similar to some ID categories appear in OOD samples, the network could easily interpret these OOD samples as the ID data and classify them into the wrong ID categories. \n\nTo remedy this issue, we introduce the reconstruction-based pretext task. Different from contrastive learning in existing OOD detection approaches \\cite{csi, ssd}, our method forces the network to achieve the training purpose of reconstructing the image and thus makes it learn pixel-level data distribution. \n\nSpecifically, we adopt the masked image modeling (MIM) \\cite{bert, beit, mae} as our self-supervised pretext task, which has been demonstrated to have great potential in both natural language processing \\cite{bert} and computer vision \\cite{beit, mae}. In the MIM task, we split images into patches and randomly mask a proportion of image patches before feeding the corrupted input to the vision transformer. Then we use the tokens from discrete VAE \\cite{tokenzier} as labels to supervise the network during training. With its procedure, the network learns information from remaining patches to speculate the masked patches and restore tokens of the original image. The reconstruction process enables the model to learn from the prior based on the intrinsic data distribution of images rather than just learning different patterns among categories in the classification process. \n\nIn our extensive experiments, it is noteworthy that masked image modeling for OOD detection (MOOD) outperforms the current SOTA on all four tasks of one-class OOD detection, multi-class OOD detection, near-distribution OOD detection, and even few-shot outlier exposure OOD detection, as shown in \\cref{fig:performance}. A few statistics are the following.\n\n\\begin{enumerate}\n\\item For one-class OOD detection (\\cref{tab:one-class}), MOOD boosts the AUROC of current SOTA, i.e., CSI \\cite{csi}, by \\textbf{5.7\\%} to \\textbf{94.9\\%}. \n\\item For multi-class OOD detection (\\cref{tab:multi-class}), MOOD outperforms current SOTA of SSD+ \\cite{ssd} by \\textbf{3.0\\%} and reaches \\textbf{97.6\\%}. \n\\item For near-distribution OOD detection (\\cref{tab:structure}), AUROC of MOOD achieves \\textbf{98.3\\%}, which is \\textbf{2.1\\%} higher than the current SOTA of R50+ViT \\cite{oodlimits}.\n\\item For few-shot outlier exposure OOD detection (\\cref{tab:exposure}), MOOD (\\textbf{99.41\\%}) surprisingly defeats current SOTA of R50+ViT \\cite{oodlimits} (with \\textbf{99.29\\%}), which makes use of 10 OOD samples per class. It is notable that we do not even include any OOD samples in MOOD.\n\\end{enumerate}\n\n%====================================relatex==================================\n\\section{Related Work}\n\\subsection{Out-of-distribution Detection}\nA straightforward out-of-distribution (OOD) approach is to estimate the in-distribution (ID) density \\cite{density_1,density_2,density_3,density_4} and reject test samples that deviate from the estimated distribution. Alternative methods base on the image reconstruction \\cite{reconstruct_1, reconstruct_2, reconstruct_3}, learn the decision boundary between in- and out-of-distribution data \\cite{boundary_1, boundary_2, boundary_3}, compute the distance between train and test features \\cite{distance_1, distance_2, distance_3, csi, ssd}, etc..\n\nIn comparison, our work focuses on distance-based methods and yet includes the reconstruction-based methods as a pretext task. The key idea of distance-based approaches is that the OOD samples are supposedly far from the center of the in-distribution (ID) data \\cite{ood_survey} in the feature space. Representative methods include K-nearest Neighbors \\cite{distance_1}, prototype-based methods \\cite{distance_2, distance_3}, etc.. We will explain the difference between our work and previous OOD detection methods later in this paper.\n\n\\subsection{Vision Transformer}\nTransformer has achieved promising performance in computer vision \\cite{beit, mae} and natural language processing \\cite{bert}. Existing OOD detection research \\cite{oodlimits} performs vision transformer (ViT  \\cite{vit}) with classification pre-train on ImageNet-21k \\cite{imagenet}. It mainly explores the impact of different structures on OOD detection tasks while we deeply explore the effect from four dimensions for OOD detection, including various pretext tasks, architectures, fine-tune processes, and OOD detection metrics. \n\nIt is notable that extra OOD samples are utilized in various previous methods \\cite{ssd, oodlimits} to further improve performance. In contrast, we argue that the exposure of OOD samples violates the original intention of OOD detection. In fact, a sufficient pretext task can achieve comparable or even superior results. Therefore, in our work, we focus on exploring an appropriate pretext task for OOD detection without including any OOD samples.\n\n%----------------------------------------pretask---------------------------------------\n\\begin{table*}[t]\n\\centering\n\\small\n\\setlength{\\tabcolsep}{3.5mm}\n\\begin{tabular}{c|cccccccccccccc}\n\\toprule\nIn-Distribution          & \\multicolumn{4}{c|}{CIFAR-10 $\\longrightarrow $}                                    & \\multicolumn{4}{c}{CIFAR-100 $\\longrightarrow $}              \\\\\nOut-of-Distribution         & SVHN          & CIFAR-100     & LSUN           & \\multicolumn{1}{c|}{Avg}           & SVHN          & CIFAR-10      & LSUN          & Avg           \\\\\n\\midrule\n%Cls(SM) & \\textbf{99.8} & 91.5          & 95.4           & \\multicolumn{1}{c|}{95.6}          & 90.8          & 90.9          & 79.2          & 87.0          \\\\\nClassification & 98.3          & 98.6          & 98.6           & \\multicolumn{1}{c|}{98.5}          & 78.0          & 93.5          & 88.6          & 86.7          \\\\\nMoCov3    & 98.6          & 92.4          & 89.8           & \\multicolumn{1}{c|}{93.6}          & 78.8          & 72.8          & 75.8          & 75.8          \\\\\nMIM         & \\textbf{99.8} & \\textbf{99.4} & \\textbf{99.9}  & \\multicolumn{1}{c|}{\\textbf{99.7}} & \\textbf{96.5} & \\textbf{98.3} & \\textbf{96.3} & \\textbf{97.0} \\\\\n\\midrule\nIn-Distribution          & \\multicolumn{8}{c}{ImageNet-30 $\\longrightarrow $}                                                                                                  \\\\\nOut-of-Distribution         & Dogs          & Places365     & Flowers102     & Pets                               & Food          & Dtd           & Caltech256    & Avg           \\\\\n\\midrule\n%Cls(SM) & 83.5          & 83.8          & 99.3           & 77.7                               & 69.6          & 91.8          & 89.4          & 85.0          \\\\\nClassification & \\textbf{99.7} & 98.4          & 99.9           & \\textbf{99.6}                      & \\textbf{98.3} & 98.6          & 96.8          & 98.8          \\\\\nMoCov3      & 88.2          & 82.0          & 99.3           & 81.1                               & 71.4          & 91.3          & 88.5          & 86.0          \\\\\nMIM         & 99.4          & \\textbf{98.9} & \\textbf{100.0} & 99.1                               & 96.6          & \\textbf{99.5} & \\textbf{98.9} & \\textbf{98.9} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Pretext Task}. AUROC (\\%) of OOD detection on ViT with different pretext tasks on ImageNet22k. }\n\\label{tab:pretask}\n\\end{table*}\n\n\n\\subsection{Self-Supervised Pretext Task}\nIt has been long in the community to pre-train vision networks in various self-supervised manners, including generative learning \\cite{pixelcnn, gpt, bert, beit}, contrastive learning \\cite{moco, supcon, simclr, simsiam} and adversarial learning \\cite{colorization, gan, adversial_ae}.\nAmong them, representative generative approaches include auto-regressive \\cite{pixelcnn, gpt}, flow-based \\cite{nice,glow}, auto-encoding \\cite{bert, beit}, and hybrid generative methods \\cite{graphaf, xlnet}. \n\nThe self-supervised pretext task in our framework is Masked Image Modeling (MIM). It generally belongs to auto-encoding generative approaches. MIM was first proposed in natural language processing \\cite{beit}. Its language modeling task randomly masks varying percentages of tokens of text and recovers the masked tokens from encoding results of the rest of text. Follow-up research \\cite{bert, mae} transfers the similar idea from natural language processing to computer vision, masking different proportions of the image patches to recover results.\n\nMultiple existing methods take advantage of self-supervised tasks to guide learning of representation for OOD detection. The latest work \\cite{csi, ssd} presents contrastive learning models as feature extractors. However, existing approaches of classifying transformed images according to contrastive learning possess similar limitations -- that is, the model tends to learn the specific patterns of categories, which are beneficial for classification but do not help understand intrinsic data  distributions of ID images. \n\nResearch of \\cite{oodlimits} also mentioned this problem. However, the introduced large-scale pre-trained transformers \\cite{oodlimits} may not jump out of the loop, in our observation, because the pretext task remained to be classification. In our work, we address this issue by performing the masked image modeling task for OOD detection.\n\n% By contrast, our framework performs the masked image modeling task to force the network to reconstruct the image and thus learn the pixel-level data distribution. Benefiting from the prior based on such data distribution, our OOD detection network can learn more distinguishable representations, which enlarge the divergence between in- and out-of-distribution data. The performance of MOOD compared with methods based on contrastive learning is vividly shown in the distances from ID and OOD testing samples to the training data in \\cref{fig:distribution}.\n\n\n\n%=================================Method========================================\n\\section{Method}\nIn this section, we first explain the main factors to help OOD detection and finally propose our framework to achieve this goal. \n\nWe first define the notations. For a given dataset $X_{\\rm ID}$, the goal of out-of-distribution (OOD) detection is to model a detector that identifies whether an input image $x \\in X_{\\rm ID}$ or $x \\notin X_{\\rm ID}$ (that is, $x \\in X_{\\rm OOD}$). A majority of existing methods for OOD detection define an OOD score function $s(x)$. Its abnormal high or low value represents that $x$ is from out-of-distribution. \n\n\\subsection{Choosing the Pretext Task} \n\\label{sec:pretask}\n\nIn this section, we choose the pretext task that can provide the intrinsic prior to suit the OOD detection task. Most previous OOD methods learn the ID representation through classification \\cite{baseline_ood, oodlimits} or contrastive learning \\cite{csi, ssd} on ID samples, which take advantage of either the ground truth or pseudo labels to supervise the classification networks. \n\nOn the other hand, work of \\cite{backdoor_attack, frog_attack} shows that classification networks only learn different patterns among training categories because it is a shortcut to fulfill classification. It is indicated that the network actually does not understand the intrinsic data distribution of the ID images.\n\nIn comparison, the reconstruction-based pretext task forces the network to learn the real data distribution of the ID images during training to reconstruct the image instead of the patterns for classification. Benefiting from these priors, the network can learn a more representative feature of the ID dataset. It enlarges the divergence between the OOD and ID samples. \n\nIn our method, we pre-train the model with Masked Image Modeling (MIM) pretext \\cite{bert} on a large dataset and fine-tune it on the ID dataset. We compare the performance of MIM and contrastive learning pretext task MoCov3 \\cite{mocov3} in ~\\cref{tab:pretask}. It shows that the performance of MIM is much increased by 13.3\\% to 98.66\\%. \n\n\\subsection{Exploring Architecture} \n\\label{sec:arch}\n%----------------------------------------structure------------------------------------------\n\\begin{table}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{3.5mm}\n\\begin{tabular}{c|ccccccccccc}\n\\toprule\n\\multirow{2}{*}{Model} & Fine-tuned          & \\multirow{2}{*}{AUROC(\\%)} \\\\\n                       & Test Acc(\\%)        &                            \\\\\n\\midrule\nBiT R50 \\cite{oodlimits} & 87.01               & 81.71                      \\\\\nBiT R101$\\times$3 \\cite{oodlimits}& 91.55               & 90.10                      \\\\\nViT \\cite{oodlimits} & 90.95               & 95.53                      \\\\\nMLP-Mixer \\cite{oodlimits} & 90.40               & 95.31                      \\\\\nR50 + ViT (SOTA) \\cite{oodlimits} & \\textbf{91.71}               & \\textbf{96.23}  \\\\\n% \\midrule\n% MOOD (ours)          & \\textbf{95.73}      & \\textbf{98.30}             \\\\\n%(Difference)              & {\\color{teal}+0.16} & {\\color{teal}+0.70}        \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Architecture}. AUROC (\\%) of OOD detection with various architectures. The last line shows our improvement. The ID and OOD datasets are CIFAR-100 and CIFAR-10, respectively. }\n\\label{tab:structure}\n\\end{table}\n\nTo explore an effective architecture \\cite{oodlimits}, we evaluate OOD detection performance on BiT (Big Transfer \\cite{bit}) and MLP-Mixer, in comparison with ViT. We adopt CIFAR-100 and CIFAR-10 \\cite{cifar} as the ID-OOD pair. They have close distributions because of their similar semantics and construction. Results are in \\cref{tab:structure}.\n\nR50 + ViT \\cite{vit, resnet} is the current SOTA on near-distribution OOD detection \\cite{oodlimits}, which doubles the model size and testing time but achieves only 96.23\\% (0.70\\% higher than ViT). However, MIM on a single ViT significantly improves its AUROC to 98.30\\% (2.07\\% higher), without any additional source assumption. It manifests that efficient pretext itself is sufficient for producing distinguishable representation -- {\\it there is no need to use a larger model or combination of multiple models} in this regard.\n\n\n\n\n\\subsection{About Fine-Tuning} \n\\label{sec:fine-tune}\n\n\\begin{table}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{2.1mm}\n\\begin{tabular}{c|ccc|ccccccc}\n\\toprule\nOne-Class                    & \\multicolumn{3}{c|}{fine-tune}         & \\multirow{2}{*}{AUROC(\\%)} \\\\\nDataset                      & MIM-pt     & inter-ft   & fine-tune         & \\\\\n\\midrule\n\\multirow{2}{*}{CIFAR-10}    & \\checkmark &            &            & 72.2 \\\\\n                             & \\checkmark & \\checkmark &            & \\textbf{97.9} \\\\\n\\midrule\n\\multirow{2}{*}{CIFAR-100}   & \\checkmark &            &            & 66.3  \\\\\n                             & \\checkmark & \\checkmark &            & \\textbf{96.5} \\\\\n\\midrule\n\\multirow{2}{*}{ImageNet-30} & \\checkmark &            &            & 75.2   \\\\\n                             & \\checkmark &            & \\checkmark & \\textbf{92.0} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Fine-tuning} (One-class). AUROC (\\%) of OOD detection with different fine-tuning processes on one-class CIFAR-10, CIFAR-100 (super-classes) and ImageNet-30.}\n\\label{tab:1class-fine-tune}\n\\end{table}\n\n%----------------------------------------metric--------------------------------------\n\\begin{table*}[t]\n\\centering\n\\small\n\\setlength{\\tabcolsep}{3.2mm}\n\\begin{tabular}{ccc|ccccccccccccc}\n\\toprule\n\\multicolumn{3}{c}{finetune}                   & \\multicolumn{4}{|c}{CIFAR-10 $\\longrightarrow $}                             & \\multicolumn{4}{|c}{CIFAR-100 $\\longrightarrow $}                                   \\\\\nMIM-pt               & inter ft   & ft         & SVHN           & CIFAR-100     & LSUN           & Avg                       & \\multicolumn{1}{|c}{SVHN}          & CIFAR-10             & LSUN                 & Avg                  \\\\\n\\midrule\n\\checkmark           &            &            & 62.2           & 62.9          & 98.5           & \\multicolumn{1}{c|}{74.5} & 48.4          & 42.2                 & 96.0                 & 62.2                 \\\\\n\\checkmark           & \\checkmark &            & 89.5           & 90.0          & 99.8           & \\multicolumn{1}{c|}{93.1} & 74.3          & 62.0                 & \\textbf{98.3}        & 68.2                 \\\\\n\\checkmark           &            & \\checkmark & 99.1           & 94.6          & 97.4           & \\multicolumn{1}{c|}{97.0}   & 93.7          & 83.7                 & 91.4                 & 89.6                 \\\\\n\\checkmark           & \\checkmark & \\checkmark & \\textbf{99.8}  & \\textbf{99.4} & \\textbf{99.9}  & \\multicolumn{1}{c|}{\\textbf{99.7}} & \\textbf{96.5} & \\textbf{98.3}        & 96.3                 & \\textbf{97.0}        \\\\\n\\midrule\n\\multicolumn{3}{c}{finetune}                   & \\multicolumn{8}{|c}{ImageNet30 $\\longrightarrow $}                                                                                                                \\\\\nMIM-pt               & inter-ft   & ft         & Dogs           & Places365     & Flowers102     & Pets                      & Food          & Caltech256           & Dtd                  & Avg                  \\\\\n\\midrule\n\\checkmark           &            &            & 60.2           & 82.7          & 28.6           & 41.9                      & 72.5          & 42.2                 & 29.4                 & 51.1                 \\\\\n\\checkmark           & \\checkmark &            & \\textbf{100.0} & 97.9          & 99.9           & \\textbf{99.6}             & \\textbf{97.1} & 96.9                 & 98.2                 & 98.2                 \\\\\n\\checkmark           &            & \\checkmark & 91.3           & 97.0          & 95.1           & 93.8                      & 99.3          & 84.0                 & 95.4                 & 92.9                 \\\\\n\\checkmark           & \\checkmark & \\checkmark & 99.4           & \\textbf{98.9} & \\textbf{100.0} & 99.1                      & 96.6          & \\textbf{99.5}        & \\textbf{98.9}        & \\textbf{98.9}       \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Fine-tuning} (Multi-class). AUROC (\\%) of OOD detection with different fine-tuning processes on multi-class CIFAR-10, CIFAR-100 and ImageNet-30. }\n\\label{tab:metric}\n\\label{tab:fine-tune}\n\\end{table*}\n\n%----------------------------------------metric--------------------------------------\n\\begin{table*}[t]\n\\centering\n\\small\n\\setlength{\\tabcolsep}{3.5mm}\n\\begin{tabular}{c|ccccccccccccccc}\n\\toprule\nIn-Distribution                           & \\multicolumn{4}{c|}{CIFAR-10 $\\longrightarrow $}                                                        & \\multicolumn{4}{c}{CIFAR-100 $\\longrightarrow $}                                          \\\\\nOut-of-Distribution                          & SVHN                 & CIFAR-100            & LSUN                 & \\multicolumn{1}{c|}{Avg}           & SVHN                 & CIFAR-10             & LSUN                 & Avg                  \\\\\n\\midrule\nSoftmax  \\cite{baseline_ood} & 88.6                 & 85.8                 & 90.7                 & \\multicolumn{1}{c|}{88.4}          & 81.9                 & 81.1                 & 86.6                 & 83.2                 \\\\\nEntropy  \\cite{baseline_ood} & \\textbf{99.9}        & 97.1                 & 98.1                 & \\multicolumn{1}{c|}{98.4}          & 93.7                 & 94.1                 & 88.7                 & 92.2                 \\\\\nEnergy   \\cite{energy}       & \\textbf{99.9}        & 97.0                 & 97.6                 & \\multicolumn{1}{c|}{98.2}          & 92.8                 & 93.5                 & 86.1                 & 90.8                 \\\\\nGradNorm  \\cite{gradnorm}    & 99.6                 & 94.3                 & 87.8                 & \\multicolumn{1}{c|}{93.9}          & 61.6                 & 87.7                 & 38.4                 & 62.6                 \\\\\nDistance  \\cite{mahalanobis} & 99.8                 & \\textbf{99.4}        & \\textbf{99.9}        & \\multicolumn{1}{c|}{\\textbf{99.7}} & \\textbf{96.5}        & \\textbf{98.3}        & \\textbf{96.3}        & \\textbf{97.0}        \\\\\n\\midrule\nIn-Distribution                           & \\multicolumn{8}{c}{ImageNet-30 $\\longrightarrow $}                                                                                                                                                  \\\\\nOut-of-Distribution                          & Dogs                 & Places365            & Flowers102           & Pets                               & Food                 & Dtd                  & Caltech256           & Avg                  \\\\\n\\midrule\nSoftmax  \\cite{baseline_ood} & 96.7                 & 90.5                 & 89.7                 & 95.0                               & 79.8                 & 90.6                 & 90.1                 & 90.3                 \\\\\nEntropy  \\cite{baseline_ood} & 92.5                 & 87.2                 & 97.5                 & 90.6                               & 69.6                 & 94.9                 & 85.7                 & 88.3                 \\\\\nEnergy   \\cite{energy}       & 89.7                 & 82.1                 & 95.8                 & 88.1                               & 67.8                 & 93.1                 & 82.3                 & 85.6                 \\\\\nGradNorm  \\cite{gradnorm}    & 74.8                 & 78.7                 & 92.0                 & 70.6                               & 61.5                 & 90.3                 & 74.3                 & 77.5                 \\\\\nDistance  \\cite{mahalanobis} & \\textbf{99.4}        & \\textbf{98.9}        & \\textbf{100.0}       & \\textbf{99.1}                      & \\textbf{96.6}        & \\textbf{99.5}        & \\textbf{98.9}        & \\textbf{98.9}       \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Metric}. AUROC (\\%) of OOD detection with different metrics on multi-class CIFAR-10, CIFAR-100 and ImageNet-30. }\n\\label{tab:metric}\n\\label{tab:fine-tune}\n\\end{table*}\n\n\n\\noindent\\textbf{One-class Fine-tuning.} For one-class OOD detection, we pre-train the MIM model and finely tune it on ImageNet-21k \\cite{imagenet}, as recommended by BEiT \\cite{beit}. In particular, when performing one-class OOD detection on ImageNet-30, since we do not include the OOD labels during training, we only pre-train it on ImageNet-21k without intermediate fine-tuning. Therefore, we utilize the label smoothing \\cite{ls} to help the model learn from the one-class fine-tune task on the ID dataset as\n\\begin{equation}\\label{equ:ls}\n    y_c^{LS} = y_c(1-\\alpha)+\\alpha/N_c, \\quad\\quad c=1, 2, \\dots, N_c\n\\end{equation}\nwhere $c$ is the index of category; $N_c$ is the number of classes; and $\\alpha$ is the hyperparameter that determines smoothing level. If $\\alpha=0$, we obtain the original one-hot encoded $y_c$ and if $\\alpha=1$, we get the uniform distribution.\n\nLabel smoothing was used to address overfitting and overconfidence in normal fine-tuning process. We, instead, find that it can be utilized in one-class fine-tuning. The performance of the model before and after one-class fine-tune is illustrated in \\cref{tab:1class-fine-tune}. It is clear that the model actually learns information from the one-class fine-tuning operation. This may be counter-intuitive because the labels are equal. The reason is, due to label smoothing, the loss is larger than 0 and persuades the model to update parameters, although the accuracy reaches 1.\n\n\\vspace{2mm}\\noindent\\textbf{Multi-class Fine-tuning.}\nFor multi-class OOD detection, we pre-train the MIM model, intermediately use fine-tuning on ImageNet-21k \\cite{imagenet}, and apply fine-tuning again on the ID dataset. We perform experiments to validate the effectiveness of each stage in \\cref{tab:fine-tune}. It proves that all stages contribute well to the performance of OOD detection.\n\n%------------------------------------OOD------------------------------------\n\n\\subsection{OOD Detection Metric is Important}\n\\label{sec:metric}\n\nHere, we compare the performance of several commonly-used OOD detection metrics, including Softmax \\cite{baseline_ood}, Entropy \\cite{baseline_ood}, Energy \\cite{energy}, GradNorm  \\cite{gradnorm} and Mahalanobis distance \\cite{mahalanobis}. We perform OOD detection with MIM pretext task with each metric -- the results are shown in \\cref{tab:metric}. They prove that the Mahalanobis distance is a better metric for MOOD.\n\n\\subsection{Final Algorithm of MOOD} \n\\label{sec:alg}\n\nTo sum up, in this section, we have explored the effect of contributors to OOD detection, including various pretext tasks, architectures, fine-tuning processes, and OOD detection metrics. In general, we find that the finely tuned MOOD on ViT with Mahalanobis distances achieves the best result. The outstanding performance of MOOD demonstrates that an efficient pretext task itself is sufficient for producing distinguishable representation, and there is no need for a larger model or multi-models. \n\nIn \\cref{sec:exp}, we will show that few-shot outlier exposure utilized in multiple existing OOD detection approaches \\cite{ssd, oodlimits} is also unnecessary. The algorithm of MOOD is shown in the Appendix. It mainly includes the following stages.\n\n\\begin{enumerate}\n% \\renewcommand{\\labelenumi}{Step \\theenumi.}\n\\item Pre-train the Masked Image Modeling ViT on ImageNet-21k.\n\\item Apply intermediate fine-tuning ViT on ImageNet-21k. %(Not for one-class detection on ImageNet-30). \n\\item Apply fine-tuning of pre-trained ViT on the ID dataset. % (Not for one-class detection on CIFAR-10 and CIFAR-100). \n\\item Extract features from the trained ViT and calculate the Mahalanobis distance metric for OOD detection.\n\\end{enumerate}\n\n%--------------------------------------one-class-----------------------------------\n\\begin{table*}[t]\n\\small\n\\centering\n\\begin{subtable}{0.99\\linewidth}\n\\centering\n\\setlength{\\tabcolsep}{1.5mm}\n\\begin{tabular}{c|ccccccccccccccc}\n\\toprule\n%Method & Network   & Airplane & Automobile & Bird & Cat  & Dear & Dog  & Frog & Horse & Ship & Truck & Average \\\\\nMethod   & Plane & Car   & Bird & Cat & Dear& Dog & Frog& Horse & Ship& Truck & Average  \\\\\n\\midrule\nOC-SVM\\cite{goad} & 65.6 & 40.9 & 65.3 & 50.1 & 75.2 & 51.2 & 71.8 & 51.2 & 67.9 & 48.5 & 58.8   \\\\\nDeepSVDD\\cite{deepsvdd}  & 61.7 & 65.9 & 50.8 & 59.1 & 60.9 & 65.7 & 67.7 & 67.3 & 75.9 & 73.1 & 64.8   \\\\\nAnoGAN\\cite{anogan} & 67.1 & 54.7 & 52.9 & 54.5 & 65.1 & 60.3 & 58.5 & 62.5 & 75.8 & 66.5 & 61.8   \\\\\nOCGANOCGAN\\cite{ocgan} & 75.7 & 53.1 & 64.0 & 62.0 & 72.3 & 62.0 & 72.3 & 57.5 & 82.0 & 55.4 & 65.7   \\\\\nGeom\\cite{geom}  & 74.7 & 95.7 & 78.1 & 72.4 & 87.8 & 87.8 & 83.4 & 95.5 & 93.3 & 91.3 & 86.0   \\\\\nRot\\cite{rot}   & 71.9 & 94.5 & 78.4 & 70.0 & 77.2 & 86.6 & 81.6 & 93.7 & 90.7 & 88.8 & 83.3   \\\\\nRot+Trans\\cite{rot} & 77.5 & 96.9 & 87.3 & 80.9 & 92.7 & 90.2 & 90.9 & 96.5 & 95.2 & 93.3 & 90.1   \\\\\nGOAD\\cite{goad}  & 77.2 & 96.7 & 83.3 & 77.7 & 87.8 & 87.8 & 90.0 & 96.1 & 93.8 & 92.0 & 88.2   \\\\\nCSI (SOTA)\\cite{csi}   & 89.9 & 99.1 & 93.1 & 86.4 & 93.9 & 93.2 & 95.1 & 98.7 & 97.9 & 95.5 & 94.3   \\\\\nours  & \\textbf{98.6}\\tiny{$\\pm$0.4} & \\textbf{99.3}\\tiny{$\\pm$0.5} & \\textbf{94.3}\\tiny{$\\pm$0.6} & \\textbf{93.2}\\tiny{$\\pm$0.5} & \\textbf{98.1}\\tiny{$\\pm$0.6} & \\textbf{96.5}\\tiny{$\\pm$0.4} & \\textbf{99.3}\\tiny{$\\pm$0.2} & \\textbf{99.0}\\tiny{$\\pm$0.1} & \\textbf{98.8}\\tiny{$\\pm$0.1} & \\textbf{97.8}\\tiny{$\\pm$0.4} & \\textbf{97.8}\\tiny{$\\pm$0.4} \\\\\n(improve)   & {\\color{teal}+8.7}& {\\color{teal}+0.2}& {\\color{teal}+1.2}& {\\color{teal}+6.8}& {\\color{teal}+4.2}& {\\color{teal}+3.3}& {\\color{teal}+4.2}& {\\color{teal}+0.3}& {\\color{teal}+0.9}& {\\color{teal}+2.3}& {\\color{teal}+3.5} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{CIFAR-10}\n\\label{tab:one-class-cifar10}\n\\end{subtable}\n\\\\\n\\begin{subtable}{0.48\\linewidth}\n\\centering\n\\setlength{\\tabcolsep}{12mm}\n\\begin{tabular}{c|ccccccccccc}\n\\toprule\nMethod & AUROC   \\\\\n\\midrule\nOC-SVM\\cite{goad} & 63.1  \\\\\nGeom\\cite{geom}  & 78.7  \\\\\nRot\\cite{rot}   & 77.7  \\\\\nRot+Trans\\cite{rot} & 79.8  \\\\\nGOAD\\cite{goad}  & 74.5  \\\\\nCSI (SOTA)\\cite{csi}    & 89.6  \\\\\nours  & \\textbf{94.8}   \\\\\n(improve)   & {\\color{teal}+5.2}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{CIFAR-100}\n\\label{tab:one-class-cifar100}\n\\end{subtable}\n\\hspace{2.2mm}\n\\begin{subtable}{0.48\\linewidth}\n\\centering\n\\setlength{\\tabcolsep}{8mm}\n\\begin{tabular}{c|ccccccccccc}\n\\toprule\nMethod   & AUROC   \\\\\n\\midrule\nRot\\cite{rot}  & 65.3  \\\\\nRot+Trans\\cite{rot} & 77.9  \\\\\nRot+Attn\\cite{rot} & 81.6  \\\\\nRot+Trans+Attn\\cite{rot} & 84.8  \\\\\nRot+Trans+Attn+Resize\\cite{rot} & 85.7  \\\\\nCSI (SOTA) \\cite{csi}  & 91.6  \\\\\nours & \\textbf{92.0}   \\\\\n(improve)  & {\\color{teal}+0.4}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{ImageNet-30}\n\\label{tab:one-class-imagenet30}\n\\end{subtable}\n\\caption{\\textbf{One-class OOD detection.} AUROC (\\%) of OOD methods on one-class (a) CIFAR-10, (b) CIFAR-100 (super-classes) and (c) ImageNet-30. The reported results on CIFAR-10 are averaged over 3 trials. Subscripts denote standard deviation, and bold ones denote the best results. The last line lists improvement of MOOD over the current SOTA.}\n\\label{tab:one-class}\n\\end{table*}\n\n%--------------------------------------multi-class-----------------------------------\n\\begin{table*}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{3.2mm}\n\\begin{subtable}{0.99\\linewidth}\n\\centering\n\\begin{tabular}{c|cccccccccc}\n\\toprule\nIn-Distribution & \\multicolumn{4}{c|}{CIFAR-10 $\\longrightarrow $} &\\multicolumn{4}{c}{CIFAR-100 $\\longrightarrow $} \\\\\nOut-of-Distribution & SVHN  & CIFAR-100  & LSUN & \\multicolumn{1}{c|}{Average} & SVHN  & CIFAR-10   & LSUN & Average \\\\\n\\midrule\nBaseline OOD\\cite{baseline_ood} & 88.6  & 85.8 &90.7 &\\multicolumn{1}{c|}{88.4} & 81.9  & 81.1 &86.6 &83.2   \\\\\nODIN\\cite{odin}   & 96.4  & 89.6 &-   &\\multicolumn{1}{c|}{93.0} & 60.9  & 77.9 &-   &69.4   \\\\\nMahalanobis\\cite{mahalanobis}  & 99.4  & 90.5 &-   &\\multicolumn{1}{c|}{95.0} & 94.5  & 55.3 &-   &74.9   \\\\\nResidual Flows\\cite{residual_flows}   & 99.1  & 89.4 &-   &\\multicolumn{1}{c|}{94.3} & 97.5  & 77.1 &-   &87.3   \\\\\nGram Matrix\\cite{gram_matrix}  & 99.5  & 79.0 &-   &\\multicolumn{1}{c|}{89.3} & 96.0  & 67.9 &-   &82.0   \\\\\nOutlier exposure\\cite{outlier_exposure} & 98.4  & 93.3 &-   &\\multicolumn{1}{c|}{95.9} & 86.9  & 75.7 &-   &81.3   \\\\\nRotation loss\\cite{rot} & 98.9  & 90.9 &\u2013   &\\multicolumn{1}{c|}{94.9} & - & -   &-   &-   \\\\\nContrastive loss\\cite{supcon} & 97.3  & 88.6 &92.8 &\\multicolumn{1}{c|}{92.9} & 95.6  & 78.3 &-   &87.0   \\\\\nCSI\\cite{csi} & 97.9  & 92.2 &97.7 &\\multicolumn{1}{c|}{95.9} & - & -   &-   &-   \\\\\nSSD+ (SOTA) \\cite{ssd}   & \\textbf{99.9}   & 93.4 &98.4 &\\multicolumn{1}{c|}{97.2} & \\textbf{98.2}   & 78.3 &79.8 &85.4   \\\\\nours   & 99.8\\tiny{$\\pm$0.0} & \\textbf{99.4}\\tiny{$\\pm$0.0} & \\textbf{99.9}\\tiny{$\\pm$0.0} & \\multicolumn{1}{c|}{\\textbf{99.7}} & 96.5\\tiny{$\\pm$0.6} & \\textbf{98.3}\\tiny{$\\pm$0.1} & \\textbf{96.3}\\tiny{$\\pm$0.6} & \\textbf{97.0}   \\\\\n(improve) & {\\color{gray}-0.1}   & {\\color{teal}+6.0}  & {\\color{teal}+1.5}  & \\multicolumn{1}{c|}{{\\color{teal}+2.5}} & {\\color{gray}-1.7}   & {\\color{teal}+20.0}   & {\\color{teal}+16.5}   & {\\color{teal}+11.6}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{CIFAR}\n\\end{subtable}\n\n\\begin{subtable}{0.99\\linewidth}\n\\centering\n\\setlength{\\tabcolsep}{3.8mm}\n\\begin{tabular}{c|ccccccccccccccc}\n\\toprule\nIn-Distribution & \\multicolumn{8}{c}{ImageNet-30 $\\longrightarrow $}   \\\\\nOut-of-Distribution & Dogs  & Places365 & Flowers102 & Pets  & Food  & Caltech256 & DTD   & Average  \\\\\n\\midrule\nBaseline OOD\\cite{baseline_ood} & 96.7  & 90.5  & 89.7  & 95.0  & 79.8  & 90.6  & 90.1  & 90.3  \\\\\nContrastive loss\\cite{supcon} & 95.6  & 89.7  & 92.2  & 94.2  & 81.2  & 90.2  & 92.1  & 90.7  \\\\\nCSI (SOTA)\\cite{csi}   & 98.3  & 94.0  & 96.2  & 97.4  & 87.0  & 93.2  & 97.4  & 94.8  \\\\\nours  & \\textbf{99.4} & \\textbf{98.9} & \\textbf{100.0} & \\textbf{99.1} & \\textbf{96.6} & \\textbf{99.5} & \\textbf{98.9} & \\textbf{98.9}  \\\\\n(improve)   & {\\color{teal}+0.9} & {\\color{teal}+4.9} & {\\color{teal}+3.8} & {\\color{teal}+1.7} & {\\color{teal}+9.6} & {\\color{teal}+6.3} & {\\color{teal}+1.5} & {\\color{teal}+4.1}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{ImageNet-30}\n\\end{subtable}\n\n\\begin{subtable}{0.99\\linewidth}\n\\centering\n\\setlength{\\tabcolsep}{7.7mm}\n\\begin{tabular}{c|ccccccccccccccc}\n\\toprule\nIn-Distribution                              & \\multicolumn{5}{c}{ImageNet-1k $\\longrightarrow $}                                                       \\\\\nOut-of-Distribution                             & iNaturalist        & SUN                & Places             & Textures            & Average            \\\\\n\\midrule\nBaseline OOD \\cite{baseline_ood}         & 87.6               & 78.3               & 76.8               & 74.5                & 79.3               \\\\\nODIN \\cite{odin}                & 89.4               & 83.9               & 80.7               & 76.3                & 82.6               \\\\\nEnergy \\cite{energy}            & 88.5               & 85.3               & 81.4               & 75.8                & 82.7               \\\\\nMahalanobis \\cite{mahalanobis}  & 46.3               & 65.2               & 64.5               & 72.1                & 62.0               \\\\\nGradNorm (SOTA) \\cite{gradnorm} & \\textbf{90.3}      & 89.0               & 84.8               & 81.1                & 86.3               \\\\\nours                     & 86.9               & \\textbf{89.8}      & \\textbf{88.5}      & \\textbf{91.3}       & \\textbf{89.1}      \\\\\n(improve)                       & {\\color{gray}-3.4} & {\\color{teal}+0.8} & {\\color{teal}+3.7} & {\\color{teal}+10.2} & {\\color{teal}+2.8}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{ImageNet-1k}\n\\end{subtable}\n\\caption{\\textbf{Multi-class OOD detection.} AUROC (\\%) of OOD detection methods on multi-class CIFAR-10, CIFAR-100, ImageNet-30 and ImageNet-1k. The reported results on CIFAR-10 and CIFAR-100 are averaged over 3 trials. Subscripts denote standard deviation, and bold ones stand for the best results. The last line lists improvement of MOOD over the current SOTA approach.}\n\\label{tab:multi-class}\n\\end{table*}\n\n\n\n\n%====================================exp===============================================\n\n\\section{Experiments}\n\\label{sec:exp}\nIn this section, we compare Masked Image Modeling for OOD detection (MOOD) with current SOTA approaches in one-class OOD detection (\\cref{sec:1class}), multi-class OOD detection (\\cref{sec:multi-class}), near-distribution OOD detection (\\cref{sec:near-distribution}) and OOD detection with few-shot outlier exposure (\\cref{sec:exposure}). Our MOOD outperforms all previous approaches on all four OOD detection tasks significantly. % In \\cref{sec:discuss}, we visualize the distribution of ID and OOD samples and discuss the performance for near-distribution OOD detection. \n\n\\vspace{2mm}\\noindent\\textbf{Experimental Configuration.} We report the commonly-used Area Under the Receiver Operating Characteristic Curve (AUROC) as a threshold-free evaluation metric for detecting OOD score. We perform experiments on (i) CIFAR-10 \\cite{cifar}, which consists of 50,000 training and 10,000 testing images with 10 image classes, (ii) CIFAR-100 \\cite{cifar} and CIFAR-100 (super-classes) \\cite{cifar}, which consists of 50,000 training and 10,000 testing images with 100 and 20 (super-classes) image classes. respectively, (iii) ImageNet-30 \\cite{imagenet}, which contains 39,000 training and 3,000 testing images with 30 image classes, and (iv) ImageNet-1k \\cite{imagenet}, which contains around 120k and 50k testing images with 1k image classes.\nMore details of training settings are given in the Appendix.\n\n\\subsection{One-Class OOD Detection}\n\\label{sec:1class}\nWe start with the one-class OOD detection. For a given multi-class dataset of $N_c$ classes, we conduct $N_c$ one-class OOD tasks, where each task regards one of the classes as in-distribution and the remaining classes as out-of-distribution. We run our experiments on three datasets, following prior work \\cite{geom, rot, goad}, of CIFAR-10, CIFAR-100 (super-classes), and ImageNet-30. \n\n\\Cref{tab:one-class} summarizes the results, showing that MOOD outperforms current SOTA of CSI \\cite{csi} on all tested cases significantly. The improvement is of 5.7\\% to 94.9\\% on average. The improvement is comparatively smaller on ImageNet-30 \\cref{tab:one-class-imagenet30}. It is because we do not apply intermediate fine-tuning of the model on ImageNet-30. More details are shown in \\cref{sec:fine-tune}. We provide the class-wise AUROC in the Appendix for detailed exhibition.\n\n%  with OC-SVM\\cite{goad}, DeepSVDD\\cite{deepsvdd}, AnoGAN\\cite{anogan}, OCGAN\\cite{ocgan}, Geom\\cite{geom}, Rot\\cite{rot}, Rot+Trans\\cite{rot}, GOAD\\cite{goad} and CSI\\cite{csi}\n\n%------------------------------multi-class-------------------------------------------\n\n\\subsection{Multi-Class OOD Detection} \n\\label{sec:multi-class}\nFor multi-class OOD Detection, we assume that ID samples are from a specific multi-class dataset. They are tested on various external datasets as out-of-distribution. We perform MOOD on CIFAR-10, CIFAR-100, ImageNet-30 and ImageNet-1k. For CIFAR-10, We consider CIFAR-100 \\cite{cifar}, SVHN \\cite{svhn} and LSUN \\cite{lsun} as OOD datasets. For CIFAR-100, We consider CIFAR-10 \\cite{cifar}, SVHN \\cite{svhn} and LSUN \\cite{lsun} as OOD datasets. For ImageNet-30, OOD samples are from CUB-200 \\cite{cub}, Stanford Dogs \\cite{dogs}, Oxford Pets \\cite{pets}, Oxford Flowers \\cite{flowers}, Food-101 \\cite{food}, Places-365 \\cite{places}, Caltech-256 \\cite{caltech}, and Describable Textures Dataset (DTD) \\cite{dtd}. For ImageNet-1k, we utilize non-natural images as OOD datasets, which includes iNatualist \\cite{inaturalist}, SUN \\cite{sun}, places \\cite{places}, Textures \\cite{dtd}.\n\nAs shown in \\cref{tab:multi-class}, MOOD boosts performance of current SOTA of SSD+ \\cite{ssd} by 3.0\\% to 97.6\\% and SOTA of GradNorm \\cite{gradnorm} by 2.8\\% to 89.1\\% on ImageNet-1k. We remark that when detecting hard (i.e., near-distribution) OOD samples on ImageNet30 and Food, MOOD still yields decent performance, while previous methods often fail. \n\n\\vspace{2mm}\\noindent\\textbf{Visualization.} In \\cref{fig:distribution}, we illustrate the probability distribution of the test samples according to metrics of three OOD detection approaches: baseline OOD detection \\cite{baseline_ood}, SSD+ \\cite{ssd}, and MOOD. The baseline OOD detection performs softmax as its OOD detection metric, where ID samples tend to have greater value than OOD samples. MOOD and SSD+ perform the Mahalanobis distance as their metrics. \n\nAs shown in the figure, the distance of a majority of testing ID samples to the training data is close to zero, demonstrating a similar representation of training and testing ID samples. In contrast, the distances from most OOD samples to the training data are much larger, especially on CIFAR-10 and ImageNet-30. \n\nAlso, in \\cref{fig:distribution}, we reveal that the difference in the distribution of ID and OOD samples according to MOOD is significantly larger compared with other approaches \\cite{baseline_ood, ssd}. It demonstrates that MOOD can separate ID and OOD samples more clearly. In order to illustrate the appearance of images in each ID and OOD dataset, we plot several images as examples with their corresponding distances in the Appendix.\n\n% We compare MOOD with Baseline OOD\\cite{baseline_ood}, ODIN\\cite{odin}, Mahalanobis\\cite{mahalanobis}, Residual Flows\\cite{residual_flows}, Gram Matrix\\cite{gram_matrix}, Outlier exposure\\cite{outlier_exposure}, SupCon\\cite{supcon} and SSD+\\cite{ssd}. \n\n\n%--------------------------------------visualization-----------------------------------\n\\begin{figure}[t]\n  \\centering\n  \\begin{subfigure}{\\linewidth}\n    \\includegraphics[width=0.99\\linewidth, trim=0 0 0 0, clip]{figures/distances_cifar10.pdf}\n    \\caption{ID: CIFAR-10}\n    \\label{fig:distribution_cifar100}\n  \\end{subfigure}\n  \\\\\n  \\begin{subfigure}{\\linewidth}\n    \\includegraphics[width=0.99\\linewidth, trim=0 0 0 0, clip]{figures/distances_cifar100.pdf}\n    \\caption{ID: CIFAR-100}\n    \\label{fig:distribution_cifar100}\n  \\end{subfigure}\n   \\\\\n  \\begin{subfigure}{\\linewidth}\n    \\includegraphics[width=0.99\\linewidth, trim=0 0 0 0, clip]{figures/distances_imagenet30.pdf}\n    \\caption{ID: ImageNet-30}\n    \\label{fig:distribution_imagenet30}\n    \\end{subfigure}\n    \\caption{Line chart to illustrate the relation between the probability distribution of test samples and OOD detection metrics on (a) CIFAR-10, (b) CIFAR-100, and (c) ImageNet-30. Each line in the sub-figures represents an OOD or ID dataset. We compare three OOD detection approaches, including baseline OOD detection, SSD+ (current SOTA, \\cite{ssd} ), and our proposed MOOD. The baseline OOD detection takes the maximum softmax probabilities as its OOD detection metric, while SSD+ and MOOD both use the Mahalanobis distance as their metrics.} %  The greater the difference in the distribution of ID and OOD samples according to a specific metric, the larger the degree of separation between ID and OOD samples, representing the better performance of OOD detection. As can be seen from the probability distribution, our proposed MOOD illustrates obvious priority compared with other OOD detection approaches\n  \\label{fig:distribution}\n\\end{figure}\n\n\n\n%------------------------hard samples---------------------\n\\iffalse\n\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=0.99\\linewidth]{figures/one-class-near-distribution.pdf}\n    \\caption{AUROC (\\%) of near-distribution pairs in one-class detection on CIFAR-10, compared with current SOTA (CSI \\cite{csi}).}\n  \\label{fig:hardood-oneclass}\n\\end{figure}\n\\fi\n\n\\subsection{Near-Distribution OOD Detection} \n\\label{sec:near-distribution}\nCompared with existing approaches on normal OOD detection tasks, SOTA results of near-distribution OOD detection is much worse -- AUROC of some ID-OOD pairs \\cite{csi, ssd} is even lower than 70\\%. Therefore, improving SOTA for near-OOD detection is essential for the application to work on real-world data. \n\n\\begin{table}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{2.8mm}\n\\begin{tabular}{cc|cccccc|cccc}\n\\toprule\nID & OOD  & \\multicolumn{3}{c}{AUROC (\\%)} \\\\\nclass & class  & CSI \\cite{csi} & ours  & (improve)           \\\\\n\\midrule\nPlane   & Automobile   & 74.1 & 99.0  & {\\color{teal}+24.9} \\\\\nPlane   & Ship  & 79.6 & 99.4  & {\\color{teal}+19.8} \\\\\nPlane   & Truck & 82.8 & 98.5  & {\\color{teal}+15.7} \\\\\nBird    & Horse & 83.2 & 94.3  & {\\color{teal}+11.1} \\\\\nCat     & Deer  & 83.3 & 92.6  & {\\color{teal}+9.3}  \\\\\nCat     & Dog   & 67.0 & 75.5  & {\\color{teal}+8.5}  \\\\\nCat     & Frog  & 89.6 & 92.5  & {\\color{teal}+2.9}  \\\\\nCat     & Horse & 79.0 & 95.5  & {\\color{teal}+16.5} \\\\\nDeer    & Horse & 69.0 & 100.0 & {\\color{teal}+31.0}   \\\\\nDog     & Deer  & 88.1 & 96.4  & {\\color{teal}+8.3}  \\\\\nDog     & Horse & 76.6 & 95.5  & {\\color{teal}+18.9} \\\\\nTrunk   & Automobile   & 72.3 & 87.8  & {\\color{teal}+15.5}\\\\\n\\midrule\n\\multicolumn{2}{c|}{Average} & 78.7 & 93.9 & {\\color{teal}+15.2} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Near-distribution OOD detection} (one-class). AUROC (\\%) of near-distribution pairs in one-class detection on CIFAR-10, compared with current SOTA (CSI \\cite{csi}).}\n\\label{tab:hardood-oneclass}\n\\end{table}\n\n\n\n\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=0.99\\linewidth, trim=0 15 0 10 clip]{figures/multi-class-near-distribution.pdf}\n    \\caption{\\textbf{Near-distribution OOD detection} (multi-class). Number of some mistakenly-classified OOD samples (when TPR = 95\\%). These samples are wrongly taken as ID samples by the current SOTA of SSD+ \\cite{ssd} in multi-class detection on CIFAR-10. `*' indicates SOTA.}\n  \\label{fig:hardood-multiclass}\n\\end{figure}\n\nIn \\cref{tab:structure}, we have compared MOOD with the current SOTA on near-distribution CIFAR10-CIFAR100 (ID-OOD) pair, R50+ViT \\cite{oodlimits}, and MOOD outperforms the latter significantly by 2.07\\% to 98.30\\%. In this section, we focus on the hard-detected pairs with similar semantics from \\cref{sec:1class} and \\cref{sec:multi-class}. \n\nFor one-class OOD detection, we adopt 12 hard-detected ID-OOD pairs (AUROC under 90\\%) from the confusion matrix of current one-class OOD detection SOTA of CSI \\cite{csi}. The semantics of these ID-OOD pairs are more similar than normal ID-OOD combinations, such as trunk and car, deer and horse, etc., leading to their poor OOD detection performance. As shown in \\cref{tab:hardood-oneclass}, MOOD significantly boosts the AUROC of current SOTA from 78.7\\% to 93.9\\%. \n\nFor multi-class OOD detection, we examine the large mistakenly-classified value in the OOD-ID confusion matrix, which represents the number of classifying the OOD image to the category in the ID dataset. For example, when the True-Positive Rate (TPR) is 95\\%, 48 testing tiger images from CIFAR-100 are classified as cat by the current multi-class OOD detection SOTA method of SSD+ \\cite{ssd}, while only 2 of them are wrongly classified by MOOD. More results are shown in \\cref{fig:hardood-multiclass}. For the listed 12 ID-OOD pairs, MOOD averagely reduces the number of mistakenly-classified OOD samples notably by 79\\%. \n\n\n\n\n\\begin{table}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{3.1mm}\n\\begin{tabular}{c|ccccccccccc}\n\\toprule\n\\multirow{2}{*}{Method}  & \\# OOD samples & \\multirow{2}{*}{AUROC(\\%)} \\\\\n                         & per class  &                           \\\\\n\\midrule\n\\multirow{5}{*}{R50+ViT (SOTA) \\cite{oodlimits}} & 0          & 98.52                     \\\\\n                         & 1          & 98.96                     \\\\\n                         & 2          & 99.11                     \\\\\n                         & 3          & 99.17                     \\\\\n                         & 10         & 99.29                     \\\\\n\\midrule\nours           & 0          & \\textbf{99.41}            \\\\\n(improve)                & -          & {\\color{teal}+0.12}      \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Outlier Exposure OOD detection.} AUROC (\\%) of current SOTA of R50+ViT \\cite{oodlimits} for near-distribution OOD detection and MOOD. SOTA utilizes up to 10 known OOD samples per class for detection, while ours do not include any OOD samples.}\n\\label{tab:exposure}\n\\end{table}\n\n\\subsection{OOD Detection with Outlier Exposure} \n\\label{sec:exposure}\nSeveral representative OOD detection methods \\cite{ssd, oodlimits} utilize OOD samples to improve the performance in extra stages. We note they are not included in our work because we generally believe that exposure of OOD samples violates the original intention of OOD detection. \n\nIn \\cref{tab:exposure}, we compare MOOD with current SOTA \\cite{oodlimits} for near-distribution OOD detection with up to 10 OOD samples per class. We surprisingly find that MOOD works better in terms of AUROC than current SOTA \\cite{oodlimits}, even though we do not include any OOD samples for detection. The outstanding performance of MOOD demonstrates that an effective pretext task is already sufficient for producing a distinguishable representation that OOD detection requires. Thus, there is no need to include extra OOD samples.\n\n\n%==================================Conclusion=========================================\n\n\n\\section{Conclusion}\nIn this paper, we have extensively explored the effect of multiple contributors for OOD detection and observed that reconstruction-based pretext tasks have the potential to provide effective priors for OOD detection to learn the real data distribution of the ID dataset. Specifically, we take the Masked Image Modeling pretext task for our OOD detection framework (MOOD). We perform MOOD on one-class OOD detection, multi-class OOD detection, near-distribution OOD detection, and few-shot outlier exposure OOD detection -- MOOD all achieve new SOTA results, although we do not include any OOD samples for detection.\n\n\\section{Acknowledgement}\nThis work is partially supported by Shenzhen Science and Technology Program KQTD20210811090149095.\n. The first paper ends. The second paper begins. \\IEEEtitleabstractindextext{%\n\\begin{abstract}\n\\revise{The crux of effective out-of-distribution (OOD) detection lies in acquiring a robust in-distribution (ID) representation, distinct from OOD samples. While previous methods predominantly leaned on recognition-based techniques for this purpose, they often resulted in shortcut learning, lacking comprehensive representations. In our study, we conducted a comprehensive analysis, exploring distinct pretraining tasks and employing various OOD score functions. The results highlight that the feature representations pre-trained through reconstruction yield a notable enhancement and narrow the performance gap among various score functions. This suggests that even simple score functions can rival complex ones when leveraging reconstruction-based pretext tasks. Reconstruction-based pretext tasks adapt well to various score functions. As such, it holds promising potential for further expansion. Our OOD detection framework, MOODv2, employs the masked image modeling pretext task. Without bells and whistles, MOODv2 impressively enhances 14.30\\% AUROC to 95.68\\% on ImageNet and achieves 99.98\\% on CIFAR-10.}\n\\end{abstract}\n\n% Note that keywords are not normally used for peerreview papers.\n\\begin{IEEEkeywords}\nComputer Vision, Out-of-Distribution Detection, Outlier Detection, Masked Image Modeling\n\\end{IEEEkeywords}\n\n% Codes are available at \\href{}{https://github.com/JulietLJY/MOOD}\n}\n\n\\maketitle\n\\IEEEdisplaynontitleabstractindextext\n\\IEEEpeerreviewmaketitle\n\n\\IEEEraisesectionheading{\\section{Introduction}\\label{sec:intro}}\n\\IEEEPARstart{A} reliable visual recognition system not only provides correct predictions on known context (also known as in-distribution data) but also detects unknown out-of-distribution (OOD) samples and rejects (or transfers) them to human intervention for safe handling. This motivates the applications of outlier detectors before feeding input to the downstream networks, which is the main task of OOD detection, also referred to as novelty or anomaly detection. OOD detection is the task of identifying whether a test sample is drawn far from the in-distribution (ID) data or not. It is at the cornerstone of various safety-critical applications, including medical diagnosis \\cite{caruana2015intelligible}, fraud detection \\cite{phua2010comprehensive}, autonomous driving \\cite{eykholt2018robust}, etc~\\cite{tagclip, motcoder, bal}. A representative in-distribution feature space representation is crucial for out-of-distribution detection. A well-crafted feature representation significantly enhances the performance via most mainstream OOD detection score functions. Our research is dedicated to refining feature representations tailored for OOD detection, with the aim of advancing the entire field.\n\nExisting methods perform contrastive learning \\cite{csi, ssd} or pretrain classification on a large dataset \\cite{oodlimits, vim, yang2021generalized, sariyildiz2023fake} to detect OOD samples. The former methods classify images according to the pseudo labels while the latter classifies images based on ground truth, whose core tasks are both to fulfill the classification target. However, research on backdoor attack \\cite{backdoor_attack, frog_attack} shows that when learning is represented by classifying data, networks tend to take a shortcut to classify images. In a typical backdoor attack scene \\cite{frog_attack}, the attacker adds secret triggers on original training images with the visibly correct label. During the course of testing, the victim model classifies images with secret triggers into the wrong category. Research in this area demonstrates that networks only learn specific distinguishable patterns of different categories because it is a shortcut to fulfill the classification requirement. Nonetheless, learning these patterns is ineffective for OOD detection. Thus, learning representations by classifying ID data for OOD detection may not be satisfying. For example, when patterns similar to some ID categories appear in OOD samples, the network could easily interpret these OOD samples as the ID data and classify them into the wrong ID categories\\revise{, as shown in \\cref{fig:intro}}. \n\n\n\\begin{figure}[t]\n  \\centering    \n    \\includegraphics[width=0.8\\linewidth]{figures/ablation_imagenet.pdf}\n    \\caption{\\re{The average AUROC (\\%) tested on four OOD datasets applied to a ViT model with different pre-text tasks. \n    Methods in blue use the feature space;\n    methods in green use logits;\n    methods in yellow use the softmax probability;\n    and methods in red use both features and logits. The stars show the average performance of a category of methods.}}\n  \\label{fig:ablation}\n\\end{figure}\n\n\n\\begin{figure*}[t]\n  \\centering\n    \\includegraphics[width=0.9\\linewidth, trim=75 225 380 120, clip]{figures/mood.pdf}\n    \\caption{\\revise{Comparison of reconstruction-based and classification-based methods. In the context of image classification, networks often take a shortcut when categorizing images \\cite{backdoor_attack, frog_attack}. For example, ears are a distinctive feature for distinguishing between cats and dogs, and a classification model typically assumes that animals with pointed ears are cats, while those without are dogs. Consequently, when the network encounters an out-of-distribution animal, such as a fox with pointed ears, it readily misclassifies it as a cat. In contrast, reconstruction-based tasks effectively mitigate this issue. By randomly masking portions of images, the model avoids learning localized, stereotypical features (e.g., masked ears), thus preventing shortcuts and instead acquiring effective pixel-level representations for ID data. This significantly improves the model's ability to detect OOD instances.}}\n  \\label{fig:intro}\n\\end{figure*}\n\n\n\\iffalse\n\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=0.99\\linewidth]{figures/impression.pdf}\n    \\caption{\\revise{The AUROC (in percentage) of eight OOD detection algorithms applied to a ViT model with five pre-text tasks.The OOD datasets are ImageNet-O ($x$-axis) and OpenImage-O ($y$-axis).}}\n  \\label{fig:impression}\n\\end{figure}\n\\fi\n\n\\revise{To remedy this issue, we introduce the reconstruction-based pretext task. Different from contrastive learning in existing OOD detection approaches \\cite{csi, ssd}, our method forces the network to achieve the training purpose of reconstructing the image and thus makes it learn pixel-level feature representation. Specifically, we adopt the masked image modeling (MIM) \\cite{beit} as our self-supervised pretext task, which has been demonstrated to have great potential in both natural language processing \\cite{bert} and computer vision \\cite{beit, mae}. In the MIM task, a proportion of image patches are randomly masked. The network learns information from the remaining patches to speculate the masked patches and restore tokens of the original image. The reconstruction process enables the model to learn from the prior effective ID feature representation rather than just learning different patterns among categories in the classification process. In our work, we observed that the pre-trained models effectively reconstruct ID images, whereas they exhibit distinct domain differences when it comes to the OOD domain (\\cref{fig:recover}). This visual discrepancy clearly underscores the existing domain gap in model features between ID and OOD data, offering valuable insights for OOD detection.}\n\n\\revise{To validate the effectiveness of our ID feature representation, we conduct experiments to test its performance with various mainstream OOD detection score functions. We employed OOD score functions encompassing probability-based \\cite{baseline_ood, maxlogit}, logits-based \\cite{energy, maxlogit}, features-based \\cite{vim, react, mahalanobis}, and hybrid methods utilizing both logits and features \\cite{vim}. In the context of a comparative analysis spanning classic classification \\cite{vit}, contrastive learning \\cite{mocov3, dinov2}, and masked image modeling pretext tasks \\cite{beit, beitv2}, our findings underscore the dominant role of reconstruction-based strategies in the field of OOD detection, as illustrated in \\cref{fig:ablation}.}\n\n\\revise{Furthermore, we conduct a comprehensive analysis of the experimental results and observe that our approach not only significantly improves the overall results but also substantially reduces the disparities among score functions. This observation underscores that even simple score functions can perform on par with more complex ones when a representative ID feature representation is utilized. These findings further emphasize the critical importance of effective feature representation in OOD detection. More details are in \\cref{sec:pretask}.} \\revise{Ultimately, MOODv2 demonstrates remarkable enhancements, achieving a substantial 14.30\\% increase, reaching 95.68\\% AUROC on ImageNet. On CIFAR-10, our results significantly improved to an impressive 99.98\\%, marking a notable 0.35\\% enhancement compared to the previous state-of-the-art.}\n\n\n\n\\section{Related Works}\\label{sec:related}\n\n\\subsection{Out-of-distribution Detection}\n\\revise{Many scoring functions have been developed by researchers to distinguish between in-distribution and out-of-distribution examples. These functions are designed to exploit properties that are typically exhibited by ID examples but violated by OOD examples, and vice versa. These scores are primarily derived from three sources:}\n\\begin{enumerate}\n\n    \\item \\revise{\\textbf{Probability-based}: This category includes measures like the maximum softmax probabilities~\\cite{baseline_ood} and the minimum KL-divergence between the softmax and the mean class-conditional distributions~\\cite{maxlogit}, etc.}\n\n    \\item \\revise{\\textbf{Logit-based}: These functions rely on maximum logits~\\cite{maxlogit} and the \\(\\mathrm{logsumexp}\\) function computed over logits~\\cite{energy}, etc.}\n\n    \\item \\revise{\\textbf{Feature-based}: These functions involve the norm of the residual between a feature and the pre-image of its low-dimensional embedding~\\cite{ndiour2020out} and the minimum Mahalanobis distance between a feature and the class centroids~\\cite{mahalanobis}, among others.}\n\n\\end{enumerate}\n\\revise{After a thorough analysis of the performance and their correlations with various score functions and pretext tasks, our work follows the hybrid methods combining logit and feature \\cite{vim}} and includes the reconstruction-based methods as a pretext task. We will explain the implementation details later in this paper.\n\n\\subsection{Self-Supervised Pretext Task}\n\\revise{In the ever-evolving landscape of computer vision and deep learning, a multitude of strategies and techniques have been devised to enhance the capacity of models to understand and process visual data\\re{:}}\n\n\\begin{enumerate}\n\n    \\item \\revise{\\textbf{Classification task:} Vision models are pre-trained via classical classification task \\cite{vit}.}\n\n    \\item \\revise{\\textbf{Contrastive Learning tasks: }MOCOv3 \\cite{mocov3} and DINOv2 \\cite{dinov2} are advanced contrastive learning methods used for self-supervised representation learning. These methods focus on learning representations by contrasting positive pairs (e.g., different augmentations of the same image) with negative pairs (e.g., augmentations from different images). MOCOv3 extends the MOCO framework \\cite{moco} with a momentum encoder and dynamic queues for improved performance. DINOv2 introduces a clustered teacher network and an asymmetric loss to learn efficient representations.}\n\n    \\item \\revise{\\textbf{Masked Image Modeling Tasks: } Data-Efficient Image Transformer (BEiT series \\cite{beit, beitv2}) are self-supervised learning tasks that involve masked image modeling. In these tasks, a portion of an image is randomly masked, and the model's objective is to predict the masked pixels, effectively filling in the blanks.}\n    \n\\end{enumerate}\n\n\\revise{These methods and tasks represent cutting-edge approaches in the field of computer vision and deep learning. They have led to substantial improvements in the ability of models to learn useful visual representations from unlabeled data, enabling better performance on various downstream vision tasks.}\n\nMultiple existing methods take advantage of self-supervised tasks to guide the learning of representation for OOD detection. Previous work \\cite{csi, ssd} presents contrastive learning models as feature extractors. However, existing approaches of classifying transformed images according to contrastive learning possess similar limitations -- that is, the model tends to learn the specific patterns of categories \\cite{backdoor_attack, backdoor_survey}, which are beneficial for classification but do not help understand the intrinsic ID representation. In our work, we address this issue by performing the masked image modeling task for OOD detection.\n\n\\subsection{Training Strategy}\n\\revise{Numerous approaches have been developed to address OOD-awareness in training loss~\\cite{confbranch2018arxiv}. These methods often involve the introduction of regularization terms aimed at encouraging a clearer separation between ID and OOD features~\\cite{onedim21cvpr,huang2021mos}. In some cases, networks are augmented with confidence estimation branches, utilizing misclassified in-distribution examples as proxies for out-of-distribution ones~\\cite{confbranch2018arxiv}. MOS~\\cite{huang2021mos} adapts the loss function by incorporating a predefined group structure, enabling the minimum group-wise ``else\" class probability to serve as an indicator of OOD classification. An alternative approach~\\cite{onedim21cvpr} focuses on compelling ID samples to embed into a union of 1-dimensional subspaces during training, and it evaluates the minimum angular distance between the feature and class-wise subspaces. }\n\n\\revise{In contrast to these approaches, our method belongs to the lightweight training-free methods \\cite{vim, MOOD}, which doesn't necessitate retraining the model. Therefore, it not only offers a more straightforward application but also preserves the accuracy of in-distribution classification.}\n\n\n\\begin{figure}[t]\n  \\centering\n    \\subfloat[\\re{ID: ImageNet}]{\\includegraphics[width=0.99\\linewidth]{figures/compare_moodv1_imagenet.pdf}}\n    \n    \\subfloat[\\re{ID: CIFAR-10}]{\\includegraphics[width=0.99\\linewidth]{figures/compare_moodv1_cifar10.pdf}}\n    % \\includegraphics[width=0.99\\linewidth]{figures/compare_moodv1_imagenet.pdf}\n    \\caption{\\re{The AUROC (\\%) of MOODv2 and MOODv1 tested on ID datasets (a) ImageNet and (b) CIFAR-10. OOD datasets including OpenImage-O \\cite{openimages_o}, Texture \\cite{dtd}, iNaturalist \\cite{inaturalist}, and ImageNet-O \\cite{imagenet_o}. }}\n  \\label{fig:moodv1}\n\\end{figure}\n\n\\begin{figure*}[t]\n  \\centering\n    \\includegraphics[width=0.99\\linewidth, trim=20 10 20 10]{figures/plot_recover_images.pdf}\n    \\caption{\\revise{Each image pair consists of the original image (left) and reconstructed image (right). The rows of images are sourced from ImageNet \\cite{imagenet}, Texture \\cite{cimpoi14describing}, iNaturalist \\cite{van2018inaturalist}, ImageNet-O \\cite{hendrycks2021natural}, and OpenImage-O \\cite{openimages_o}. The number in the top left corner of each image pair represents the Euclidean distance between the two images.}}\n  \\label{fig:recover}\n\\end{figure*}\n\n\\begin{table}[t]\n\\small\n\\centering\n\n\\subfloat[ID: CIFAR-10]{\n\\setlength{\\tabcolsep}{1.1mm}\n\\begin{tabular}{c|ccccccccc}\n\\toprule\nMethods & prob & feat & logit & feat+logit \\\\\n\\midrule\nViT\\cite{vit} & \\revise{73.61\\scriptsize{$\\pm$21.36}} & \\revise{82.61\\scriptsize{$\\pm$23.81}} & \\revise{45.11\\scriptsize{$\\pm$4.45}} & \\revise{\\textbf{99.63}} \\\\\nMoCov3\\cite{mocov3} & \\revise{70.96\\scriptsize{$\\pm$23.68}} & \\revise{79.17\\scriptsize{$\\pm$28.75}} & \\revise{41.42\\scriptsize{$\\pm$3.50}} & \\revise{\\textbf{99.73}} \\\\\nDINOv2\\cite{dinov2} & \\revise{87.20\\scriptsize{$\\pm$10.62}} & \\revise{84.73\\scriptsize{$\\pm$21.57}} & \\revise{80.30\\scriptsize{$\\pm$0.10}} & \\revise{\\textbf{99.98}} \\\\\n\\rowcolor{gray!20}BEiTv2\\cite{beitv2} & \\revise{79.96\\scriptsize{$\\pm$13.71}} & \\revise{91.77\\scriptsize{$\\pm$11.47}} & \\revise{72.87\\scriptsize{$\\pm$2.08}} & \\revise{\\textbf{99.87}} \\\\\n\\rowcolor{gray!20}BEiT\\cite{beit} & \\revise{77.51\\scriptsize{$\\pm$17.83}} & \\revise{89.05\\scriptsize{$\\pm$15.46}} & \\revise{65.05\\scriptsize{$\\pm$2.06}} & \\revise{\\textbf{99.98}} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\subfloat[ID: ImageNet]{\n\\setlength{\\tabcolsep}{1.3mm}\n\\begin{tabular}{c|ccccccccc}\n\\toprule\nMethods & prob & feat & logit & feat+logit \\\\\n\\midrule\nViT\\cite{vit} & \\revise{\\textbf{78.52}\\scriptsize{$\\pm$1.76}} & \\revise{76.86\\scriptsize{$\\pm$3.20}} & \\revise{70.61\\scriptsize{$\\pm$4.76}} & \\revise{77.65} \\\\\nMoCov3\\cite{mocov3} & \\revise{\\textbf{78.36}\\scriptsize{$\\pm$1.42}} & \\revise{72.51\\scriptsize{$\\pm$6.21}} & \\revise{70.61\\scriptsize{$\\pm$5.04}} & \\revise{72.07} \\\\\nDINOv2\\cite{dinov2} & \\revise{59.64\\scriptsize{$\\pm$7.82}} & \\revise{\\textbf{63.56}\\scriptsize{$\\pm$2.89}} & \\revise{60.70\\scriptsize{$\\pm$4.51}} & \\revise{61.32} \\\\\n\\rowcolor{gray!20}BEiTv2\\cite{beitv2} & \\revise{89.07\\scriptsize{$\\pm$0.24}} & \\revise{92.96\\scriptsize{$\\pm$1.27}} & \\revise{90.29\\scriptsize{$\\pm$0.13}} & \\revise{\\textbf{95.42}} \\\\\n\\rowcolor{gray!20}BEiT\\cite{beit} & \\revise{89.47\\scriptsize{$\\pm$0.47}} & \\revise{93.30\\scriptsize{$\\pm$1.89}} & \\revise{89.84\\scriptsize{$\\pm$0.01}} & \\revise{\\textbf{95.68}} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\caption{\\revise{The AUROC (\\%) of four types of methods: probability-based methods MSP \\cite{baseline_ood} and KL-Matching \\cite{maxlogit}; logits-based methods Energy \\cite{energy} and MaxLogit \\cite{maxlogit}; features-based methods Residual \\cite{vim}, React \\cite{react} and Mahalanobis \\cite{mahalanobis}; and methods using both logits and features include ViM \\cite{vim}. The best method for each model is emphasized in bold.}\n}\n\\label{tab:ablation-statistic}\n\\end{table}\n\n\n\\subsection{MOODv1}\n\\revise{Our previous version MOODv1 \\cite{MOOD} has introduced masked image modeling pretraining strategy into the OOD detection (MOOD) and achieved promising results. However, there are still concerns:}\n\n\\revise{Firstly, previous studies \\cite{MOOD, csi, ssd} have typically necessitated fine-tuning a model on each in-distribution dataset. The expense of training becomes notably high when dealing with a substantial number of ID datasets to be assessed, such as in one-class OOD detection \\cite{csi, MOOD}. However, through experimental validation, we have discovered that a well-prepared masked image modeling model doesn't require additional fine-tuning to achieve outstanding detection performance, conserving substantial fine-tuning resource consumption when dealing with a plethora of ID datasets that require evaluation.}\n\n\\revise{Secondly, as the field has seen the emergence of more advanced OOD score functions \\cite{maxlogit, vim, react, baseline_ood, energy, mahalanobis} and pretraining techniques \\cite{beitv2, dinov2, mocov3, beit, vit}, it raises the question of whether masked image modeling continues to maintain its leading role. In MOODv2, we integrate the latest advancements in pretraining methods and conduct experiments with an array of state-of-the-art OOD score functions. This broader spectrum of pretraining methods and score functions allows for a more comprehensive assessment of the MOODv2's performance, better aligning MOODv2 with the increasingly intricate challenges of OOD detection. }\n\n\\revise{Lastly, it is well known that if the network has seen similar samples in training, regardless of pre-training or fine-tuning, the OOD performance will be more or less trivial \\cite{openimages_o}. Previous works \\cite{oodlimits, MOOD} rely on pre-training on ImageNet-21K, so that the benchmark OOD dataset such as CIFAR \\cite{cifar}, Places \\cite{places}, etc., is unlikely to be untouched by the ImageNet-21K \\cite{imagenet} dataset. In this work, MOODv2 introduces the latest unnatural datasets as OOD, which rules out the possibility of overlap between the OOD test set and the training set \\cite{openimages_o, imagenet_o}.}\n\n\n\\revise{In summary, MOODv2 incorporates improved score functions, advanced pretraining techniques, a wider range of unnatural OOD datasets, and a streamlined general framework. The performance improvement of MOODv2 compared to MOODv1 is depicted in Fig. \\ref{fig:moodv1}. On ImageNet, MOODv2 exhibits a noteworthy 2.17\\% improvement in AUROC compared to MOODv1. Furthermore, on CIFAR-10, MOODv2, without finetuning on the ID dataset, achieves an exceptional AUROC score of up to 99.98\\%. }\n\n\\section{Methods}\\label{sec:methods}\n\n\n\n% \\iffalse\n\\begin{table}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{0.5mm}\n\\begin{tabular}{c|cccccccccc|c}\n\\toprule\nDatasets & ImageNet & Texture & iNaturalist & ImageNet-O & OpenImage-O \\\\\n\\midrule\nDistance & 18.09 & 32.89 & 38.96 & 30.76 & 45.56 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Difference.} .}\n\\label{tab:difference}\n\\end{table}\n\\fi \n\n\\begin{table}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{3mm}\n\\begin{tabular}{c|cc}\n\\toprule\nID/OOD & Datasets & Recovery Distance \\\\\n\\midrule\nID & ImageNet (ID) \\cite{imagenet} & \\revise{18.09} \\\\\n\\midrule\n\\multirow{4}{*}{OOD} & Texture \\cite{dtd} & \\revise{32.89} \\\\\n& INaturalist \\cite{inaturalist}& \\revise{38.96} \\\\\n& ImageNet-O \\cite{imagenet_o} & \\revise{30.76} \\\\\n& OpenImage-O \\cite{openimages_o} & \\revise{45.56} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\revise{The average recovery distance (L2 norm between the original images and their corresponding reconstructions) for ID and OOD datasets. For each dataset, the number of sampled images is the minimum of 5000 and the dataset size.}}\n\\label{tab:difference}\n\\end{table}\n\n% We first define the notations. For a given dataset $X_{\\rm ID}$, the goal of out-of-distribution (OOD) detection is to model a detector that identifies whether an input image $x \\in X_{\\rm ID}$ or $x \\notin X_{\\rm ID}$ (that is, $x \\in X_{\\rm OOD}$). A majority of existing methods for OOD detection define an OOD score function $s(x)$. Its abnormal high or low value represents that $x$ is from out-of-distribution. \n\n\\revise{In this section, we initiate our exploration of reconstruction tasks for OOD detection by presenting the underlying motivation in \\cref{sec:motivation}. Following that, in \\cref{sec:pretask}, we delve into a comprehensive analysis of the essential attributes that play a pivotal role in OOD detection.}\n\n\\subsection{Motivation: seeking for effective ID representation} \n\\label{sec:motivation}\n\n% In this section, we choose the pretext task that can provide the intrinsic prior to suit the OOD detection task. \nMost previous OOD methods learn the ID representation through classification \\cite{baseline_ood, oodlimits} or contrastive learning \\cite{csi, ssd} on ID samples, which take advantage of either the ground truth or pseudo labels to supervise the classification networks. On the other hand, work of \\cite{backdoor_attack, frog_attack} shows that classification networks only learn different patterns among training categories because it is a shortcut to fulfill classification. \\revise{It is indicated that the network actually does not learn the effective in-distribution representation. In comparison, the reconstruction-based pretext task forces the network to learn the pixel-level image representation of the ID images during training to reconstruct the image instead of the patterns for classification. In this way, the network can learn a more representative feature of the ID dataset.}\n\n\\iffalse\n\\begin{equation}\nx_{r} = f_{e}\\cdot f_{d}(x),\n\\end{equation}\nwhere $x$ and $x_{r}$ represent the original and reconstructed images. $f_{e}$ and $f_{d}$ correspond to the image encoder and decoder, and $f_{e}$ needs to learn representative features from the training dataset to reconstruct images. To assess the benefits of using the pretask features, we compute the recovery distance between the original and reconstructed images as follows:\n\\begin{equation}\nd_r = \\left\\lVert x - x_{r} \\right\\rVert.\n\\end{equation}\n\\fi\n\\revise{To verify this, we reconstruct ID and OOD data and compute the Euclidean distance between the original and reconstructed images. A greater distance indicates a larger deviation of the reconstructed image from the original image. We collect recovery distances for ID and OOD data. Examples of the reconstruction are depicted in \\cref{fig:recover}. In the first row, for ID images, pre-trained models reconstruct the images effectively. Instead, for unnatural OOD images in the following rows, clear domain discrepancies emerge. For instance, in the case of textured images, the models still apply lighting and shadows reminiscent of natural images. In the case of sketch images,  the models render the images smoother and brighter. This discrepancy visually highlights the domain gap in model features between ID and OOD data, which can be leveraged for OOD detection.}\n\n\n\n\\begin{figure*}[t]\n  \\centering\n    \\subfloat[\\re{ID: CIFAR-10}]{\\includegraphics[width=0.99\\linewidth, trim=10 15 5 15]{figures/detailed_ablation_cifar10.pdf}}\n    \n    \\subfloat[\\re{ID: ImageNet}]{\\includegraphics[width=0.99\\linewidth, trim=10 15 5 15]{figures/detailed_ablation_imagenet.pdf}}\n\n    \\caption{\\revise{The AUROC (\\%) tested on unnatural OOD datasets of various OOD detection algorithms applied to a ViT model. \n    The pre-text tasks include classification task \\cite{vit}, contrastive learning tasks MoCov3 \\cite{mocov3} and DINOv2 \\cite{dinov2}, and masked image modeling tasks BEiT series \\cite{beit,beitv2}.\n    Methods in blue utilize the feature space;\n    methods in green use logits;\n    methods in yellow make use of the softmax probability.\n    and methods in red leverage both features and logits.\n    Stars represent the average AUROC for methods in the corresponding colors; light vertical lines represent the standard deviation.\n    }}\n    \n    % probability-based methods MSP \\cite{baseline_ood} and KL-Matching \\cite{maxlogit}; logits-based methods Energy \\cite{energy} and MaxLogit \\cite{maxlogit}; features-based methods Residual \\cite{vim}, React \\cite{react} and Mahalanobis \\cite{mahalanobis}; and methods using both logits and features include ViM \\cite{vim}.\n    \n  \\label{fig:detailed_ablation}\n\\end{figure*}\n\n\\subsection{Reconstruction Tasks for OOD Detection} \\label{sec:pretask}\n%--------------------------------------multi-class-----------------------------------\n\\begin{table*}[tp]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{0.6mm}\n\\begin{tabular}{c|c|ccccccccccc}\n\\toprule\n\\multirow{2}{*}{Methods} & \\multirow{2}{*}{Models} & \\multicolumn{2}{c}{Texture \\cite{dtd}} & \\multicolumn{2}{c}{iNaturalist \\cite{inaturalist}} & \\multicolumn{2}{c}{ImageNet-O \\cite{imagenet_o}} & \\multicolumn{2}{c}{ OpenImage-O \\cite{openimages_o}} & \\multicolumn{2}{c}{Average} \\\\\n& & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ \\\\\n\\midrule\n\\multirow{5}{*}{MSP\\cite{baseline_ood}}\n& ViT\\cite{vit} & \\revise{71.31} & \\revise{71.31} & \\revise{90.70} & \\revise{90.70} & \\revise{60.77} & \\revise{60.77} & \\revise{84.29} & \\revise{84.29} & \\revise{76.77} & \\revise{76.77} \\\\\n& MoCov3\\cite{mocov3} & \\revise{66.85} & \\revise{66.85} & \\revise{90.68} & \\revise{90.68} & \\revise{64.80} & \\revise{64.80} & \\revise{85.42} & \\revise{85.42} & \\revise{76.94} & \\revise{76.94} \\\\\n& DINOv2\\cite{dinov2} & \\revise{47.49} & \\revise{47.49} & \\revise{62.13} & \\revise{62.13} & \\revise{44.87} & \\revise{44.87} & \\revise{52.83} & \\revise{52.83} & \\revise{51.83} & \\revise{51.83} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.61}}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.61}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.05}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.05}}} & \\cellcolor{gray!20}{\\revise{81.15}} & \\cellcolor{gray!20}{\\revise{81.15}} & \\cellcolor{gray!20}{\\revise{\\textbf{92.52}}} & \\cellcolor{gray!20}{\\revise{\\textbf{92.52}}} & \\cellcolor{gray!20}{\\revise{88.83}} & \\cellcolor{gray!20}{\\revise{88.83}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{85.05}} & \\cellcolor{gray!20}{\\revise{85.05}} & \\cellcolor{gray!20}{\\revise{95.50}} & \\cellcolor{gray!20}{\\revise{95.50}} & \\cellcolor{gray!20}{\\revise{\\textbf{83.17}}} & \\cellcolor{gray!20}{\\revise{\\textbf{83.17}}} & \\cellcolor{gray!20}{\\revise{92.28}} & \\cellcolor{gray!20}{\\revise{92.28}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.00}}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.00}}} \\\\\n\\midrule\n\\multirow{5}{*}{Energy\\cite{energy}}\n& ViT\\cite{vit} & \\revise{54.11} & \\revise{54.11} & \\revise{76.61} & \\revise{76.61} & \\revise{61.63} & \\revise{61.63} & \\revise{71.06} & \\revise{71.06} & \\revise{65.85} & \\revise{65.85} \\\\\n& MoCov3\\cite{mocov3} & \\revise{48.79} & \\revise{48.79} & \\revise{76.80} & \\revise{76.80} & \\revise{64.56} & \\revise{64.56} & \\revise{72.13} & \\revise{72.13} & \\revise{65.57} & \\revise{65.57} \\\\\n& DINOv2\\cite{dinov2} & \\revise{73.89} & \\revise{73.89} & \\revise{80.34} & \\revise{80.34} & \\revise{49.98} & \\revise{49.98} & \\revise{56.64} & \\revise{56.64} & \\revise{65.21} & \\revise{65.21} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.32}}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.32}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.95}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.95}}} & \\cellcolor{gray!20}{\\revise{85.27}} & \\cellcolor{gray!20}{\\revise{85.27}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.14}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.14}}} & \\cellcolor{gray!20}{\\revise{\\textbf{90.42}}} & \\cellcolor{gray!20}{\\revise{\\textbf{90.42}}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{83.04}} & \\cellcolor{gray!20}{\\revise{83.04}} & \\cellcolor{gray!20}{\\revise{96.48}} & \\cellcolor{gray!20}{\\revise{96.48}} & \\cellcolor{gray!20}{\\revise{\\textbf{86.36}}} & \\cellcolor{gray!20}{\\revise{\\textbf{86.36}}} & \\cellcolor{gray!20}{\\revise{93.50}} & \\cellcolor{gray!20}{\\revise{93.50}} & \\cellcolor{gray!20}{\\revise{89.85}} & \\cellcolor{gray!20}{\\revise{89.85}} \\\\\n\\midrule\n\\multirow{5}{*}{MaxLogit\\cite{maxlogit}}\n& ViT\\cite{vit} & \\revise{67.22} & \\revise{67.22} & \\revise{89.88} & \\revise{89.88} & \\revise{61.68} & \\revise{61.68} & \\revise{82.73} & \\revise{82.73} & \\revise{75.37} & \\revise{75.37} \\\\\n& MoCov3\\cite{mocov3} & \\revise{62.36} & \\revise{62.36} & \\revise{90.38} & \\revise{90.38} & \\revise{65.65} & \\revise{65.65} & \\revise{84.19} & \\revise{84.19} & \\revise{75.64} & \\revise{75.64} \\\\\n& DINOv2\\cite{dinov2} & \\revise{54.70} & \\revise{54.70} & \\revise{69.98} & \\revise{69.98} & \\revise{45.60} & \\revise{45.60} & \\revise{54.52} & \\revise{54.52} & \\revise{56.20} & \\revise{56.20} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.94}}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.94}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.90}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.90}}} & \\cellcolor{gray!20}{\\revise{83.97}} & \\cellcolor{gray!20}{\\revise{83.97}} & \\cellcolor{gray!20}{\\revise{\\textbf{93.82}}} & \\cellcolor{gray!20}{\\revise{\\textbf{93.82}}} & \\cellcolor{gray!20}{\\revise{\\textbf{90.16}}} & \\cellcolor{gray!20}{\\revise{\\textbf{90.16}}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{84.17}} & \\cellcolor{gray!20}{\\revise{84.17}} & \\cellcolor{gray!20}{\\revise{96.48}} & \\cellcolor{gray!20}{\\revise{96.48}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.34}}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.34}}} & \\cellcolor{gray!20}{\\revise{93.31}} & \\cellcolor{gray!20}{\\revise{93.31}} & \\cellcolor{gray!20}{\\revise{89.83}} & \\cellcolor{gray!20}{\\revise{89.83}} \\\\\n\\midrule\n\\multirow{5}{*}{KL-Matching\\cite{maxlogit}}\n& ViT\\cite{vit} & \\revise{82.59} & \\revise{82.59} & \\revise{87.63} & \\revise{87.63} & \\revise{66.55} & \\revise{66.55} & \\revise{84.34} & \\revise{84.34} & \\revise{80.28} & \\revise{80.28} \\\\\n& MoCov3\\cite{mocov3} & \\revise{82.35} & \\revise{82.35} & \\revise{86.24} & \\revise{86.24} & \\revise{67.80} & \\revise{67.80} & \\revise{82.73} & \\revise{82.73} & \\revise{79.78} & \\revise{79.78} \\\\\n& DINOv2\\cite{dinov2} & \\revise{80.51} & \\revise{80.51} & \\revise{56.93} & \\revise{56.93} & \\revise{69.77} & \\revise{69.77} & \\revise{62.63} & \\revise{62.63} & \\revise{67.46} & \\revise{67.46} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{87.14}} & \\cellcolor{gray!20}{\\revise{87.14}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.13}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.13}}} & \\cellcolor{gray!20}{\\revise{82.87}} & \\cellcolor{gray!20}{\\revise{82.87}} & \\cellcolor{gray!20}{\\revise{92.10}} & \\cellcolor{gray!20}{\\revise{92.10}} & \\cellcolor{gray!20}{\\revise{89.31}} & \\cellcolor{gray!20}{\\revise{89.31}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{\\textbf{87.87}}} & \\cellcolor{gray!20}{\\revise{\\textbf{87.87}}} & \\cellcolor{gray!20}{\\revise{94.82}} & \\cellcolor{gray!20}{\\revise{94.82}} & \\cellcolor{gray!20}{\\revise{\\textbf{84.56}}} & \\cellcolor{gray!20}{\\revise{\\textbf{84.56}}} & \\cellcolor{gray!20}{\\revise{\\textbf{92.48}}} & \\cellcolor{gray!20}{\\revise{\\textbf{92.48}}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.93}}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.93}}} \\\\\n\\midrule\n\\multirow{5}{*}{Residual\\cite{vim}}\n& ViT\\cite{vit} & \\revise{82.39} & \\revise{82.39} & \\revise{73.72} & \\revise{73.72} & \\revise{68.44} & \\revise{68.44} & \\revise{74.88} & \\revise{74.88} & \\revise{74.86} & \\revise{74.86} \\\\\n& MoCov3\\cite{mocov3} & \\revise{75.25} & \\revise{75.25} & \\revise{73.80} & \\revise{73.80} & \\revise{57.69} & \\revise{57.69} & \\revise{67.82} & \\revise{67.82} & \\revise{68.64} & \\revise{68.64} \\\\\n& DINOv2\\cite{dinov2} & \\revise{66.50} & \\revise{66.50} & \\revise{61.90} & \\revise{61.90} & \\revise{58.94} & \\revise{58.94} & \\revise{56.84} & \\revise{56.84} & \\revise{61.04} & \\revise{61.04} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.99}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.99}}} & \\cellcolor{gray!20}{\\revise{99.01}} & \\cellcolor{gray!20}{\\revise{99.01}} & \\cellcolor{gray!20}{\\revise{87.23}} & \\cellcolor{gray!20}{\\revise{87.23}} & \\cellcolor{gray!20}{\\revise{95.43}} & \\cellcolor{gray!20}{\\revise{95.43}} & \\cellcolor{gray!20}{\\revise{94.17}} & \\cellcolor{gray!20}{\\revise{94.17}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{94.16}} & \\cellcolor{gray!20}{\\revise{94.16}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.50}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.50}}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.35}}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.35}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.52}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.52}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.88}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.88}}} \\\\\n\\midrule\n\\multirow{5}{*}{React\\cite{react}}\n& ViT\\cite{vit} & \\revise{62.09} & \\revise{62.09} & \\revise{91.20} & \\revise{91.20} & \\revise{63.66} & \\revise{63.66} & \\revise{80.43} & \\revise{80.43} & \\revise{74.34} & \\revise{74.34} \\\\\n& MoCov3\\cite{mocov3} & \\revise{51.47} & \\revise{51.47} & \\revise{79.30} & \\revise{79.30} & \\revise{65.33} & \\revise{65.33} & \\revise{74.35} & \\revise{74.35} & \\revise{67.61} & \\revise{67.61} \\\\\n& DINOv2\\cite{dinov2} & \\revise{76.73} & \\revise{76.73} & \\revise{74.25} & \\revise{74.25} & \\revise{56.26} & \\revise{56.26} & \\revise{63.17} & \\revise{63.17} & \\revise{67.60} & \\revise{67.60} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{86.10}}} & \\cellcolor{gray!20}{\\revise{\\textbf{86.10}}} & \\cellcolor{gray!20}{\\revise{\\textbf{98.09}}} & \\cellcolor{gray!20}{\\revise{\\textbf{98.09}}} & \\cellcolor{gray!20}{\\revise{85.69}} & \\cellcolor{gray!20}{\\revise{85.69}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.96}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.96}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.21}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.21}}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{84.32}} & \\cellcolor{gray!20}{\\revise{84.32}} & \\cellcolor{gray!20}{\\revise{96.99}} & \\cellcolor{gray!20}{\\revise{96.99}} & \\cellcolor{gray!20}{\\revise{\\textbf{87.04}}} & \\cellcolor{gray!20}{\\revise{\\textbf{87.04}}} & \\cellcolor{gray!20}{\\revise{94.21}} & \\cellcolor{gray!20}{\\revise{94.21}} & \\cellcolor{gray!20}{\\revise{90.64}} & \\cellcolor{gray!20}{\\revise{90.64}} \\\\\n\\midrule\n\\multirow{5}{*}{Mahalanobis\\cite{mahalanobis}}\n& ViT\\cite{vit} & \\revise{84.93} & \\revise{84.93} & \\revise{84.90} & \\revise{84.90} & \\revise{71.53} & \\revise{71.53} & \\revise{84.16} & \\revise{84.16} & \\revise{81.38} & \\revise{81.38} \\\\\n& MoCov3\\cite{mocov3} & \\revise{84.29} & \\revise{84.29} & \\revise{86.95} & \\revise{86.95} & \\revise{70.33} & \\revise{70.33} & \\revise{83.54} & \\revise{83.54} & \\revise{81.28} & \\revise{81.28} \\\\\n& DINOv2\\cite{dinov2} & \\revise{68.58} & \\revise{68.58} & \\revise{63.14} & \\revise{63.14} & \\revise{58.86} & \\revise{58.86} & \\revise{57.57} & \\revise{57.57} & \\revise{62.04} & \\revise{62.04} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{93.01}} & \\cellcolor{gray!20}{\\revise{93.01}} & \\cellcolor{gray!20}{\\revise{98.78}} & \\cellcolor{gray!20}{\\revise{98.78}} & \\cellcolor{gray!20}{\\revise{86.78}} & \\cellcolor{gray!20}{\\revise{86.78}} & \\cellcolor{gray!20}{\\revise{95.46}} & \\cellcolor{gray!20}{\\revise{95.46}} & \\cellcolor{gray!20}{\\revise{93.51}} & \\cellcolor{gray!20}{\\revise{93.51}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{\\textbf{93.03}}} & \\cellcolor{gray!20}{\\revise{\\textbf{93.03}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.18}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.18}}} & \\cellcolor{gray!20}{\\revise{\\textbf{88.84}}} & \\cellcolor{gray!20}{\\revise{\\textbf{88.84}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.51}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.51}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.39}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.39}}} \\\\\n\\midrule\n\\multirow{5}{*}{ViM\\cite{vim}}\n& ViT\\cite{vit} & \\revise{83.51} & \\revise{83.51} & \\revise{77.75} & \\revise{77.75} & \\revise{71.04} & \\revise{71.04} & \\revise{78.31} & \\revise{78.31} & \\revise{77.65} & \\revise{77.65} \\\\\n& MoCov3\\cite{mocov3} & \\revise{76.28} & \\revise{76.28} & \\revise{78.18} & \\revise{78.18} & \\revise{61.35} & \\revise{61.35} & \\revise{72.46} & \\revise{72.46} & \\revise{72.07} & \\revise{72.07} \\\\\n& DINOv2\\cite{dinov2} & \\revise{66.90} & \\revise{66.90} & \\revise{62.53} & \\revise{62.53} & \\revise{58.93} & \\revise{58.93} & \\revise{56.93} & \\revise{56.93} & \\revise{61.32} & \\revise{61.32} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.35}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.35}}} & \\cellcolor{gray!20}{\\revise{99.31}} & \\cellcolor{gray!20}{\\revise{99.31}} & \\cellcolor{gray!20}{\\revise{90.06}} & \\cellcolor{gray!20}{\\revise{90.06}} & \\cellcolor{gray!20}{\\revise{96.96}} & \\cellcolor{gray!20}{\\revise{96.96}} & \\cellcolor{gray!20}{\\revise{95.42}} & \\cellcolor{gray!20}{\\revise{95.42}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{94.25}} & \\cellcolor{gray!20}{\\revise{94.25}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.59}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.59}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.47}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.47}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.41}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.41}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.68}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.68}}} \\\\\n\\midrule\n\\multirow{5}{*}{Average}\n& ViT\\cite{vit} & \\revise{73.52} & \\revise{73.52} & \\revise{84.05} & \\revise{84.05} & \\revise{65.66} & \\revise{65.66} & \\revise{80.02} & \\revise{80.02} & \\revise{75.81} & \\revise{75.81} \\\\\n& MoCov3\\cite{mocov3} & \\revise{68.45} & \\revise{68.45} & \\revise{82.79} & \\revise{82.79} & \\revise{64.69} & \\revise{64.69} & \\revise{77.83} & \\revise{77.83} & \\revise{73.44} & \\revise{73.44} \\\\\n& DINOv2\\cite{dinov2} & \\revise{66.91} & \\revise{66.91} & \\revise{66.40} & \\revise{66.40} & \\revise{55.40} & \\revise{55.40} & \\revise{57.64} & \\revise{57.64} & \\revise{61.59} & \\revise{61.59} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.18}}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.18}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.53}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.53}}} & \\cellcolor{gray!20}{\\revise{85.38}} & \\cellcolor{gray!20}{\\revise{85.38}} & \\cellcolor{gray!20}{\\revise{94.42}} & \\cellcolor{gray!20}{\\revise{94.42}} & \\cellcolor{gray!20}{\\revise{91.63}} & \\cellcolor{gray!20}{\\revise{91.63}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{88.24}} & \\cellcolor{gray!20}{\\revise{88.24}} & \\cellcolor{gray!20}{\\revise{97.32}} & \\cellcolor{gray!20}{\\revise{97.32}} & \\cellcolor{gray!20}{\\revise{\\textbf{87.02}}} & \\cellcolor{gray!20}{\\revise{\\textbf{87.02}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.53}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.53}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.77}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.77}}} \\\\\n\\midrule\n\\multirow{5}{*}{Best}\n& ViT\\cite{vit} & \\revise{84.93} & \\revise{84.93} & \\revise{91.20} & \\revise{91.20} & \\revise{71.53} & \\revise{71.53} & \\revise{84.34} & \\revise{84.34} & \\revise{81.38} & \\revise{81.38} \\\\\n& MoCov3\\cite{mocov3} & \\revise{84.29} & \\revise{84.29} & \\revise{90.68} & \\revise{90.68} & \\revise{70.33} & \\revise{70.33} & \\revise{85.42} & \\revise{85.42} & \\revise{81.28} & \\revise{81.28} \\\\\n& DINOv2\\cite{dinov2} & \\revise{80.51} & \\revise{80.51} & \\revise{80.34} & \\revise{80.34} & \\revise{69.77} & \\revise{69.77} & \\revise{63.17} & \\revise{63.17} & \\revise{67.60} & \\revise{67.60} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.35}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.35}}} & \\cellcolor{gray!20}{\\revise{99.31}} & \\cellcolor{gray!20}{\\revise{99.31}} & \\cellcolor{gray!20}{\\revise{90.06}} & \\cellcolor{gray!20}{\\revise{90.06}} & \\cellcolor{gray!20}{\\revise{96.96}} & \\cellcolor{gray!20}{\\revise{96.96}} & \\cellcolor{gray!20}{\\revise{95.42}} & \\cellcolor{gray!20}{\\revise{95.42}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{94.25}} & \\cellcolor{gray!20}{\\revise{94.25}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.59}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.59}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.47}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.47}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.41}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.41}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.68}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.68}}} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\n\\revise{Performance of OOD detection methods on ViT-B/16 model with $224\\times224$-pixel inputs. The pre-text tasks include classification task \\cite{vit}, contrastive learning tasks MoCov3 \\cite{mocov3} and DINOv2 \\cite{dinov2}, and masked image modeling tasks BEiT \\cite{beit} and BEiTv2 \\cite{beitv2}. All models are per-trained on ImageNet-21k and finetuned on ImageNet-1k. Both metrics AUROC and FPR95 are in percentage.\nThe best method is emphasized in bold and a gray background indicates our choice.}\n}\n\\label{tab:multi-class-imagenet-ablation}\n\\end{table*}\n\n\n\n\\revise{In this section, we offer a comprehensive analysis of these key elements in the context of OOD detection. We employ ImageNet \\cite{imagenet} as the in-distribution dataset and evaluate pre-task texts on challenging unnatural out-of-distribution datasets, including OpenImage-O \\cite{openimages_o}, Texture \\cite{dtd}, iNaturalist \\cite{inaturalist}, and ImageNet-O \\cite{imagenet_o}. Extensive validations with various pretraining methods and OOD score functions, \\re{including {MSP} \\cite{baseline_ood}, {Energy} \\cite{energy}, {MaxLogit} \\cite{maxlogit}, {KL Matching} \\cite{maxlogit}, {Residual} \\cite{vim}, {ReAct} \\cite{react}, {Mahalanobis} \\cite{mahalanobis} and {ViM} \\cite{vim}. }}\n\n\\revise{Results are shown in ~\\cref{tab:multi-class-imagenet-ablation}. The results indicate that the masked image modeling pretext task surpasses classification and contrastive learning pretext tasks when employing all included score functions. The average AUROC across these score functions exhibits an improvement of 15.96\\%  compared to the competition. Models when using the best-performing score function saw a 14.30\\% increase in performance. This remarkable achievement can be attributed to the representative ID feature space representation, thereby aiding in distinguishing between ID and OOD data. This discovery is highly significant as it enhances performance across mainstream OOD detection score functions, thus advancing the entire field. We also employ CIFAR-10 \\cite{cifar} as the ID dataset and provide results in the appendix. Our approach attains an impressive AUROC of 99.99\\% while concurrently reducing the FPR95 to a mere 0.03\\%.}\n\n\\revise{To enhance the comprehensibility of our experimental findings, we conduct a thorough statistical analysis and illustrate them in visual representations. The outcomes are depicted in \\cref{fig:detailed_ablation}. Our approach not only leads to an overall enhancement in results but also notably minimizes the variations among different methods. For instance, the ViT, MoCov3, and DINOv2 models using logit-based methods exhibited standard deviations of 4.76\\%, 5.04\\%, and 4.51\\%, respectively, while BEiT and BEiTv2 displayed significantly lower standard deviations, reaching as low as 0.13\\% and 0.01\\%. This observation underscores that even uncomplicated score functions can perform equivalently to more intricate ones when an effective ID feature representation is applied.}\n\n\\revise{In \\cref{tab:ablation-statistic}, we underscore the optimal methods for each model. On CIFAR-10, all models achieved their best results when employing the feat and logit combination approach, achieving almost 100\\% accuracy. This suggests a highly effective grasp of CIFAR-10's feature space. Conversely, with the larger ImageNet dataset, we observed variations in outcomes. Notably, the masked image modeling pretext-pretrained model achieved the best results when using the feat and logit combination method, while other models excelled in probability-based and feature-based methods. Additionally, our masked image modeling pretext demonstrated significantly superior performance compared to other pretraining methods, underscoring the limitations of classification-based pretraining strategies and their inadequacy in harnessing advanced score functions effectively. These discoveries reinforce the pivotal role of proficient feature representation in OOD detection. Furthermore, for more detailed information, we provide illustrations of the distribution curves of OOD scores for both ID and OOD datasets in the appendix.}\n\n\\subsection{Masked Image Modeling for Out-of-Distribution v2} \n\\label{sec:method}\n\\revise{To sum up, in this section, we observed that pre-trained models adeptly reconstruct ID images, yet manifest distinctive domain differences in the OOD scenario (\\cref{fig:recover}). This visual incongruity starkly highlights the prevailing domain gap in model features between ID and OOD data. Additionally, a thorough analysis of experimental outcomes reveals that the pre-task of masked image modeling not only significantly enhances overall results but also markedly diminishes disparities among score functions. These findings emphasize the crucial significance of effective feature representation in OOD detection, highlighting the enhancement of features through masked image modeling tasks.}\n\n\\revise{Finally, we propose our Masked Image Modeling for Out-of-Distribution Detection v2 (MOODv2). The algorithm of is shown in \\cref{alg1}, mainly including the following stages.}\n\n\\begin{enumerate}\n\\item \\revise{Pre-train the vision encoder with masked image modeling on the pretrain dataset. }\n\\item \\revise{Apply fine-tuning the backbone on the in-distribution dataset.}\n\\item \\revise{Extract features from the trained image encoder and calculate the OOD score distance for OOD detection.}\n\\end{enumerate}\n\n\\revise{In terms of the OOD score function, we adopt  ViM\\cite{vim} that combines features and logits, leveraging insights from the masked image modeling pre-trained model, which has demonstrated superior performance. Mathematically, the score is\n\\begin{equation}\n    \\text{s}(x) = \\frac{e^{\\alpha\\sqrt{x^T R R^Tx}}}{\\sum_{i=1}^C e^{l_i} + e^{\\alpha\\sqrt{x^TRR^Tx}}}.\n\\end{equation}\nwhere $l_i$ is the $i$-th logit of feature $x$ in the training set $X$; $\\alpha$ is a per-model constant; $R\\in\\mathbb{R}^{N\\times(N-D)}$ is the $(D+1)$-th column to the last column of the eigenvector matrix $Q$ of $X$ and $N$ is the principal dimension; $C$ is the number of classes.}\n\n\n%--------------------------------------algorithm-----------------------------------\n\\begin{algorithm}[t!]\n\\caption{MOODv2 Detection Algorithm}\n\\label{alg1}\n\\small\n\\begin{algorithmic}[1]\n\n\\Require Pre-train set $X_P$, in-distribution set $X_{\\rm ID}$, test set $X_{\\rm test}$, , required True Positive Rate $\\eta$\\%, backbone $f$.\n\\Ensure Is $x_{\\rm test}$ outlier or not? $\\forall x_{\\rm test} \\in X_{\\rm test}$.\n\n\\State Pre-train $f$ on $X_P$ by maximizing\n$$\\sum_{x\\in X_P}\\mathbb{E}_M\\left [\\sum_{i\\in M}\\log p_{\\rm MIM}(z|x^M)\\right ]$$.\n\\State Fine-tune $f$ on $X_P$ by minimizing\n$$L_{\\rm ft}=\\sum_{x_p\\in X_P}{\\rm CrossEntropy}(f(x_p), y_P(x_p))$$ \n\\State Calculate $d(x_{\\rm test})$ for $x_{\\rm test} \\in X_{\\rm test}$ and \\re{$d(x_{\\rm ID})$ for $x_{\\rm ID}\\in X_{\\rm ID}$.}\n\\State Compute threshold $T$ as the $\\eta$ percentile of \\re{$d(x_{\\rm ID})$.}\n\\If {$d(x_{\\rm test})>T$}\n\\State $x_{\\rm test}$ is an outlier.\n\\EndIf\n\\end{algorithmic}\n\\end{algorithm}\n\n%--------------------------------------multi-class-----------------------------------\n\\begin{table*}[t!]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{0.8mm}\n\\begin{tabular}{c|c|ccccccccccc}\n\\toprule\n\\multirow{2}{*}{ID data} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c}{Texture \\cite{dtd}} & \\multicolumn{2}{c}{iNaturalist \\cite{inaturalist}} & \\multicolumn{2}{c}{ImageNet-O \\cite{imagenet_o}} & \\multicolumn{2}{c}{ OpenImage-O \\cite{openimages_o}} & \\multicolumn{2}{c}{Average} \\\\\n& & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ \\\\\n\\midrule\n\\multirow{10}{*}{CIFAR-10}\n& MSP\\cite{baseline_ood} & \\revise{45.67} & \\revise{95.17} & \\revise{71.07} & \\revise{81.76} & \\revise{32.52} & \\revise{98.85} & \\revise{59.74} & \\revise{91.45} & \\revise{52.25} & \\revise{91.81} \\\\\n& Energy\\cite{energy} & \\revise{31.16} & \\revise{97.89} & \\revise{48.95} & \\revise{97.92} & \\revise{37.22} & \\revise{97.85} & \\revise{45.29} & \\revise{96.36} & \\revise{40.65} & \\revise{97.50} \\\\\n& MaxLogit\\cite{maxlogit} & \\revise{41.21} & \\revise{95.95} & \\revise{67.83} & \\revise{86.04} & \\revise{32.58} & \\revise{98.80} & \\revise{56.64} & \\revise{92.94} & \\revise{49.56} & \\revise{93.43} \\\\\n& KL-Matching\\cite{maxlogit} & \\revise{98.00} & \\revise{10.64} & \\revise{94.23} & \\revise{35.86} & \\revise{92.99} & \\revise{32.40} & \\revise{94.68} & \\revise{27.92} & \\revise{94.97} & \\revise{26.71} \\\\\n& Residual\\cite{vim} & \\revise{99.91} & \\revise{0.21} & \\revise{99.68} & \\revise{0.45} & \\revise{99.36} & \\revise{2.85} & \\revise{99.42} & \\revise{2.46} & \\revise{99.59} & \\revise{1.49} \\\\\n& React\\cite{react} & \\revise{35.97} & \\revise{96.26} & \\revise{69.01} & \\revise{87.91} & \\revise{36.65} & \\revise{97.75} & \\revise{54.14} & \\revise{93.11} & \\revise{48.94} & \\revise{93.76} \\\\\n& Mahalanobis\\cite{mahalanobis} & \\revise{99.77} & \\revise{0.60} & \\revise{99.39} & \\revise{1.11} & \\revise{98.93} & \\revise{4.90} & \\revise{99.14} & \\revise{3.26} & \\revise{99.31} & \\revise{2.47} \\\\\n& ViM\\cite{vim} & \\revise{99.91} & \\revise{0.23} & \\revise{99.72} & \\revise{0.38} & \\revise{99.38} & \\revise{2.65} & \\revise{99.49} & \\revise{2.31} & \\revise{99.63} & \\revise{1.39} \\\\\n& \\cellcolor{gray!20}{MOODv1\\cite{MOOD}} & \\cellcolor{gray!20}{\\revise{99.95}} & \\cellcolor{gray!20}{\\revise{\\textbf{0.06}}} & \\cellcolor{gray!20}{\\revise{99.99}} & \\cellcolor{gray!20}{\\revise{0.02}} & \\cellcolor{gray!20}{\\revise{99.61}} & \\cellcolor{gray!20}{\\revise{1.90}} & \\cellcolor{gray!20}{\\revise{99.82}} & \\cellcolor{gray!20}{\\revise{0.77}} & \\cellcolor{gray!20}{\\revise{99.84}} & \\cellcolor{gray!20}{\\revise{0.69}} \\\\\n& \\cellcolor{gray!20}{MOODv2 (ours)} & \\cellcolor{gray!20}{\\revise{\\textbf{99.98}}} & \\cellcolor{gray!20}{\\revise{\\textbf{0.06}}} & \\cellcolor{gray!20}{\\revise{\\textbf{100.00}}} & \\cellcolor{gray!20}{\\revise{\\textbf{0.00}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.94}}} & \\cellcolor{gray!20}{\\revise{\\textbf{0.20}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.99}}} & \\cellcolor{gray!20}{\\revise{\\textbf{0.01}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.98}}} & \\cellcolor{gray!20}{\\revise{\\textbf{0.07}}} \\\\\n\\midrule\n\\multirow{10}{*}{ImageNet}\n& MSP\\cite{baseline_ood} & \\revise{71.31} & \\revise{77.07} & \\revise{90.70} & \\revise{43.72} & \\revise{60.77} & \\revise{90.60} & \\revise{84.29} & \\revise{61.79} & \\revise{76.77} & \\revise{68.30} \\\\\n& Energy\\cite{energy} & \\revise{54.11} & \\revise{86.28} & \\revise{76.61} & \\revise{72.70} & \\revise{61.63} & \\revise{81.00} & \\revise{71.06} & \\revise{73.99} & \\revise{65.85} & \\revise{78.49} \\\\\n& MaxLogit\\cite{maxlogit} & \\revise{67.22} & \\revise{77.98} & \\revise{89.88} & \\revise{45.57} & \\revise{61.68} & \\revise{88.60} & \\revise{82.73} & \\revise{62.52} & \\revise{75.37} & \\revise{68.67} \\\\\n& KL-Matching\\cite{maxlogit} & \\revise{82.59} & \\revise{67.27} & \\revise{87.63} & \\revise{69.71} & \\revise{66.55} & \\revise{88.15} & \\revise{84.34} & \\revise{74.23} & \\revise{80.28} & \\revise{74.84} \\\\\n& Residual\\cite{vim} & \\revise{82.39} & \\revise{64.61} & \\revise{73.72} & \\revise{86.00} & \\revise{68.44} & \\revise{87.45} & \\revise{74.88} & \\revise{77.98} & \\revise{74.86} & \\revise{79.01} \\\\\n& React\\cite{react} & \\revise{62.09} & \\revise{80.47} & \\revise{91.20} & \\revise{38.74} & \\revise{63.66} & \\revise{81.00} & \\revise{80.43} & \\revise{60.41} & \\revise{74.34} & \\revise{65.15} \\\\\n& Mahalanobis\\cite{mahalanobis} & \\revise{84.93} & \\revise{66.05} & \\revise{84.90} & \\revise{81.60} & \\revise{71.53} & \\revise{88.85} & \\revise{84.16} & \\revise{74.72} & \\revise{81.38} & \\revise{77.80} \\\\\n& ViM\\cite{vim} & \\revise{83.51} & \\revise{62.71} & \\revise{77.75} & \\revise{81.72} & \\revise{71.04} & \\revise{86.60} & \\revise{78.31} & \\revise{74.55} & \\revise{77.65} & \\revise{76.40} \\\\\n& \\cellcolor{gray!20}{MOODv1\\cite{MOOD}} & \\cellcolor{gray!20}{\\revise{93.01}} & \\cellcolor{gray!20}{\\revise{30.91}} & \\cellcolor{gray!20}{\\revise{98.78}} & \\cellcolor{gray!20}{\\revise{5.89}} & \\cellcolor{gray!20}{\\revise{86.78}} & \\cellcolor{gray!20}{\\revise{63.15}} & \\cellcolor{gray!20}{\\revise{95.46}} & \\cellcolor{gray!20}{\\revise{26.46}} & \\cellcolor{gray!20}{\\revise{93.51}} & \\cellcolor{gray!20}{\\revise{31.60}} \\\\\n& \\cellcolor{gray!20}{MOODv2 (ours)} & \\cellcolor{gray!20}{\\revise{\\textbf{94.25}}} & \\cellcolor{gray!20}{\\revise{\\textbf{24.69}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.59}}} & \\cellcolor{gray!20}{\\revise{\\textbf{1.83}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.47}}} & \\cellcolor{gray!20}{\\revise{\\textbf{40.80}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.41}}} & \\cellcolor{gray!20}{\\revise{\\textbf{13.55}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.68}}} & \\cellcolor{gray!20}{\\revise{\\textbf{20.22}}} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\n\\revise{Performance of OOD detection methods on ViT-B/16 model with $224\\times224$-pixel inputs. All methods are pre-trained on ImageNet-21k and finetuned on ImageNet-1k. ID datasets include CIFAR-10 \\cite{cifar} and ImageNet-1k \\cite{imagenet}. Both metrics AUROC and FPR95 are in percentage. The best method is emphasized in bold and a gray background indicates our methods.}\n}\n\\label{tab:multi-class}\n\\end{table*}\n\n\\begin{table*}[t]\n\\small\n\\centering\n\n\\subfloat[AUROC]{\n\\setlength{\\tabcolsep}{2.8mm}\n\\begin{tabular}{c|cccccccccc|c}\n\\toprule\n\\multirow{2}{*}{Methods} & \\multicolumn{10}{c|}{ID class} & \\multirow{2}{*}{Average} \\\\\n% & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & \\\\\n& Plane & Car & Bird & Cat & Deer & Dog & Frog & Horse & Ship & Truck & \\\\\n\\midrule\nKL-Matching\\cite{maxlogit} & \\revise{95.35} & \\revise{92.04} & \\revise{95.18} & \\revise{91.26} & \\revise{88.11} & \\revise{94.66} & \\revise{94.99} & \\revise{86.52} & \\revise{93.61} & \\revise{89.37} & \\revise{92.11} \\\\\nResidual\\cite{vim} & \\revise{97.62} & \\revise{95.88} & \\revise{97.06} & \\revise{96.30} & \\revise{89.18} & \\revise{94.33} & \\revise{96.73} & \\revise{91.46} & \\revise{94.89} & \\revise{92.36} & \\revise{94.58} \\\\\nMahalanobis\\cite{mahalanobis} & \\revise{97.52} & \\revise{96.07} & \\revise{96.77} & \\revise{96.41} & \\revise{89.60} & \\revise{94.79} & \\revise{96.41} & \\revise{91.48} & \\revise{94.80} & \\revise{92.58} & \\revise{94.64} \\\\\nViM\\cite{vim} & \\revise{97.61} & \\revise{96.36} & \\revise{97.19} & \\revise{96.50} & \\revise{88.78} & \\revise{94.21} & \\revise{96.70} & \\revise{91.60} & \\revise{94.97} & \\revise{92.35} & \\revise{94.63} \\\\\n\\rowcolor{gray!20}MOODv1\\cite{MOOD} & \\revise{98.63} & \\revise{\\textbf{99.33}} & \\revise{94.31} & \\revise{93.22} & \\revise{\\textbf{98.11}} & \\revise{96.50} & \\revise{\\textbf{99.25}} & \\revise{\\textbf{98.96}} & \\revise{\\textbf{98.76}} & \\revise{\\textbf{97.82}} & \\revise{97.83} \\\\\n\\rowcolor{gray!20}MOODv2 (ours) & \\revise{\\textbf{99.14}} & \\revise{99.03} & \\revise{\\textbf{99.51}} & \\revise{\\textbf{98.37}} & \\revise{97.12} & \\revise{\\textbf{97.20}} & \\revise{98.53} & \\revise{98.07} & \\revise{98.35} & \\revise{96.68} & \\revise{\\textbf{98.20}} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\subfloat[FPR95]{\n\\setlength{\\tabcolsep}{2.8mm}\n\\begin{tabular}{c|cccccccccc|c}\n\\toprule\n\\multirow{2}{*}{Methods} & \\multicolumn{10}{c|}{ID class} & \\multirow{2}{*}{Average} \\\\\n% & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & \\\\\n& Plane & Car & Bird & Cat & Deer & Dog & Frog & Horse & Ship & Truck & \\\\\n\\midrule\nKL-Matching\\cite{maxlogit} & \\revise{23.60} & \\revise{32.60} & \\revise{22.32} & \\revise{42.92} & \\revise{46.26} & \\revise{24.30} & \\revise{24.97} & \\revise{46.74} & \\revise{25.32} & \\revise{40.53} & \\revise{32.96} \\\\\nResidual\\cite{vim} & \\revise{12.06} & \\revise{25.58} & \\revise{16.71} & \\revise{21.17} & \\revise{48.33} & \\revise{22.12} & \\revise{17.42} & \\revise{36.72} & \\revise{17.30} & \\revise{30.76} & \\revise{24.82} \\\\\nMahalanobis\\cite{mahalanobis} & \\revise{12.59} & \\revise{25.72} & \\revise{18.92} & \\revise{21.48} & \\revise{48.44} & \\revise{20.59} & \\revise{19.20} & \\revise{38.02} & \\revise{17.47} & \\revise{30.93} & \\revise{25.34} \\\\\nViM\\cite{vim} & \\revise{12.43} & \\revise{24.83} & \\revise{15.77} & \\revise{20.13} & \\revise{48.68} & \\revise{21.77} & \\revise{17.63} & \\revise{36.63} & \\revise{17.60} & \\revise{30.78} & \\revise{24.63} \\\\\n\\rowcolor{gray!20}MOODv1\\cite{MOOD} & \\revise{7.59} & \\revise{5.04} & \\revise{2.47} & \\revise{\\textbf{7.49}} & \\revise{15.63} & \\revise{\\textbf{10.96}} & \\revise{11.37} & \\revise{13.09} & \\revise{10.06} & \\revise{19.62} & \\revise{10.33} \\\\\n\\rowcolor{gray!20}MOODv2 (ours) & \\revise{\\textbf{4.82}} & \\revise{\\textbf{4.50}} & \\revise{\\textbf{1.79}} & \\revise{8.80} & \\revise{\\textbf{15.59}} & \\revise{11.00} & \\revise{\\textbf{8.46}} & \\revise{\\textbf{12.43}} & \\revise{\\textbf{8.60}} & \\revise{\\textbf{18.96}} & \\revise{\\textbf{9.49}} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\caption{\n\\revise{Performance of OOD detection methods on ViT-B/16 model with $224\\times224$-pixel inputs. All methods are pre-trained on ImageNet-21k and finetuned on ImageNet-1k. We perform each category of CIFAR-10 \\cite{cifar} as the ID dataset and other classes as OOD datasets. We report the average results across OOD classes of each ID class. Both metrics AUROC and FPR95 are in percentage. The best method is emphasized in bold and a gray background indicates our methods.}\n} % The detailed class-wize performance is in the Appendix.\n\\label{tab:one-class}\n\\end{table*}\n\n\n\n\\section{Experiments}\\label{sec:exp}\n\\label{sec:exp}\n\\revise{In this section, we conduct a thorough comparison of our algorithm with the latest OOD detection methods. We employ the ViT-B/16 model, pre-trained on ImageNet-21K \\re{with corresponding pretext tasks from different methods} and fine-tuned on ImageNet-1K at a resolution of $224\\times224$. }\n\n\\vspace{2mm}\\noindent\\revise{\\textbf{ID/OOD Datasets.} We select CIFAR-10 \\cite{cifar} and ImageNet-1K~\\cite{imagenet} as the ID datasets. Following established procedures~\\cite{vim}, for estimating the principal space of ImageNet, we randomly sample $200,000$ images from the training set. Our experiments include the following OOD datasets:\n\\begin{enumerate}\n    \\item OpenImage-O is a newly collected large-scale OOD dataset~\\cite{openimages_o}.\n    \\item Texture~\\cite{cimpoi14describing} comprises natural textural images, with four overlapping categories (\\emph{bubbly, honeycombed, cobwebbed, spiraled}) removed since they coincide with ImageNet.\n    \\item iNaturalist~\\cite{van2018inaturalist} is a fine-grained species classification dataset, and we use a specific subset from previous works ~\\cite{huang2021mos}.\n    \\item ImageNet-O~\\cite{hendrycks2021natural} contains images that are adversarially filtered to challenge OOD detectors. \n\\end{enumerate} \n}\n\n\\re{Although these OOD datasets are specifically tailored to ensure that they do not belong to any category in ImageNet, rather than being customized for CIFAR-10, each category in CIFAR-10 has a similar counterpart in ImageNet, as referenced in the appendix. Consequently, we use the same OOD datasets for CIFAR-10 as well.}\n\n\n\\vspace{2mm}\\noindent\\revise{\\textbf{Evaluation Metrics.} \nWe report two commonly used evaluation metrics AUROC and FPR95. The AUROC is a threshold-free metric, indicating the area under the receiver operating characteristic curve, with a higher value denoting better detection performance. FPR95, or FPR at TPR95, stands for the false positive rate when the true positive rate is 95\\%, and a smaller FPR95 is preferable. Both metrics are expressed as percentages.}\n\n\\vspace{2mm}\\noindent\\revise{\\textbf{Baseline Methods.} \nFollowing previous works \\cite{vim}, we compare MOODv2 with the baseline algorithms that do not require fine-tuning including {MSP} \\cite{baseline_ood}, {Energy} \\cite{energy}, {MaxLogit} \\cite{maxlogit}, {KL Matching} \\cite{maxlogit}, {Residual}, {ReAct} \\cite{react}, and {Mahalanobis} \\cite{mahalanobis}. }\n\n%--------------------------------------visualization----------------\n\\begin{figure*}[tp]\n  \\centering\n    \\includegraphics[width=\\textwidth]{figures/distribution_method.pdf}\n    \\caption{\\revise{The distribution curves of OOD score functions for ID and OOD datasets obtained using various mainstream methods, including {MSP} \\cite{baseline_ood}, {Energy} \\cite{energy}, {MaxLogit} \\cite{maxlogit}, {KL Matching} \\cite{maxlogit}, {Residual} \\cite{vim}, {ReAct} \\cite{react}, {Mahalanobis} \\cite{mahalanobis} and {ViM} \\cite{vim}. The red line indicates the ID dataset ImageNet \\cite{imagenet}; the blue line indicates Texture \\cite{dtd}; the green line indicates iNaturalist \\cite{inaturalist}; the purple line indicates ImageNet-O \\cite{imagenet_o}; the orange line indicates OpenImage-O \\cite{openimages_o}}}\n  \\label{fig:distribution}\n\\end{figure*}\n\n\\subsection{One-Class OOD Detection}\n\\label{sec:1class}\nWe start with the one-class OOD detection. For a given multi-class dataset of $N_c$ classes, we conduct $N_c$ one-class OOD tasks, where each task regards one of the classes as in-distribution and the remaining classes as out-of-distribution. We run our experiments on CIFAR-10 \\cite{cifar}. \\Cref{tab:one-class} summarizes the average results across OOD classes of each ID class and the detailed \\re{class-wise} performance is in the appendix.\n\n\\revise{It's worth noting that all methods were pre-trained on ImageNet-21k and fine-tuned on ImageNet-1k, which may have had some influence on the results to varying degrees. Nevertheless, we ensure consistent training strategies for all methods to ensure a fair comparison. \\re{Experimental results have demonstrated that MOODv2 achieves significant improvements across all ID classes.} Notably, we achieved a remarkable 3.56\\% increase in the AUROC, reaching 98.20\\%, while simultaneously reducing the FPR95 by 15.14\\% to achieve an impressive 9.49\\%.}\n\n\\subsection{Multi-Class OOD Detection} \n\\label{sec:multi-class}\n\\revise{For multi-class OOD Detection, we assume that ID samples are from a multi-class dataset, either CIFAR-10 \\cite{cifar} or ImageNet \\cite{imagenet}. They are tested on external datasets as out-of-distribution, including OpenImage-O \\cite{openimages_o}, Texture \\cite{cimpoi14describing}, iNaturalist \\cite{van2018inaturalist} and ImageNet-O \\cite{hendrycks2021natural}.}\n\n\\revise{Results are shown in \\cref{tab:multi-class}. MOODv2 delivers outstanding results on CIFAR-10, achieving an impressive AUROC of 99.98\\% (0.35\\% enhancement) and the FPR95 reaches an astonishingly low rate of 0.07\\%, marking a substantial 95\\% reduction compared to the prior SOTA (1.39\\%). On ImageNet, MOODv2 also exhibited significant improvements, showcasing a remarkable 14.30\\% increase in AUROC, resulting in 95.68\\%. Additionally, the FPR95 saw a substantial reduction of 44.93\\%, reaching 20.22\\%.}\n\n\\revise{In \\cref{fig:distribution}, we illustrate the distribution curves of OOD scores for ID and OOD datasets using various mainstream methods. A smaller overlap between ID and OOD data indicates superior OOD detection performance, while a larger overlap signifies weaker detection results. The ID curve (in red) for MOODv2 features a distinct peak at a higher position, resulting in minimal overlap with other OOD data, indicating a notable OOD detection capability. This success can be attributed to the high-quality ID feature representation.}\n\n\\section{Conclusion}\\label{sec:conclusion}\n\\revise{In our work, we focus on the critical aspect of effective out-of-distribution (OOD) detection, which involves acquiring a robust in-distribution (ID) representation that distinguishes it from OOD samples. We conduct comprehensive experiments with distinct pretraining tasks and employ various OOD score functions. The findings indicate that feature representations pre-trained through reconstruction significantly enhance performance and reduce the performance gap among different score functions. This implies that even simple score functions can perform as well as complex ones when utilizing reconstruction-based pretext tasks. These findings hold promise for further development in OOD detection. Ultimately, we introduce the MOODv2 OOD detection framework, employing the masked image modeling pretext task, which achieves a remarkable 14.30\\% increase in AUROC, reaching 95.68\\% on ImageNet, and substantially improving CIFAR-10 to 99.98\\%.}\n. The second paper ends.", "input": "Please summarize the papers.", "answer": ""}
{"context": "The first paper begins. \\begin{abstract}\n\nThe core of out-of-distribution (OOD) detection is to learn the in-distribution (ID) representation, which is distinguishable from OOD samples. Previous work applied recognition-based methods to learn the ID features, which tend to learn shortcuts instead of comprehensive representations. In this work, we find surprisingly that simply using reconstruction-based methods could boost the performance of OOD detection significantly. We deeply explore the main contributors of OOD detection and find that reconstruction-based pretext tasks have the potential to provide a generally applicable and efficacious prior, which benefits the model in learning intrinsic data distributions of the ID dataset. Specifically, we take Masked Image Modeling as a pretext task for our OOD detection framework (MOOD). Without bells and whistles, MOOD outperforms previous SOTA of one-class OOD detection by 5.7\\%, multi-class OOD detection by 3.0\\%, and near-distribution OOD detection by 2.1\\%. It even defeats the 10-shot-per-class outlier exposure OOD detection, although we do not include any OOD samples for our detection. Codes are available at \\href{}{https://github.com/JulietLJY/MOOD}.\n\n% achieve new state-of-the-art results on one-class and multi-class tasks with the AUROCs reaching 94.9\\% and 97.6\\% respectively, which are 5.7\\% and 3.0\\% higher than the existing works. \n\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\n\nA reliable visual recognition system not only provides correct predictions on known context (also known as in-distribution data) but also detects unknown out-of-distribution (OOD) samples and rejects (or transfers) them to human intervention for safe handling. This motivates applications of outlier detectors before feeding input to the downstream networks, which is the main task of OOD detection, also referred to as novelty or anomaly detection. OOD detection is the task of identifying whether a test sample is drawn far from the in-distribution (ID) data or not. It is at the cornerstone of various safety-critical applications, including medical diagnosis \\cite{caruana2015intelligible}, fraud detection \\cite{phua2010comprehensive}, autonomous driving \\cite{eykholt2018robust}, etc.\n% When deploying, the OOD network is supposed to solve problems with a large amount of data. \n\n\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=0.99\\linewidth]{figures/performance.pdf}\n    \\caption{Performance of MOOD compared with current SOTA (indicated by `*') on four OOD detection tasks: (a) one-class OOD detection; (b) multi-class detection; (c) near-distribution detection; and (d) few-shot outlier exposure OOD detection.}\n  \\label{fig:performance}\n\\end{figure}\n\n\nMany previous OOD detection approaches depend on outlier exposure \\cite{ssd, oodlimits} to improve the performance of OOD detection, which turns OOD detection into a simple binary classification problem. We claim that the core of OOD detection is, instead, to learn the effective ID representation to discover OOD samples without any known outlier exposure. \n\nIn this paper, we first present our surprising finding -- that is, {\\it simply using reconstruction-based methods can {\\it notably} boost the performance on various OOD detection tasks}. Our pioneer work along this line even outperforms previous few-shot outlier exposure OOD detection, albeit we do not include any OOD samples.\n\nExisting methods perform contrastive learning \\cite{csi, ssd} or pretrain classification on a large dataset \\cite{oodlimits} to detect OOD samples. The former methods classify images according to the pseudo labels while the latter classifies images based on ground truth, whose core tasks are both to fulfill the classification target. However, research on backdoor attack \\cite{backdoor_attack, frog_attack} shows that when learning is represented by classifying data, networks tend to take a shortcut to classify images. % by only learning specific patterns between categories. \n\nIn a typical backdoor attack scene  \\cite{frog_attack}, the attacker adds secret triggers on original training images with the visibly correct label. During the course of testing, the victim model classifies images with secret triggers into the wrong category. Research in this area demonstrates that networks only learn specific distinguishable patterns of different categories because it is a shortcut to fulfill the classification requirement. \n\nNonetheless, learning these patterns is ineffective for OOD detection since the network does not understand the intrinsic data distribution of the ID images. Thus, learning representations by classifying ID data for OOD detection may not be satisfying. For example, when the patterns similar to some ID categories appear in OOD samples, the network could easily interpret these OOD samples as the ID data and classify them into the wrong ID categories. \n\nTo remedy this issue, we introduce the reconstruction-based pretext task. Different from contrastive learning in existing OOD detection approaches \\cite{csi, ssd}, our method forces the network to achieve the training purpose of reconstructing the image and thus makes it learn pixel-level data distribution. \n\nSpecifically, we adopt the masked image modeling (MIM) \\cite{bert, beit, mae} as our self-supervised pretext task, which has been demonstrated to have great potential in both natural language processing \\cite{bert} and computer vision \\cite{beit, mae}. In the MIM task, we split images into patches and randomly mask a proportion of image patches before feeding the corrupted input to the vision transformer. Then we use the tokens from discrete VAE \\cite{tokenzier} as labels to supervise the network during training. With its procedure, the network learns information from remaining patches to speculate the masked patches and restore tokens of the original image. The reconstruction process enables the model to learn from the prior based on the intrinsic data distribution of images rather than just learning different patterns among categories in the classification process. \n\nIn our extensive experiments, it is noteworthy that masked image modeling for OOD detection (MOOD) outperforms the current SOTA on all four tasks of one-class OOD detection, multi-class OOD detection, near-distribution OOD detection, and even few-shot outlier exposure OOD detection, as shown in \\cref{fig:performance}. A few statistics are the following.\n\n\\begin{enumerate}\n\\item For one-class OOD detection (\\cref{tab:one-class}), MOOD boosts the AUROC of current SOTA, i.e., CSI \\cite{csi}, by \\textbf{5.7\\%} to \\textbf{94.9\\%}. \n\\item For multi-class OOD detection (\\cref{tab:multi-class}), MOOD outperforms current SOTA of SSD+ \\cite{ssd} by \\textbf{3.0\\%} and reaches \\textbf{97.6\\%}. \n\\item For near-distribution OOD detection (\\cref{tab:structure}), AUROC of MOOD achieves \\textbf{98.3\\%}, which is \\textbf{2.1\\%} higher than the current SOTA of R50+ViT \\cite{oodlimits}.\n\\item For few-shot outlier exposure OOD detection (\\cref{tab:exposure}), MOOD (\\textbf{99.41\\%}) surprisingly defeats current SOTA of R50+ViT \\cite{oodlimits} (with \\textbf{99.29\\%}), which makes use of 10 OOD samples per class. It is notable that we do not even include any OOD samples in MOOD.\n\\end{enumerate}\n\n%====================================relatex==================================\n\\section{Related Work}\n\\subsection{Out-of-distribution Detection}\nA straightforward out-of-distribution (OOD) approach is to estimate the in-distribution (ID) density \\cite{density_1,density_2,density_3,density_4} and reject test samples that deviate from the estimated distribution. Alternative methods base on the image reconstruction \\cite{reconstruct_1, reconstruct_2, reconstruct_3}, learn the decision boundary between in- and out-of-distribution data \\cite{boundary_1, boundary_2, boundary_3}, compute the distance between train and test features \\cite{distance_1, distance_2, distance_3, csi, ssd}, etc..\n\nIn comparison, our work focuses on distance-based methods and yet includes the reconstruction-based methods as a pretext task. The key idea of distance-based approaches is that the OOD samples are supposedly far from the center of the in-distribution (ID) data \\cite{ood_survey} in the feature space. Representative methods include K-nearest Neighbors \\cite{distance_1}, prototype-based methods \\cite{distance_2, distance_3}, etc.. We will explain the difference between our work and previous OOD detection methods later in this paper.\n\n\\subsection{Vision Transformer}\nTransformer has achieved promising performance in computer vision \\cite{beit, mae} and natural language processing \\cite{bert}. Existing OOD detection research \\cite{oodlimits} performs vision transformer (ViT  \\cite{vit}) with classification pre-train on ImageNet-21k \\cite{imagenet}. It mainly explores the impact of different structures on OOD detection tasks while we deeply explore the effect from four dimensions for OOD detection, including various pretext tasks, architectures, fine-tune processes, and OOD detection metrics. \n\nIt is notable that extra OOD samples are utilized in various previous methods \\cite{ssd, oodlimits} to further improve performance. In contrast, we argue that the exposure of OOD samples violates the original intention of OOD detection. In fact, a sufficient pretext task can achieve comparable or even superior results. Therefore, in our work, we focus on exploring an appropriate pretext task for OOD detection without including any OOD samples.\n\n%----------------------------------------pretask---------------------------------------\n\\begin{table*}[t]\n\\centering\n\\small\n\\setlength{\\tabcolsep}{3.5mm}\n\\begin{tabular}{c|cccccccccccccc}\n\\toprule\nIn-Distribution          & \\multicolumn{4}{c|}{CIFAR-10 $\\longrightarrow $}                                    & \\multicolumn{4}{c}{CIFAR-100 $\\longrightarrow $}              \\\\\nOut-of-Distribution         & SVHN          & CIFAR-100     & LSUN           & \\multicolumn{1}{c|}{Avg}           & SVHN          & CIFAR-10      & LSUN          & Avg           \\\\\n\\midrule\n%Cls(SM) & \\textbf{99.8} & 91.5          & 95.4           & \\multicolumn{1}{c|}{95.6}          & 90.8          & 90.9          & 79.2          & 87.0          \\\\\nClassification & 98.3          & 98.6          & 98.6           & \\multicolumn{1}{c|}{98.5}          & 78.0          & 93.5          & 88.6          & 86.7          \\\\\nMoCov3    & 98.6          & 92.4          & 89.8           & \\multicolumn{1}{c|}{93.6}          & 78.8          & 72.8          & 75.8          & 75.8          \\\\\nMIM         & \\textbf{99.8} & \\textbf{99.4} & \\textbf{99.9}  & \\multicolumn{1}{c|}{\\textbf{99.7}} & \\textbf{96.5} & \\textbf{98.3} & \\textbf{96.3} & \\textbf{97.0} \\\\\n\\midrule\nIn-Distribution          & \\multicolumn{8}{c}{ImageNet-30 $\\longrightarrow $}                                                                                                  \\\\\nOut-of-Distribution         & Dogs          & Places365     & Flowers102     & Pets                               & Food          & Dtd           & Caltech256    & Avg           \\\\\n\\midrule\n%Cls(SM) & 83.5          & 83.8          & 99.3           & 77.7                               & 69.6          & 91.8          & 89.4          & 85.0          \\\\\nClassification & \\textbf{99.7} & 98.4          & 99.9           & \\textbf{99.6}                      & \\textbf{98.3} & 98.6          & 96.8          & 98.8          \\\\\nMoCov3      & 88.2          & 82.0          & 99.3           & 81.1                               & 71.4          & 91.3          & 88.5          & 86.0          \\\\\nMIM         & 99.4          & \\textbf{98.9} & \\textbf{100.0} & 99.1                               & 96.6          & \\textbf{99.5} & \\textbf{98.9} & \\textbf{98.9} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Pretext Task}. AUROC (\\%) of OOD detection on ViT with different pretext tasks on ImageNet22k. }\n\\label{tab:pretask}\n\\end{table*}\n\n\n\\subsection{Self-Supervised Pretext Task}\nIt has been long in the community to pre-train vision networks in various self-supervised manners, including generative learning \\cite{pixelcnn, gpt, bert, beit}, contrastive learning \\cite{moco, supcon, simclr, simsiam} and adversarial learning \\cite{colorization, gan, adversial_ae}.\nAmong them, representative generative approaches include auto-regressive \\cite{pixelcnn, gpt}, flow-based \\cite{nice,glow}, auto-encoding \\cite{bert, beit}, and hybrid generative methods \\cite{graphaf, xlnet}. \n\nThe self-supervised pretext task in our framework is Masked Image Modeling (MIM). It generally belongs to auto-encoding generative approaches. MIM was first proposed in natural language processing \\cite{beit}. Its language modeling task randomly masks varying percentages of tokens of text and recovers the masked tokens from encoding results of the rest of text. Follow-up research \\cite{bert, mae} transfers the similar idea from natural language processing to computer vision, masking different proportions of the image patches to recover results.\n\nMultiple existing methods take advantage of self-supervised tasks to guide learning of representation for OOD detection. The latest work \\cite{csi, ssd} presents contrastive learning models as feature extractors. However, existing approaches of classifying transformed images according to contrastive learning possess similar limitations -- that is, the model tends to learn the specific patterns of categories, which are beneficial for classification but do not help understand intrinsic data  distributions of ID images. \n\nResearch of \\cite{oodlimits} also mentioned this problem. However, the introduced large-scale pre-trained transformers \\cite{oodlimits} may not jump out of the loop, in our observation, because the pretext task remained to be classification. In our work, we address this issue by performing the masked image modeling task for OOD detection.\n\n% By contrast, our framework performs the masked image modeling task to force the network to reconstruct the image and thus learn the pixel-level data distribution. Benefiting from the prior based on such data distribution, our OOD detection network can learn more distinguishable representations, which enlarge the divergence between in- and out-of-distribution data. The performance of MOOD compared with methods based on contrastive learning is vividly shown in the distances from ID and OOD testing samples to the training data in \\cref{fig:distribution}.\n\n\n\n%=================================Method========================================\n\\section{Method}\nIn this section, we first explain the main factors to help OOD detection and finally propose our framework to achieve this goal. \n\nWe first define the notations. For a given dataset $X_{\\rm ID}$, the goal of out-of-distribution (OOD) detection is to model a detector that identifies whether an input image $x \\in X_{\\rm ID}$ or $x \\notin X_{\\rm ID}$ (that is, $x \\in X_{\\rm OOD}$). A majority of existing methods for OOD detection define an OOD score function $s(x)$. Its abnormal high or low value represents that $x$ is from out-of-distribution. \n\n\\subsection{Choosing the Pretext Task} \n\\label{sec:pretask}\n\nIn this section, we choose the pretext task that can provide the intrinsic prior to suit the OOD detection task. Most previous OOD methods learn the ID representation through classification \\cite{baseline_ood, oodlimits} or contrastive learning \\cite{csi, ssd} on ID samples, which take advantage of either the ground truth or pseudo labels to supervise the classification networks. \n\nOn the other hand, work of \\cite{backdoor_attack, frog_attack} shows that classification networks only learn different patterns among training categories because it is a shortcut to fulfill classification. It is indicated that the network actually does not understand the intrinsic data distribution of the ID images.\n\nIn comparison, the reconstruction-based pretext task forces the network to learn the real data distribution of the ID images during training to reconstruct the image instead of the patterns for classification. Benefiting from these priors, the network can learn a more representative feature of the ID dataset. It enlarges the divergence between the OOD and ID samples. \n\nIn our method, we pre-train the model with Masked Image Modeling (MIM) pretext \\cite{bert} on a large dataset and fine-tune it on the ID dataset. We compare the performance of MIM and contrastive learning pretext task MoCov3 \\cite{mocov3} in ~\\cref{tab:pretask}. It shows that the performance of MIM is much increased by 13.3\\% to 98.66\\%. \n\n\\subsection{Exploring Architecture} \n\\label{sec:arch}\n%----------------------------------------structure------------------------------------------\n\\begin{table}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{3.5mm}\n\\begin{tabular}{c|ccccccccccc}\n\\toprule\n\\multirow{2}{*}{Model} & Fine-tuned          & \\multirow{2}{*}{AUROC(\\%)} \\\\\n                       & Test Acc(\\%)        &                            \\\\\n\\midrule\nBiT R50 \\cite{oodlimits} & 87.01               & 81.71                      \\\\\nBiT R101$\\times$3 \\cite{oodlimits}& 91.55               & 90.10                      \\\\\nViT \\cite{oodlimits} & 90.95               & 95.53                      \\\\\nMLP-Mixer \\cite{oodlimits} & 90.40               & 95.31                      \\\\\nR50 + ViT (SOTA) \\cite{oodlimits} & \\textbf{91.71}               & \\textbf{96.23}  \\\\\n% \\midrule\n% MOOD (ours)          & \\textbf{95.73}      & \\textbf{98.30}             \\\\\n%(Difference)              & {\\color{teal}+0.16} & {\\color{teal}+0.70}        \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Architecture}. AUROC (\\%) of OOD detection with various architectures. The last line shows our improvement. The ID and OOD datasets are CIFAR-100 and CIFAR-10, respectively. }\n\\label{tab:structure}\n\\end{table}\n\nTo explore an effective architecture \\cite{oodlimits}, we evaluate OOD detection performance on BiT (Big Transfer \\cite{bit}) and MLP-Mixer, in comparison with ViT. We adopt CIFAR-100 and CIFAR-10 \\cite{cifar} as the ID-OOD pair. They have close distributions because of their similar semantics and construction. Results are in \\cref{tab:structure}.\n\nR50 + ViT \\cite{vit, resnet} is the current SOTA on near-distribution OOD detection \\cite{oodlimits}, which doubles the model size and testing time but achieves only 96.23\\% (0.70\\% higher than ViT). However, MIM on a single ViT significantly improves its AUROC to 98.30\\% (2.07\\% higher), without any additional source assumption. It manifests that efficient pretext itself is sufficient for producing distinguishable representation -- {\\it there is no need to use a larger model or combination of multiple models} in this regard.\n\n\n\n\n\\subsection{About Fine-Tuning} \n\\label{sec:fine-tune}\n\n\\begin{table}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{2.1mm}\n\\begin{tabular}{c|ccc|ccccccc}\n\\toprule\nOne-Class                    & \\multicolumn{3}{c|}{fine-tune}         & \\multirow{2}{*}{AUROC(\\%)} \\\\\nDataset                      & MIM-pt     & inter-ft   & fine-tune         & \\\\\n\\midrule\n\\multirow{2}{*}{CIFAR-10}    & \\checkmark &            &            & 72.2 \\\\\n                             & \\checkmark & \\checkmark &            & \\textbf{97.9} \\\\\n\\midrule\n\\multirow{2}{*}{CIFAR-100}   & \\checkmark &            &            & 66.3  \\\\\n                             & \\checkmark & \\checkmark &            & \\textbf{96.5} \\\\\n\\midrule\n\\multirow{2}{*}{ImageNet-30} & \\checkmark &            &            & 75.2   \\\\\n                             & \\checkmark &            & \\checkmark & \\textbf{92.0} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Fine-tuning} (One-class). AUROC (\\%) of OOD detection with different fine-tuning processes on one-class CIFAR-10, CIFAR-100 (super-classes) and ImageNet-30.}\n\\label{tab:1class-fine-tune}\n\\end{table}\n\n%----------------------------------------metric--------------------------------------\n\\begin{table*}[t]\n\\centering\n\\small\n\\setlength{\\tabcolsep}{3.2mm}\n\\begin{tabular}{ccc|ccccccccccccc}\n\\toprule\n\\multicolumn{3}{c}{finetune}                   & \\multicolumn{4}{|c}{CIFAR-10 $\\longrightarrow $}                             & \\multicolumn{4}{|c}{CIFAR-100 $\\longrightarrow $}                                   \\\\\nMIM-pt               & inter ft   & ft         & SVHN           & CIFAR-100     & LSUN           & Avg                       & \\multicolumn{1}{|c}{SVHN}          & CIFAR-10             & LSUN                 & Avg                  \\\\\n\\midrule\n\\checkmark           &            &            & 62.2           & 62.9          & 98.5           & \\multicolumn{1}{c|}{74.5} & 48.4          & 42.2                 & 96.0                 & 62.2                 \\\\\n\\checkmark           & \\checkmark &            & 89.5           & 90.0          & 99.8           & \\multicolumn{1}{c|}{93.1} & 74.3          & 62.0                 & \\textbf{98.3}        & 68.2                 \\\\\n\\checkmark           &            & \\checkmark & 99.1           & 94.6          & 97.4           & \\multicolumn{1}{c|}{97.0}   & 93.7          & 83.7                 & 91.4                 & 89.6                 \\\\\n\\checkmark           & \\checkmark & \\checkmark & \\textbf{99.8}  & \\textbf{99.4} & \\textbf{99.9}  & \\multicolumn{1}{c|}{\\textbf{99.7}} & \\textbf{96.5} & \\textbf{98.3}        & 96.3                 & \\textbf{97.0}        \\\\\n\\midrule\n\\multicolumn{3}{c}{finetune}                   & \\multicolumn{8}{|c}{ImageNet30 $\\longrightarrow $}                                                                                                                \\\\\nMIM-pt               & inter-ft   & ft         & Dogs           & Places365     & Flowers102     & Pets                      & Food          & Caltech256           & Dtd                  & Avg                  \\\\\n\\midrule\n\\checkmark           &            &            & 60.2           & 82.7          & 28.6           & 41.9                      & 72.5          & 42.2                 & 29.4                 & 51.1                 \\\\\n\\checkmark           & \\checkmark &            & \\textbf{100.0} & 97.9          & 99.9           & \\textbf{99.6}             & \\textbf{97.1} & 96.9                 & 98.2                 & 98.2                 \\\\\n\\checkmark           &            & \\checkmark & 91.3           & 97.0          & 95.1           & 93.8                      & 99.3          & 84.0                 & 95.4                 & 92.9                 \\\\\n\\checkmark           & \\checkmark & \\checkmark & 99.4           & \\textbf{98.9} & \\textbf{100.0} & 99.1                      & 96.6          & \\textbf{99.5}        & \\textbf{98.9}        & \\textbf{98.9}       \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Fine-tuning} (Multi-class). AUROC (\\%) of OOD detection with different fine-tuning processes on multi-class CIFAR-10, CIFAR-100 and ImageNet-30. }\n\\label{tab:metric}\n\\label{tab:fine-tune}\n\\end{table*}\n\n%----------------------------------------metric--------------------------------------\n\\begin{table*}[t]\n\\centering\n\\small\n\\setlength{\\tabcolsep}{3.5mm}\n\\begin{tabular}{c|ccccccccccccccc}\n\\toprule\nIn-Distribution                           & \\multicolumn{4}{c|}{CIFAR-10 $\\longrightarrow $}                                                        & \\multicolumn{4}{c}{CIFAR-100 $\\longrightarrow $}                                          \\\\\nOut-of-Distribution                          & SVHN                 & CIFAR-100            & LSUN                 & \\multicolumn{1}{c|}{Avg}           & SVHN                 & CIFAR-10             & LSUN                 & Avg                  \\\\\n\\midrule\nSoftmax  \\cite{baseline_ood} & 88.6                 & 85.8                 & 90.7                 & \\multicolumn{1}{c|}{88.4}          & 81.9                 & 81.1                 & 86.6                 & 83.2                 \\\\\nEntropy  \\cite{baseline_ood} & \\textbf{99.9}        & 97.1                 & 98.1                 & \\multicolumn{1}{c|}{98.4}          & 93.7                 & 94.1                 & 88.7                 & 92.2                 \\\\\nEnergy   \\cite{energy}       & \\textbf{99.9}        & 97.0                 & 97.6                 & \\multicolumn{1}{c|}{98.2}          & 92.8                 & 93.5                 & 86.1                 & 90.8                 \\\\\nGradNorm  \\cite{gradnorm}    & 99.6                 & 94.3                 & 87.8                 & \\multicolumn{1}{c|}{93.9}          & 61.6                 & 87.7                 & 38.4                 & 62.6                 \\\\\nDistance  \\cite{mahalanobis} & 99.8                 & \\textbf{99.4}        & \\textbf{99.9}        & \\multicolumn{1}{c|}{\\textbf{99.7}} & \\textbf{96.5}        & \\textbf{98.3}        & \\textbf{96.3}        & \\textbf{97.0}        \\\\\n\\midrule\nIn-Distribution                           & \\multicolumn{8}{c}{ImageNet-30 $\\longrightarrow $}                                                                                                                                                  \\\\\nOut-of-Distribution                          & Dogs                 & Places365            & Flowers102           & Pets                               & Food                 & Dtd                  & Caltech256           & Avg                  \\\\\n\\midrule\nSoftmax  \\cite{baseline_ood} & 96.7                 & 90.5                 & 89.7                 & 95.0                               & 79.8                 & 90.6                 & 90.1                 & 90.3                 \\\\\nEntropy  \\cite{baseline_ood} & 92.5                 & 87.2                 & 97.5                 & 90.6                               & 69.6                 & 94.9                 & 85.7                 & 88.3                 \\\\\nEnergy   \\cite{energy}       & 89.7                 & 82.1                 & 95.8                 & 88.1                               & 67.8                 & 93.1                 & 82.3                 & 85.6                 \\\\\nGradNorm  \\cite{gradnorm}    & 74.8                 & 78.7                 & 92.0                 & 70.6                               & 61.5                 & 90.3                 & 74.3                 & 77.5                 \\\\\nDistance  \\cite{mahalanobis} & \\textbf{99.4}        & \\textbf{98.9}        & \\textbf{100.0}       & \\textbf{99.1}                      & \\textbf{96.6}        & \\textbf{99.5}        & \\textbf{98.9}        & \\textbf{98.9}       \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Metric}. AUROC (\\%) of OOD detection with different metrics on multi-class CIFAR-10, CIFAR-100 and ImageNet-30. }\n\\label{tab:metric}\n\\label{tab:fine-tune}\n\\end{table*}\n\n\n\\noindent\\textbf{One-class Fine-tuning.} For one-class OOD detection, we pre-train the MIM model and finely tune it on ImageNet-21k \\cite{imagenet}, as recommended by BEiT \\cite{beit}. In particular, when performing one-class OOD detection on ImageNet-30, since we do not include the OOD labels during training, we only pre-train it on ImageNet-21k without intermediate fine-tuning. Therefore, we utilize the label smoothing \\cite{ls} to help the model learn from the one-class fine-tune task on the ID dataset as\n\\begin{equation}\\label{equ:ls}\n    y_c^{LS} = y_c(1-\\alpha)+\\alpha/N_c, \\quad\\quad c=1, 2, \\dots, N_c\n\\end{equation}\nwhere $c$ is the index of category; $N_c$ is the number of classes; and $\\alpha$ is the hyperparameter that determines smoothing level. If $\\alpha=0$, we obtain the original one-hot encoded $y_c$ and if $\\alpha=1$, we get the uniform distribution.\n\nLabel smoothing was used to address overfitting and overconfidence in normal fine-tuning process. We, instead, find that it can be utilized in one-class fine-tuning. The performance of the model before and after one-class fine-tune is illustrated in \\cref{tab:1class-fine-tune}. It is clear that the model actually learns information from the one-class fine-tuning operation. This may be counter-intuitive because the labels are equal. The reason is, due to label smoothing, the loss is larger than 0 and persuades the model to update parameters, although the accuracy reaches 1.\n\n\\vspace{2mm}\\noindent\\textbf{Multi-class Fine-tuning.}\nFor multi-class OOD detection, we pre-train the MIM model, intermediately use fine-tuning on ImageNet-21k \\cite{imagenet}, and apply fine-tuning again on the ID dataset. We perform experiments to validate the effectiveness of each stage in \\cref{tab:fine-tune}. It proves that all stages contribute well to the performance of OOD detection.\n\n%------------------------------------OOD------------------------------------\n\n\\subsection{OOD Detection Metric is Important}\n\\label{sec:metric}\n\nHere, we compare the performance of several commonly-used OOD detection metrics, including Softmax \\cite{baseline_ood}, Entropy \\cite{baseline_ood}, Energy \\cite{energy}, GradNorm  \\cite{gradnorm} and Mahalanobis distance \\cite{mahalanobis}. We perform OOD detection with MIM pretext task with each metric -- the results are shown in \\cref{tab:metric}. They prove that the Mahalanobis distance is a better metric for MOOD.\n\n\\subsection{Final Algorithm of MOOD} \n\\label{sec:alg}\n\nTo sum up, in this section, we have explored the effect of contributors to OOD detection, including various pretext tasks, architectures, fine-tuning processes, and OOD detection metrics. In general, we find that the finely tuned MOOD on ViT with Mahalanobis distances achieves the best result. The outstanding performance of MOOD demonstrates that an efficient pretext task itself is sufficient for producing distinguishable representation, and there is no need for a larger model or multi-models. \n\nIn \\cref{sec:exp}, we will show that few-shot outlier exposure utilized in multiple existing OOD detection approaches \\cite{ssd, oodlimits} is also unnecessary. The algorithm of MOOD is shown in the Appendix. It mainly includes the following stages.\n\n\\begin{enumerate}\n% \\renewcommand{\\labelenumi}{Step \\theenumi.}\n\\item Pre-train the Masked Image Modeling ViT on ImageNet-21k.\n\\item Apply intermediate fine-tuning ViT on ImageNet-21k. %(Not for one-class detection on ImageNet-30). \n\\item Apply fine-tuning of pre-trained ViT on the ID dataset. % (Not for one-class detection on CIFAR-10 and CIFAR-100). \n\\item Extract features from the trained ViT and calculate the Mahalanobis distance metric for OOD detection.\n\\end{enumerate}\n\n%--------------------------------------one-class-----------------------------------\n\\begin{table*}[t]\n\\small\n\\centering\n\\begin{subtable}{0.99\\linewidth}\n\\centering\n\\setlength{\\tabcolsep}{1.5mm}\n\\begin{tabular}{c|ccccccccccccccc}\n\\toprule\n%Method & Network   & Airplane & Automobile & Bird & Cat  & Dear & Dog  & Frog & Horse & Ship & Truck & Average \\\\\nMethod   & Plane & Car   & Bird & Cat & Dear& Dog & Frog& Horse & Ship& Truck & Average  \\\\\n\\midrule\nOC-SVM\\cite{goad} & 65.6 & 40.9 & 65.3 & 50.1 & 75.2 & 51.2 & 71.8 & 51.2 & 67.9 & 48.5 & 58.8   \\\\\nDeepSVDD\\cite{deepsvdd}  & 61.7 & 65.9 & 50.8 & 59.1 & 60.9 & 65.7 & 67.7 & 67.3 & 75.9 & 73.1 & 64.8   \\\\\nAnoGAN\\cite{anogan} & 67.1 & 54.7 & 52.9 & 54.5 & 65.1 & 60.3 & 58.5 & 62.5 & 75.8 & 66.5 & 61.8   \\\\\nOCGANOCGAN\\cite{ocgan} & 75.7 & 53.1 & 64.0 & 62.0 & 72.3 & 62.0 & 72.3 & 57.5 & 82.0 & 55.4 & 65.7   \\\\\nGeom\\cite{geom}  & 74.7 & 95.7 & 78.1 & 72.4 & 87.8 & 87.8 & 83.4 & 95.5 & 93.3 & 91.3 & 86.0   \\\\\nRot\\cite{rot}   & 71.9 & 94.5 & 78.4 & 70.0 & 77.2 & 86.6 & 81.6 & 93.7 & 90.7 & 88.8 & 83.3   \\\\\nRot+Trans\\cite{rot} & 77.5 & 96.9 & 87.3 & 80.9 & 92.7 & 90.2 & 90.9 & 96.5 & 95.2 & 93.3 & 90.1   \\\\\nGOAD\\cite{goad}  & 77.2 & 96.7 & 83.3 & 77.7 & 87.8 & 87.8 & 90.0 & 96.1 & 93.8 & 92.0 & 88.2   \\\\\nCSI (SOTA)\\cite{csi}   & 89.9 & 99.1 & 93.1 & 86.4 & 93.9 & 93.2 & 95.1 & 98.7 & 97.9 & 95.5 & 94.3   \\\\\nours  & \\textbf{98.6}\\tiny{$\\pm$0.4} & \\textbf{99.3}\\tiny{$\\pm$0.5} & \\textbf{94.3}\\tiny{$\\pm$0.6} & \\textbf{93.2}\\tiny{$\\pm$0.5} & \\textbf{98.1}\\tiny{$\\pm$0.6} & \\textbf{96.5}\\tiny{$\\pm$0.4} & \\textbf{99.3}\\tiny{$\\pm$0.2} & \\textbf{99.0}\\tiny{$\\pm$0.1} & \\textbf{98.8}\\tiny{$\\pm$0.1} & \\textbf{97.8}\\tiny{$\\pm$0.4} & \\textbf{97.8}\\tiny{$\\pm$0.4} \\\\\n(improve)   & {\\color{teal}+8.7}& {\\color{teal}+0.2}& {\\color{teal}+1.2}& {\\color{teal}+6.8}& {\\color{teal}+4.2}& {\\color{teal}+3.3}& {\\color{teal}+4.2}& {\\color{teal}+0.3}& {\\color{teal}+0.9}& {\\color{teal}+2.3}& {\\color{teal}+3.5} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{CIFAR-10}\n\\label{tab:one-class-cifar10}\n\\end{subtable}\n\\\\\n\\begin{subtable}{0.48\\linewidth}\n\\centering\n\\setlength{\\tabcolsep}{12mm}\n\\begin{tabular}{c|ccccccccccc}\n\\toprule\nMethod & AUROC   \\\\\n\\midrule\nOC-SVM\\cite{goad} & 63.1  \\\\\nGeom\\cite{geom}  & 78.7  \\\\\nRot\\cite{rot}   & 77.7  \\\\\nRot+Trans\\cite{rot} & 79.8  \\\\\nGOAD\\cite{goad}  & 74.5  \\\\\nCSI (SOTA)\\cite{csi}    & 89.6  \\\\\nours  & \\textbf{94.8}   \\\\\n(improve)   & {\\color{teal}+5.2}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{CIFAR-100}\n\\label{tab:one-class-cifar100}\n\\end{subtable}\n\\hspace{2.2mm}\n\\begin{subtable}{0.48\\linewidth}\n\\centering\n\\setlength{\\tabcolsep}{8mm}\n\\begin{tabular}{c|ccccccccccc}\n\\toprule\nMethod   & AUROC   \\\\\n\\midrule\nRot\\cite{rot}  & 65.3  \\\\\nRot+Trans\\cite{rot} & 77.9  \\\\\nRot+Attn\\cite{rot} & 81.6  \\\\\nRot+Trans+Attn\\cite{rot} & 84.8  \\\\\nRot+Trans+Attn+Resize\\cite{rot} & 85.7  \\\\\nCSI (SOTA) \\cite{csi}  & 91.6  \\\\\nours & \\textbf{92.0}   \\\\\n(improve)  & {\\color{teal}+0.4}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{ImageNet-30}\n\\label{tab:one-class-imagenet30}\n\\end{subtable}\n\\caption{\\textbf{One-class OOD detection.} AUROC (\\%) of OOD methods on one-class (a) CIFAR-10, (b) CIFAR-100 (super-classes) and (c) ImageNet-30. The reported results on CIFAR-10 are averaged over 3 trials. Subscripts denote standard deviation, and bold ones denote the best results. The last line lists improvement of MOOD over the current SOTA.}\n\\label{tab:one-class}\n\\end{table*}\n\n%--------------------------------------multi-class-----------------------------------\n\\begin{table*}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{3.2mm}\n\\begin{subtable}{0.99\\linewidth}\n\\centering\n\\begin{tabular}{c|cccccccccc}\n\\toprule\nIn-Distribution & \\multicolumn{4}{c|}{CIFAR-10 $\\longrightarrow $} &\\multicolumn{4}{c}{CIFAR-100 $\\longrightarrow $} \\\\\nOut-of-Distribution & SVHN  & CIFAR-100  & LSUN & \\multicolumn{1}{c|}{Average} & SVHN  & CIFAR-10   & LSUN & Average \\\\\n\\midrule\nBaseline OOD\\cite{baseline_ood} & 88.6  & 85.8 &90.7 &\\multicolumn{1}{c|}{88.4} & 81.9  & 81.1 &86.6 &83.2   \\\\\nODIN\\cite{odin}   & 96.4  & 89.6 &-   &\\multicolumn{1}{c|}{93.0} & 60.9  & 77.9 &-   &69.4   \\\\\nMahalanobis\\cite{mahalanobis}  & 99.4  & 90.5 &-   &\\multicolumn{1}{c|}{95.0} & 94.5  & 55.3 &-   &74.9   \\\\\nResidual Flows\\cite{residual_flows}   & 99.1  & 89.4 &-   &\\multicolumn{1}{c|}{94.3} & 97.5  & 77.1 &-   &87.3   \\\\\nGram Matrix\\cite{gram_matrix}  & 99.5  & 79.0 &-   &\\multicolumn{1}{c|}{89.3} & 96.0  & 67.9 &-   &82.0   \\\\\nOutlier exposure\\cite{outlier_exposure} & 98.4  & 93.3 &-   &\\multicolumn{1}{c|}{95.9} & 86.9  & 75.7 &-   &81.3   \\\\\nRotation loss\\cite{rot} & 98.9  & 90.9 &\u2013   &\\multicolumn{1}{c|}{94.9} & - & -   &-   &-   \\\\\nContrastive loss\\cite{supcon} & 97.3  & 88.6 &92.8 &\\multicolumn{1}{c|}{92.9} & 95.6  & 78.3 &-   &87.0   \\\\\nCSI\\cite{csi} & 97.9  & 92.2 &97.7 &\\multicolumn{1}{c|}{95.9} & - & -   &-   &-   \\\\\nSSD+ (SOTA) \\cite{ssd}   & \\textbf{99.9}   & 93.4 &98.4 &\\multicolumn{1}{c|}{97.2} & \\textbf{98.2}   & 78.3 &79.8 &85.4   \\\\\nours   & 99.8\\tiny{$\\pm$0.0} & \\textbf{99.4}\\tiny{$\\pm$0.0} & \\textbf{99.9}\\tiny{$\\pm$0.0} & \\multicolumn{1}{c|}{\\textbf{99.7}} & 96.5\\tiny{$\\pm$0.6} & \\textbf{98.3}\\tiny{$\\pm$0.1} & \\textbf{96.3}\\tiny{$\\pm$0.6} & \\textbf{97.0}   \\\\\n(improve) & {\\color{gray}-0.1}   & {\\color{teal}+6.0}  & {\\color{teal}+1.5}  & \\multicolumn{1}{c|}{{\\color{teal}+2.5}} & {\\color{gray}-1.7}   & {\\color{teal}+20.0}   & {\\color{teal}+16.5}   & {\\color{teal}+11.6}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{CIFAR}\n\\end{subtable}\n\n\\begin{subtable}{0.99\\linewidth}\n\\centering\n\\setlength{\\tabcolsep}{3.8mm}\n\\begin{tabular}{c|ccccccccccccccc}\n\\toprule\nIn-Distribution & \\multicolumn{8}{c}{ImageNet-30 $\\longrightarrow $}   \\\\\nOut-of-Distribution & Dogs  & Places365 & Flowers102 & Pets  & Food  & Caltech256 & DTD   & Average  \\\\\n\\midrule\nBaseline OOD\\cite{baseline_ood} & 96.7  & 90.5  & 89.7  & 95.0  & 79.8  & 90.6  & 90.1  & 90.3  \\\\\nContrastive loss\\cite{supcon} & 95.6  & 89.7  & 92.2  & 94.2  & 81.2  & 90.2  & 92.1  & 90.7  \\\\\nCSI (SOTA)\\cite{csi}   & 98.3  & 94.0  & 96.2  & 97.4  & 87.0  & 93.2  & 97.4  & 94.8  \\\\\nours  & \\textbf{99.4} & \\textbf{98.9} & \\textbf{100.0} & \\textbf{99.1} & \\textbf{96.6} & \\textbf{99.5} & \\textbf{98.9} & \\textbf{98.9}  \\\\\n(improve)   & {\\color{teal}+0.9} & {\\color{teal}+4.9} & {\\color{teal}+3.8} & {\\color{teal}+1.7} & {\\color{teal}+9.6} & {\\color{teal}+6.3} & {\\color{teal}+1.5} & {\\color{teal}+4.1}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{ImageNet-30}\n\\end{subtable}\n\n\\begin{subtable}{0.99\\linewidth}\n\\centering\n\\setlength{\\tabcolsep}{7.7mm}\n\\begin{tabular}{c|ccccccccccccccc}\n\\toprule\nIn-Distribution                              & \\multicolumn{5}{c}{ImageNet-1k $\\longrightarrow $}                                                       \\\\\nOut-of-Distribution                             & iNaturalist        & SUN                & Places             & Textures            & Average            \\\\\n\\midrule\nBaseline OOD \\cite{baseline_ood}         & 87.6               & 78.3               & 76.8               & 74.5                & 79.3               \\\\\nODIN \\cite{odin}                & 89.4               & 83.9               & 80.7               & 76.3                & 82.6               \\\\\nEnergy \\cite{energy}            & 88.5               & 85.3               & 81.4               & 75.8                & 82.7               \\\\\nMahalanobis \\cite{mahalanobis}  & 46.3               & 65.2               & 64.5               & 72.1                & 62.0               \\\\\nGradNorm (SOTA) \\cite{gradnorm} & \\textbf{90.3}      & 89.0               & 84.8               & 81.1                & 86.3               \\\\\nours                     & 86.9               & \\textbf{89.8}      & \\textbf{88.5}      & \\textbf{91.3}       & \\textbf{89.1}      \\\\\n(improve)                       & {\\color{gray}-3.4} & {\\color{teal}+0.8} & {\\color{teal}+3.7} & {\\color{teal}+10.2} & {\\color{teal}+2.8}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{ImageNet-1k}\n\\end{subtable}\n\\caption{\\textbf{Multi-class OOD detection.} AUROC (\\%) of OOD detection methods on multi-class CIFAR-10, CIFAR-100, ImageNet-30 and ImageNet-1k. The reported results on CIFAR-10 and CIFAR-100 are averaged over 3 trials. Subscripts denote standard deviation, and bold ones stand for the best results. The last line lists improvement of MOOD over the current SOTA approach.}\n\\label{tab:multi-class}\n\\end{table*}\n\n\n\n\n%====================================exp===============================================\n\n\\section{Experiments}\n\\label{sec:exp}\nIn this section, we compare Masked Image Modeling for OOD detection (MOOD) with current SOTA approaches in one-class OOD detection (\\cref{sec:1class}), multi-class OOD detection (\\cref{sec:multi-class}), near-distribution OOD detection (\\cref{sec:near-distribution}) and OOD detection with few-shot outlier exposure (\\cref{sec:exposure}). Our MOOD outperforms all previous approaches on all four OOD detection tasks significantly. % In \\cref{sec:discuss}, we visualize the distribution of ID and OOD samples and discuss the performance for near-distribution OOD detection. \n\n\\vspace{2mm}\\noindent\\textbf{Experimental Configuration.} We report the commonly-used Area Under the Receiver Operating Characteristic Curve (AUROC) as a threshold-free evaluation metric for detecting OOD score. We perform experiments on (i) CIFAR-10 \\cite{cifar}, which consists of 50,000 training and 10,000 testing images with 10 image classes, (ii) CIFAR-100 \\cite{cifar} and CIFAR-100 (super-classes) \\cite{cifar}, which consists of 50,000 training and 10,000 testing images with 100 and 20 (super-classes) image classes. respectively, (iii) ImageNet-30 \\cite{imagenet}, which contains 39,000 training and 3,000 testing images with 30 image classes, and (iv) ImageNet-1k \\cite{imagenet}, which contains around 120k and 50k testing images with 1k image classes.\nMore details of training settings are given in the Appendix.\n\n\\subsection{One-Class OOD Detection}\n\\label{sec:1class}\nWe start with the one-class OOD detection. For a given multi-class dataset of $N_c$ classes, we conduct $N_c$ one-class OOD tasks, where each task regards one of the classes as in-distribution and the remaining classes as out-of-distribution. We run our experiments on three datasets, following prior work \\cite{geom, rot, goad}, of CIFAR-10, CIFAR-100 (super-classes), and ImageNet-30. \n\n\\Cref{tab:one-class} summarizes the results, showing that MOOD outperforms current SOTA of CSI \\cite{csi} on all tested cases significantly. The improvement is of 5.7\\% to 94.9\\% on average. The improvement is comparatively smaller on ImageNet-30 \\cref{tab:one-class-imagenet30}. It is because we do not apply intermediate fine-tuning of the model on ImageNet-30. More details are shown in \\cref{sec:fine-tune}. We provide the class-wise AUROC in the Appendix for detailed exhibition.\n\n%  with OC-SVM\\cite{goad}, DeepSVDD\\cite{deepsvdd}, AnoGAN\\cite{anogan}, OCGAN\\cite{ocgan}, Geom\\cite{geom}, Rot\\cite{rot}, Rot+Trans\\cite{rot}, GOAD\\cite{goad} and CSI\\cite{csi}\n\n%------------------------------multi-class-------------------------------------------\n\n\\subsection{Multi-Class OOD Detection} \n\\label{sec:multi-class}\nFor multi-class OOD Detection, we assume that ID samples are from a specific multi-class dataset. They are tested on various external datasets as out-of-distribution. We perform MOOD on CIFAR-10, CIFAR-100, ImageNet-30 and ImageNet-1k. For CIFAR-10, We consider CIFAR-100 \\cite{cifar}, SVHN \\cite{svhn} and LSUN \\cite{lsun} as OOD datasets. For CIFAR-100, We consider CIFAR-10 \\cite{cifar}, SVHN \\cite{svhn} and LSUN \\cite{lsun} as OOD datasets. For ImageNet-30, OOD samples are from CUB-200 \\cite{cub}, Stanford Dogs \\cite{dogs}, Oxford Pets \\cite{pets}, Oxford Flowers \\cite{flowers}, Food-101 \\cite{food}, Places-365 \\cite{places}, Caltech-256 \\cite{caltech}, and Describable Textures Dataset (DTD) \\cite{dtd}. For ImageNet-1k, we utilize non-natural images as OOD datasets, which includes iNatualist \\cite{inaturalist}, SUN \\cite{sun}, places \\cite{places}, Textures \\cite{dtd}.\n\nAs shown in \\cref{tab:multi-class}, MOOD boosts performance of current SOTA of SSD+ \\cite{ssd} by 3.0\\% to 97.6\\% and SOTA of GradNorm \\cite{gradnorm} by 2.8\\% to 89.1\\% on ImageNet-1k. We remark that when detecting hard (i.e., near-distribution) OOD samples on ImageNet30 and Food, MOOD still yields decent performance, while previous methods often fail. \n\n\\vspace{2mm}\\noindent\\textbf{Visualization.} In \\cref{fig:distribution}, we illustrate the probability distribution of the test samples according to metrics of three OOD detection approaches: baseline OOD detection \\cite{baseline_ood}, SSD+ \\cite{ssd}, and MOOD. The baseline OOD detection performs softmax as its OOD detection metric, where ID samples tend to have greater value than OOD samples. MOOD and SSD+ perform the Mahalanobis distance as their metrics. \n\nAs shown in the figure, the distance of a majority of testing ID samples to the training data is close to zero, demonstrating a similar representation of training and testing ID samples. In contrast, the distances from most OOD samples to the training data are much larger, especially on CIFAR-10 and ImageNet-30. \n\nAlso, in \\cref{fig:distribution}, we reveal that the difference in the distribution of ID and OOD samples according to MOOD is significantly larger compared with other approaches \\cite{baseline_ood, ssd}. It demonstrates that MOOD can separate ID and OOD samples more clearly. In order to illustrate the appearance of images in each ID and OOD dataset, we plot several images as examples with their corresponding distances in the Appendix.\n\n% We compare MOOD with Baseline OOD\\cite{baseline_ood}, ODIN\\cite{odin}, Mahalanobis\\cite{mahalanobis}, Residual Flows\\cite{residual_flows}, Gram Matrix\\cite{gram_matrix}, Outlier exposure\\cite{outlier_exposure}, SupCon\\cite{supcon} and SSD+\\cite{ssd}. \n\n\n%--------------------------------------visualization-----------------------------------\n\\begin{figure}[t]\n  \\centering\n  \\begin{subfigure}{\\linewidth}\n    \\includegraphics[width=0.99\\linewidth, trim=0 0 0 0, clip]{figures/distances_cifar10.pdf}\n    \\caption{ID: CIFAR-10}\n    \\label{fig:distribution_cifar100}\n  \\end{subfigure}\n  \\\\\n  \\begin{subfigure}{\\linewidth}\n    \\includegraphics[width=0.99\\linewidth, trim=0 0 0 0, clip]{figures/distances_cifar100.pdf}\n    \\caption{ID: CIFAR-100}\n    \\label{fig:distribution_cifar100}\n  \\end{subfigure}\n   \\\\\n  \\begin{subfigure}{\\linewidth}\n    \\includegraphics[width=0.99\\linewidth, trim=0 0 0 0, clip]{figures/distances_imagenet30.pdf}\n    \\caption{ID: ImageNet-30}\n    \\label{fig:distribution_imagenet30}\n    \\end{subfigure}\n    \\caption{Line chart to illustrate the relation between the probability distribution of test samples and OOD detection metrics on (a) CIFAR-10, (b) CIFAR-100, and (c) ImageNet-30. Each line in the sub-figures represents an OOD or ID dataset. We compare three OOD detection approaches, including baseline OOD detection, SSD+ (current SOTA, \\cite{ssd} ), and our proposed MOOD. The baseline OOD detection takes the maximum softmax probabilities as its OOD detection metric, while SSD+ and MOOD both use the Mahalanobis distance as their metrics.} %  The greater the difference in the distribution of ID and OOD samples according to a specific metric, the larger the degree of separation between ID and OOD samples, representing the better performance of OOD detection. As can be seen from the probability distribution, our proposed MOOD illustrates obvious priority compared with other OOD detection approaches\n  \\label{fig:distribution}\n\\end{figure}\n\n\n\n%------------------------hard samples---------------------\n\\iffalse\n\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=0.99\\linewidth]{figures/one-class-near-distribution.pdf}\n    \\caption{AUROC (\\%) of near-distribution pairs in one-class detection on CIFAR-10, compared with current SOTA (CSI \\cite{csi}).}\n  \\label{fig:hardood-oneclass}\n\\end{figure}\n\\fi\n\n\\subsection{Near-Distribution OOD Detection} \n\\label{sec:near-distribution}\nCompared with existing approaches on normal OOD detection tasks, SOTA results of near-distribution OOD detection is much worse -- AUROC of some ID-OOD pairs \\cite{csi, ssd} is even lower than 70\\%. Therefore, improving SOTA for near-OOD detection is essential for the application to work on real-world data. \n\n\\begin{table}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{2.8mm}\n\\begin{tabular}{cc|cccccc|cccc}\n\\toprule\nID & OOD  & \\multicolumn{3}{c}{AUROC (\\%)} \\\\\nclass & class  & CSI \\cite{csi} & ours  & (improve)           \\\\\n\\midrule\nPlane   & Automobile   & 74.1 & 99.0  & {\\color{teal}+24.9} \\\\\nPlane   & Ship  & 79.6 & 99.4  & {\\color{teal}+19.8} \\\\\nPlane   & Truck & 82.8 & 98.5  & {\\color{teal}+15.7} \\\\\nBird    & Horse & 83.2 & 94.3  & {\\color{teal}+11.1} \\\\\nCat     & Deer  & 83.3 & 92.6  & {\\color{teal}+9.3}  \\\\\nCat     & Dog   & 67.0 & 75.5  & {\\color{teal}+8.5}  \\\\\nCat     & Frog  & 89.6 & 92.5  & {\\color{teal}+2.9}  \\\\\nCat     & Horse & 79.0 & 95.5  & {\\color{teal}+16.5} \\\\\nDeer    & Horse & 69.0 & 100.0 & {\\color{teal}+31.0}   \\\\\nDog     & Deer  & 88.1 & 96.4  & {\\color{teal}+8.3}  \\\\\nDog     & Horse & 76.6 & 95.5  & {\\color{teal}+18.9} \\\\\nTrunk   & Automobile   & 72.3 & 87.8  & {\\color{teal}+15.5}\\\\\n\\midrule\n\\multicolumn{2}{c|}{Average} & 78.7 & 93.9 & {\\color{teal}+15.2} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Near-distribution OOD detection} (one-class). AUROC (\\%) of near-distribution pairs in one-class detection on CIFAR-10, compared with current SOTA (CSI \\cite{csi}).}\n\\label{tab:hardood-oneclass}\n\\end{table}\n\n\n\n\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=0.99\\linewidth, trim=0 15 0 10 clip]{figures/multi-class-near-distribution.pdf}\n    \\caption{\\textbf{Near-distribution OOD detection} (multi-class). Number of some mistakenly-classified OOD samples (when TPR = 95\\%). These samples are wrongly taken as ID samples by the current SOTA of SSD+ \\cite{ssd} in multi-class detection on CIFAR-10. `*' indicates SOTA.}\n  \\label{fig:hardood-multiclass}\n\\end{figure}\n\nIn \\cref{tab:structure}, we have compared MOOD with the current SOTA on near-distribution CIFAR10-CIFAR100 (ID-OOD) pair, R50+ViT \\cite{oodlimits}, and MOOD outperforms the latter significantly by 2.07\\% to 98.30\\%. In this section, we focus on the hard-detected pairs with similar semantics from \\cref{sec:1class} and \\cref{sec:multi-class}. \n\nFor one-class OOD detection, we adopt 12 hard-detected ID-OOD pairs (AUROC under 90\\%) from the confusion matrix of current one-class OOD detection SOTA of CSI \\cite{csi}. The semantics of these ID-OOD pairs are more similar than normal ID-OOD combinations, such as trunk and car, deer and horse, etc., leading to their poor OOD detection performance. As shown in \\cref{tab:hardood-oneclass}, MOOD significantly boosts the AUROC of current SOTA from 78.7\\% to 93.9\\%. \n\nFor multi-class OOD detection, we examine the large mistakenly-classified value in the OOD-ID confusion matrix, which represents the number of classifying the OOD image to the category in the ID dataset. For example, when the True-Positive Rate (TPR) is 95\\%, 48 testing tiger images from CIFAR-100 are classified as cat by the current multi-class OOD detection SOTA method of SSD+ \\cite{ssd}, while only 2 of them are wrongly classified by MOOD. More results are shown in \\cref{fig:hardood-multiclass}. For the listed 12 ID-OOD pairs, MOOD averagely reduces the number of mistakenly-classified OOD samples notably by 79\\%. \n\n\n\n\n\\begin{table}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{3.1mm}\n\\begin{tabular}{c|ccccccccccc}\n\\toprule\n\\multirow{2}{*}{Method}  & \\# OOD samples & \\multirow{2}{*}{AUROC(\\%)} \\\\\n                         & per class  &                           \\\\\n\\midrule\n\\multirow{5}{*}{R50+ViT (SOTA) \\cite{oodlimits}} & 0          & 98.52                     \\\\\n                         & 1          & 98.96                     \\\\\n                         & 2          & 99.11                     \\\\\n                         & 3          & 99.17                     \\\\\n                         & 10         & 99.29                     \\\\\n\\midrule\nours           & 0          & \\textbf{99.41}            \\\\\n(improve)                & -          & {\\color{teal}+0.12}      \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Outlier Exposure OOD detection.} AUROC (\\%) of current SOTA of R50+ViT \\cite{oodlimits} for near-distribution OOD detection and MOOD. SOTA utilizes up to 10 known OOD samples per class for detection, while ours do not include any OOD samples.}\n\\label{tab:exposure}\n\\end{table}\n\n\\subsection{OOD Detection with Outlier Exposure} \n\\label{sec:exposure}\nSeveral representative OOD detection methods \\cite{ssd, oodlimits} utilize OOD samples to improve the performance in extra stages. We note they are not included in our work because we generally believe that exposure of OOD samples violates the original intention of OOD detection. \n\nIn \\cref{tab:exposure}, we compare MOOD with current SOTA \\cite{oodlimits} for near-distribution OOD detection with up to 10 OOD samples per class. We surprisingly find that MOOD works better in terms of AUROC than current SOTA \\cite{oodlimits}, even though we do not include any OOD samples for detection. The outstanding performance of MOOD demonstrates that an effective pretext task is already sufficient for producing a distinguishable representation that OOD detection requires. Thus, there is no need to include extra OOD samples.\n\n\n%==================================Conclusion=========================================\n\n\n\\section{Conclusion}\nIn this paper, we have extensively explored the effect of multiple contributors for OOD detection and observed that reconstruction-based pretext tasks have the potential to provide effective priors for OOD detection to learn the real data distribution of the ID dataset. Specifically, we take the Masked Image Modeling pretext task for our OOD detection framework (MOOD). We perform MOOD on one-class OOD detection, multi-class OOD detection, near-distribution OOD detection, and few-shot outlier exposure OOD detection -- MOOD all achieve new SOTA results, although we do not include any OOD samples for detection.\n\n\\section{Acknowledgement}\nThis work is partially supported by Shenzhen Science and Technology Program KQTD20210811090149095.\n. The first paper ends. The second paper begins. \\IEEEtitleabstractindextext{%\n\\begin{abstract}\n\\revise{The crux of effective out-of-distribution (OOD) detection lies in acquiring a robust in-distribution (ID) representation, distinct from OOD samples. While previous methods predominantly leaned on recognition-based techniques for this purpose, they often resulted in shortcut learning, lacking comprehensive representations. In our study, we conducted a comprehensive analysis, exploring distinct pretraining tasks and employing various OOD score functions. The results highlight that the feature representations pre-trained through reconstruction yield a notable enhancement and narrow the performance gap among various score functions. This suggests that even simple score functions can rival complex ones when leveraging reconstruction-based pretext tasks. Reconstruction-based pretext tasks adapt well to various score functions. As such, it holds promising potential for further expansion. Our OOD detection framework, MOODv2, employs the masked image modeling pretext task. Without bells and whistles, MOODv2 impressively enhances 14.30\\% AUROC to 95.68\\% on ImageNet and achieves 99.98\\% on CIFAR-10.}\n\\end{abstract}\n\n% Note that keywords are not normally used for peerreview papers.\n\\begin{IEEEkeywords}\nComputer Vision, Out-of-Distribution Detection, Outlier Detection, Masked Image Modeling\n\\end{IEEEkeywords}\n\n% Codes are available at \\href{}{https://github.com/JulietLJY/MOOD}\n}\n\n\\maketitle\n\\IEEEdisplaynontitleabstractindextext\n\\IEEEpeerreviewmaketitle\n\n\\IEEEraisesectionheading{\\section{Introduction}\\label{sec:intro}}\n\\IEEEPARstart{A} reliable visual recognition system not only provides correct predictions on known context (also known as in-distribution data) but also detects unknown out-of-distribution (OOD) samples and rejects (or transfers) them to human intervention for safe handling. This motivates the applications of outlier detectors before feeding input to the downstream networks, which is the main task of OOD detection, also referred to as novelty or anomaly detection. OOD detection is the task of identifying whether a test sample is drawn far from the in-distribution (ID) data or not. It is at the cornerstone of various safety-critical applications, including medical diagnosis \\cite{caruana2015intelligible}, fraud detection \\cite{phua2010comprehensive}, autonomous driving \\cite{eykholt2018robust}, etc~\\cite{tagclip, motcoder, bal}. A representative in-distribution feature space representation is crucial for out-of-distribution detection. A well-crafted feature representation significantly enhances the performance via most mainstream OOD detection score functions. Our research is dedicated to refining feature representations tailored for OOD detection, with the aim of advancing the entire field.\n\nExisting methods perform contrastive learning \\cite{csi, ssd} or pretrain classification on a large dataset \\cite{oodlimits, vim, yang2021generalized, sariyildiz2023fake} to detect OOD samples. The former methods classify images according to the pseudo labels while the latter classifies images based on ground truth, whose core tasks are both to fulfill the classification target. However, research on backdoor attack \\cite{backdoor_attack, frog_attack} shows that when learning is represented by classifying data, networks tend to take a shortcut to classify images. In a typical backdoor attack scene \\cite{frog_attack}, the attacker adds secret triggers on original training images with the visibly correct label. During the course of testing, the victim model classifies images with secret triggers into the wrong category. Research in this area demonstrates that networks only learn specific distinguishable patterns of different categories because it is a shortcut to fulfill the classification requirement. Nonetheless, learning these patterns is ineffective for OOD detection. Thus, learning representations by classifying ID data for OOD detection may not be satisfying. For example, when patterns similar to some ID categories appear in OOD samples, the network could easily interpret these OOD samples as the ID data and classify them into the wrong ID categories\\revise{, as shown in \\cref{fig:intro}}. \n\n\n\\begin{figure}[t]\n  \\centering    \n    \\includegraphics[width=0.8\\linewidth]{figures/ablation_imagenet.pdf}\n    \\caption{\\re{The average AUROC (\\%) tested on four OOD datasets applied to a ViT model with different pre-text tasks. \n    Methods in blue use the feature space;\n    methods in green use logits;\n    methods in yellow use the softmax probability;\n    and methods in red use both features and logits. The stars show the average performance of a category of methods.}}\n  \\label{fig:ablation}\n\\end{figure}\n\n\n\\begin{figure*}[t]\n  \\centering\n    \\includegraphics[width=0.9\\linewidth, trim=75 225 380 120, clip]{figures/mood.pdf}\n    \\caption{\\revise{Comparison of reconstruction-based and classification-based methods. In the context of image classification, networks often take a shortcut when categorizing images \\cite{backdoor_attack, frog_attack}. For example, ears are a distinctive feature for distinguishing between cats and dogs, and a classification model typically assumes that animals with pointed ears are cats, while those without are dogs. Consequently, when the network encounters an out-of-distribution animal, such as a fox with pointed ears, it readily misclassifies it as a cat. In contrast, reconstruction-based tasks effectively mitigate this issue. By randomly masking portions of images, the model avoids learning localized, stereotypical features (e.g., masked ears), thus preventing shortcuts and instead acquiring effective pixel-level representations for ID data. This significantly improves the model's ability to detect OOD instances.}}\n  \\label{fig:intro}\n\\end{figure*}\n\n\n\\iffalse\n\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=0.99\\linewidth]{figures/impression.pdf}\n    \\caption{\\revise{The AUROC (in percentage) of eight OOD detection algorithms applied to a ViT model with five pre-text tasks.The OOD datasets are ImageNet-O ($x$-axis) and OpenImage-O ($y$-axis).}}\n  \\label{fig:impression}\n\\end{figure}\n\\fi\n\n\\revise{To remedy this issue, we introduce the reconstruction-based pretext task. Different from contrastive learning in existing OOD detection approaches \\cite{csi, ssd}, our method forces the network to achieve the training purpose of reconstructing the image and thus makes it learn pixel-level feature representation. Specifically, we adopt the masked image modeling (MIM) \\cite{beit} as our self-supervised pretext task, which has been demonstrated to have great potential in both natural language processing \\cite{bert} and computer vision \\cite{beit, mae}. In the MIM task, a proportion of image patches are randomly masked. The network learns information from the remaining patches to speculate the masked patches and restore tokens of the original image. The reconstruction process enables the model to learn from the prior effective ID feature representation rather than just learning different patterns among categories in the classification process. In our work, we observed that the pre-trained models effectively reconstruct ID images, whereas they exhibit distinct domain differences when it comes to the OOD domain (\\cref{fig:recover}). This visual discrepancy clearly underscores the existing domain gap in model features between ID and OOD data, offering valuable insights for OOD detection.}\n\n\\revise{To validate the effectiveness of our ID feature representation, we conduct experiments to test its performance with various mainstream OOD detection score functions. We employed OOD score functions encompassing probability-based \\cite{baseline_ood, maxlogit}, logits-based \\cite{energy, maxlogit}, features-based \\cite{vim, react, mahalanobis}, and hybrid methods utilizing both logits and features \\cite{vim}. In the context of a comparative analysis spanning classic classification \\cite{vit}, contrastive learning \\cite{mocov3, dinov2}, and masked image modeling pretext tasks \\cite{beit, beitv2}, our findings underscore the dominant role of reconstruction-based strategies in the field of OOD detection, as illustrated in \\cref{fig:ablation}.}\n\n\\revise{Furthermore, we conduct a comprehensive analysis of the experimental results and observe that our approach not only significantly improves the overall results but also substantially reduces the disparities among score functions. This observation underscores that even simple score functions can perform on par with more complex ones when a representative ID feature representation is utilized. These findings further emphasize the critical importance of effective feature representation in OOD detection. More details are in \\cref{sec:pretask}.} \\revise{Ultimately, MOODv2 demonstrates remarkable enhancements, achieving a substantial 14.30\\% increase, reaching 95.68\\% AUROC on ImageNet. On CIFAR-10, our results significantly improved to an impressive 99.98\\%, marking a notable 0.35\\% enhancement compared to the previous state-of-the-art.}\n\n\n\n\\section{Related Works}\\label{sec:related}\n\n\\subsection{Out-of-distribution Detection}\n\\revise{Many scoring functions have been developed by researchers to distinguish between in-distribution and out-of-distribution examples. These functions are designed to exploit properties that are typically exhibited by ID examples but violated by OOD examples, and vice versa. These scores are primarily derived from three sources:}\n\\begin{enumerate}\n\n    \\item \\revise{\\textbf{Probability-based}: This category includes measures like the maximum softmax probabilities~\\cite{baseline_ood} and the minimum KL-divergence between the softmax and the mean class-conditional distributions~\\cite{maxlogit}, etc.}\n\n    \\item \\revise{\\textbf{Logit-based}: These functions rely on maximum logits~\\cite{maxlogit} and the \\(\\mathrm{logsumexp}\\) function computed over logits~\\cite{energy}, etc.}\n\n    \\item \\revise{\\textbf{Feature-based}: These functions involve the norm of the residual between a feature and the pre-image of its low-dimensional embedding~\\cite{ndiour2020out} and the minimum Mahalanobis distance between a feature and the class centroids~\\cite{mahalanobis}, among others.}\n\n\\end{enumerate}\n\\revise{After a thorough analysis of the performance and their correlations with various score functions and pretext tasks, our work follows the hybrid methods combining logit and feature \\cite{vim}} and includes the reconstruction-based methods as a pretext task. We will explain the implementation details later in this paper.\n\n\\subsection{Self-Supervised Pretext Task}\n\\revise{In the ever-evolving landscape of computer vision and deep learning, a multitude of strategies and techniques have been devised to enhance the capacity of models to understand and process visual data\\re{:}}\n\n\\begin{enumerate}\n\n    \\item \\revise{\\textbf{Classification task:} Vision models are pre-trained via classical classification task \\cite{vit}.}\n\n    \\item \\revise{\\textbf{Contrastive Learning tasks: }MOCOv3 \\cite{mocov3} and DINOv2 \\cite{dinov2} are advanced contrastive learning methods used for self-supervised representation learning. These methods focus on learning representations by contrasting positive pairs (e.g., different augmentations of the same image) with negative pairs (e.g., augmentations from different images). MOCOv3 extends the MOCO framework \\cite{moco} with a momentum encoder and dynamic queues for improved performance. DINOv2 introduces a clustered teacher network and an asymmetric loss to learn efficient representations.}\n\n    \\item \\revise{\\textbf{Masked Image Modeling Tasks: } Data-Efficient Image Transformer (BEiT series \\cite{beit, beitv2}) are self-supervised learning tasks that involve masked image modeling. In these tasks, a portion of an image is randomly masked, and the model's objective is to predict the masked pixels, effectively filling in the blanks.}\n    \n\\end{enumerate}\n\n\\revise{These methods and tasks represent cutting-edge approaches in the field of computer vision and deep learning. They have led to substantial improvements in the ability of models to learn useful visual representations from unlabeled data, enabling better performance on various downstream vision tasks.}\n\nMultiple existing methods take advantage of self-supervised tasks to guide the learning of representation for OOD detection. Previous work \\cite{csi, ssd} presents contrastive learning models as feature extractors. However, existing approaches of classifying transformed images according to contrastive learning possess similar limitations -- that is, the model tends to learn the specific patterns of categories \\cite{backdoor_attack, backdoor_survey}, which are beneficial for classification but do not help understand the intrinsic ID representation. In our work, we address this issue by performing the masked image modeling task for OOD detection.\n\n\\subsection{Training Strategy}\n\\revise{Numerous approaches have been developed to address OOD-awareness in training loss~\\cite{confbranch2018arxiv}. These methods often involve the introduction of regularization terms aimed at encouraging a clearer separation between ID and OOD features~\\cite{onedim21cvpr,huang2021mos}. In some cases, networks are augmented with confidence estimation branches, utilizing misclassified in-distribution examples as proxies for out-of-distribution ones~\\cite{confbranch2018arxiv}. MOS~\\cite{huang2021mos} adapts the loss function by incorporating a predefined group structure, enabling the minimum group-wise ``else\" class probability to serve as an indicator of OOD classification. An alternative approach~\\cite{onedim21cvpr} focuses on compelling ID samples to embed into a union of 1-dimensional subspaces during training, and it evaluates the minimum angular distance between the feature and class-wise subspaces. }\n\n\\revise{In contrast to these approaches, our method belongs to the lightweight training-free methods \\cite{vim, MOOD}, which doesn't necessitate retraining the model. Therefore, it not only offers a more straightforward application but also preserves the accuracy of in-distribution classification.}\n\n\n\\begin{figure}[t]\n  \\centering\n    \\subfloat[\\re{ID: ImageNet}]{\\includegraphics[width=0.99\\linewidth]{figures/compare_moodv1_imagenet.pdf}}\n    \n    \\subfloat[\\re{ID: CIFAR-10}]{\\includegraphics[width=0.99\\linewidth]{figures/compare_moodv1_cifar10.pdf}}\n    % \\includegraphics[width=0.99\\linewidth]{figures/compare_moodv1_imagenet.pdf}\n    \\caption{\\re{The AUROC (\\%) of MOODv2 and MOODv1 tested on ID datasets (a) ImageNet and (b) CIFAR-10. OOD datasets including OpenImage-O \\cite{openimages_o}, Texture \\cite{dtd}, iNaturalist \\cite{inaturalist}, and ImageNet-O \\cite{imagenet_o}. }}\n  \\label{fig:moodv1}\n\\end{figure}\n\n\\begin{figure*}[t]\n  \\centering\n    \\includegraphics[width=0.99\\linewidth, trim=20 10 20 10]{figures/plot_recover_images.pdf}\n    \\caption{\\revise{Each image pair consists of the original image (left) and reconstructed image (right). The rows of images are sourced from ImageNet \\cite{imagenet}, Texture \\cite{cimpoi14describing}, iNaturalist \\cite{van2018inaturalist}, ImageNet-O \\cite{hendrycks2021natural}, and OpenImage-O \\cite{openimages_o}. The number in the top left corner of each image pair represents the Euclidean distance between the two images.}}\n  \\label{fig:recover}\n\\end{figure*}\n\n\\begin{table}[t]\n\\small\n\\centering\n\n\\subfloat[ID: CIFAR-10]{\n\\setlength{\\tabcolsep}{1.1mm}\n\\begin{tabular}{c|ccccccccc}\n\\toprule\nMethods & prob & feat & logit & feat+logit \\\\\n\\midrule\nViT\\cite{vit} & \\revise{73.61\\scriptsize{$\\pm$21.36}} & \\revise{82.61\\scriptsize{$\\pm$23.81}} & \\revise{45.11\\scriptsize{$\\pm$4.45}} & \\revise{\\textbf{99.63}} \\\\\nMoCov3\\cite{mocov3} & \\revise{70.96\\scriptsize{$\\pm$23.68}} & \\revise{79.17\\scriptsize{$\\pm$28.75}} & \\revise{41.42\\scriptsize{$\\pm$3.50}} & \\revise{\\textbf{99.73}} \\\\\nDINOv2\\cite{dinov2} & \\revise{87.20\\scriptsize{$\\pm$10.62}} & \\revise{84.73\\scriptsize{$\\pm$21.57}} & \\revise{80.30\\scriptsize{$\\pm$0.10}} & \\revise{\\textbf{99.98}} \\\\\n\\rowcolor{gray!20}BEiTv2\\cite{beitv2} & \\revise{79.96\\scriptsize{$\\pm$13.71}} & \\revise{91.77\\scriptsize{$\\pm$11.47}} & \\revise{72.87\\scriptsize{$\\pm$2.08}} & \\revise{\\textbf{99.87}} \\\\\n\\rowcolor{gray!20}BEiT\\cite{beit} & \\revise{77.51\\scriptsize{$\\pm$17.83}} & \\revise{89.05\\scriptsize{$\\pm$15.46}} & \\revise{65.05\\scriptsize{$\\pm$2.06}} & \\revise{\\textbf{99.98}} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\subfloat[ID: ImageNet]{\n\\setlength{\\tabcolsep}{1.3mm}\n\\begin{tabular}{c|ccccccccc}\n\\toprule\nMethods & prob & feat & logit & feat+logit \\\\\n\\midrule\nViT\\cite{vit} & \\revise{\\textbf{78.52}\\scriptsize{$\\pm$1.76}} & \\revise{76.86\\scriptsize{$\\pm$3.20}} & \\revise{70.61\\scriptsize{$\\pm$4.76}} & \\revise{77.65} \\\\\nMoCov3\\cite{mocov3} & \\revise{\\textbf{78.36}\\scriptsize{$\\pm$1.42}} & \\revise{72.51\\scriptsize{$\\pm$6.21}} & \\revise{70.61\\scriptsize{$\\pm$5.04}} & \\revise{72.07} \\\\\nDINOv2\\cite{dinov2} & \\revise{59.64\\scriptsize{$\\pm$7.82}} & \\revise{\\textbf{63.56}\\scriptsize{$\\pm$2.89}} & \\revise{60.70\\scriptsize{$\\pm$4.51}} & \\revise{61.32} \\\\\n\\rowcolor{gray!20}BEiTv2\\cite{beitv2} & \\revise{89.07\\scriptsize{$\\pm$0.24}} & \\revise{92.96\\scriptsize{$\\pm$1.27}} & \\revise{90.29\\scriptsize{$\\pm$0.13}} & \\revise{\\textbf{95.42}} \\\\\n\\rowcolor{gray!20}BEiT\\cite{beit} & \\revise{89.47\\scriptsize{$\\pm$0.47}} & \\revise{93.30\\scriptsize{$\\pm$1.89}} & \\revise{89.84\\scriptsize{$\\pm$0.01}} & \\revise{\\textbf{95.68}} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\caption{\\revise{The AUROC (\\%) of four types of methods: probability-based methods MSP \\cite{baseline_ood} and KL-Matching \\cite{maxlogit}; logits-based methods Energy \\cite{energy} and MaxLogit \\cite{maxlogit}; features-based methods Residual \\cite{vim}, React \\cite{react} and Mahalanobis \\cite{mahalanobis}; and methods using both logits and features include ViM \\cite{vim}. The best method for each model is emphasized in bold.}\n}\n\\label{tab:ablation-statistic}\n\\end{table}\n\n\n\\subsection{MOODv1}\n\\revise{Our previous version MOODv1 \\cite{MOOD} has introduced masked image modeling pretraining strategy into the OOD detection (MOOD) and achieved promising results. However, there are still concerns:}\n\n\\revise{Firstly, previous studies \\cite{MOOD, csi, ssd} have typically necessitated fine-tuning a model on each in-distribution dataset. The expense of training becomes notably high when dealing with a substantial number of ID datasets to be assessed, such as in one-class OOD detection \\cite{csi, MOOD}. However, through experimental validation, we have discovered that a well-prepared masked image modeling model doesn't require additional fine-tuning to achieve outstanding detection performance, conserving substantial fine-tuning resource consumption when dealing with a plethora of ID datasets that require evaluation.}\n\n\\revise{Secondly, as the field has seen the emergence of more advanced OOD score functions \\cite{maxlogit, vim, react, baseline_ood, energy, mahalanobis} and pretraining techniques \\cite{beitv2, dinov2, mocov3, beit, vit}, it raises the question of whether masked image modeling continues to maintain its leading role. In MOODv2, we integrate the latest advancements in pretraining methods and conduct experiments with an array of state-of-the-art OOD score functions. This broader spectrum of pretraining methods and score functions allows for a more comprehensive assessment of the MOODv2's performance, better aligning MOODv2 with the increasingly intricate challenges of OOD detection. }\n\n\\revise{Lastly, it is well known that if the network has seen similar samples in training, regardless of pre-training or fine-tuning, the OOD performance will be more or less trivial \\cite{openimages_o}. Previous works \\cite{oodlimits, MOOD} rely on pre-training on ImageNet-21K, so that the benchmark OOD dataset such as CIFAR \\cite{cifar}, Places \\cite{places}, etc., is unlikely to be untouched by the ImageNet-21K \\cite{imagenet} dataset. In this work, MOODv2 introduces the latest unnatural datasets as OOD, which rules out the possibility of overlap between the OOD test set and the training set \\cite{openimages_o, imagenet_o}.}\n\n\n\\revise{In summary, MOODv2 incorporates improved score functions, advanced pretraining techniques, a wider range of unnatural OOD datasets, and a streamlined general framework. The performance improvement of MOODv2 compared to MOODv1 is depicted in Fig. \\ref{fig:moodv1}. On ImageNet, MOODv2 exhibits a noteworthy 2.17\\% improvement in AUROC compared to MOODv1. Furthermore, on CIFAR-10, MOODv2, without finetuning on the ID dataset, achieves an exceptional AUROC score of up to 99.98\\%. }\n\n\\section{Methods}\\label{sec:methods}\n\n\n\n% \\iffalse\n\\begin{table}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{0.5mm}\n\\begin{tabular}{c|cccccccccc|c}\n\\toprule\nDatasets & ImageNet & Texture & iNaturalist & ImageNet-O & OpenImage-O \\\\\n\\midrule\nDistance & 18.09 & 32.89 & 38.96 & 30.76 & 45.56 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Difference.} .}\n\\label{tab:difference}\n\\end{table}\n\\fi \n\n\\begin{table}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{3mm}\n\\begin{tabular}{c|cc}\n\\toprule\nID/OOD & Datasets & Recovery Distance \\\\\n\\midrule\nID & ImageNet (ID) \\cite{imagenet} & \\revise{18.09} \\\\\n\\midrule\n\\multirow{4}{*}{OOD} & Texture \\cite{dtd} & \\revise{32.89} \\\\\n& INaturalist \\cite{inaturalist}& \\revise{38.96} \\\\\n& ImageNet-O \\cite{imagenet_o} & \\revise{30.76} \\\\\n& OpenImage-O \\cite{openimages_o} & \\revise{45.56} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\revise{The average recovery distance (L2 norm between the original images and their corresponding reconstructions) for ID and OOD datasets. For each dataset, the number of sampled images is the minimum of 5000 and the dataset size.}}\n\\label{tab:difference}\n\\end{table}\n\n% We first define the notations. For a given dataset $X_{\\rm ID}$, the goal of out-of-distribution (OOD) detection is to model a detector that identifies whether an input image $x \\in X_{\\rm ID}$ or $x \\notin X_{\\rm ID}$ (that is, $x \\in X_{\\rm OOD}$). A majority of existing methods for OOD detection define an OOD score function $s(x)$. Its abnormal high or low value represents that $x$ is from out-of-distribution. \n\n\\revise{In this section, we initiate our exploration of reconstruction tasks for OOD detection by presenting the underlying motivation in \\cref{sec:motivation}. Following that, in \\cref{sec:pretask}, we delve into a comprehensive analysis of the essential attributes that play a pivotal role in OOD detection.}\n\n\\subsection{Motivation: seeking for effective ID representation} \n\\label{sec:motivation}\n\n% In this section, we choose the pretext task that can provide the intrinsic prior to suit the OOD detection task. \nMost previous OOD methods learn the ID representation through classification \\cite{baseline_ood, oodlimits} or contrastive learning \\cite{csi, ssd} on ID samples, which take advantage of either the ground truth or pseudo labels to supervise the classification networks. On the other hand, work of \\cite{backdoor_attack, frog_attack} shows that classification networks only learn different patterns among training categories because it is a shortcut to fulfill classification. \\revise{It is indicated that the network actually does not learn the effective in-distribution representation. In comparison, the reconstruction-based pretext task forces the network to learn the pixel-level image representation of the ID images during training to reconstruct the image instead of the patterns for classification. In this way, the network can learn a more representative feature of the ID dataset.}\n\n\\iffalse\n\\begin{equation}\nx_{r} = f_{e}\\cdot f_{d}(x),\n\\end{equation}\nwhere $x$ and $x_{r}$ represent the original and reconstructed images. $f_{e}$ and $f_{d}$ correspond to the image encoder and decoder, and $f_{e}$ needs to learn representative features from the training dataset to reconstruct images. To assess the benefits of using the pretask features, we compute the recovery distance between the original and reconstructed images as follows:\n\\begin{equation}\nd_r = \\left\\lVert x - x_{r} \\right\\rVert.\n\\end{equation}\n\\fi\n\\revise{To verify this, we reconstruct ID and OOD data and compute the Euclidean distance between the original and reconstructed images. A greater distance indicates a larger deviation of the reconstructed image from the original image. We collect recovery distances for ID and OOD data. Examples of the reconstruction are depicted in \\cref{fig:recover}. In the first row, for ID images, pre-trained models reconstruct the images effectively. Instead, for unnatural OOD images in the following rows, clear domain discrepancies emerge. For instance, in the case of textured images, the models still apply lighting and shadows reminiscent of natural images. In the case of sketch images,  the models render the images smoother and brighter. This discrepancy visually highlights the domain gap in model features between ID and OOD data, which can be leveraged for OOD detection.}\n\n\n\n\\begin{figure*}[t]\n  \\centering\n    \\subfloat[\\re{ID: CIFAR-10}]{\\includegraphics[width=0.99\\linewidth, trim=10 15 5 15]{figures/detailed_ablation_cifar10.pdf}}\n    \n    \\subfloat[\\re{ID: ImageNet}]{\\includegraphics[width=0.99\\linewidth, trim=10 15 5 15]{figures/detailed_ablation_imagenet.pdf}}\n\n    \\caption{\\revise{The AUROC (\\%) tested on unnatural OOD datasets of various OOD detection algorithms applied to a ViT model. \n    The pre-text tasks include classification task \\cite{vit}, contrastive learning tasks MoCov3 \\cite{mocov3} and DINOv2 \\cite{dinov2}, and masked image modeling tasks BEiT series \\cite{beit,beitv2}.\n    Methods in blue utilize the feature space;\n    methods in green use logits;\n    methods in yellow make use of the softmax probability.\n    and methods in red leverage both features and logits.\n    Stars represent the average AUROC for methods in the corresponding colors; light vertical lines represent the standard deviation.\n    }}\n    \n    % probability-based methods MSP \\cite{baseline_ood} and KL-Matching \\cite{maxlogit}; logits-based methods Energy \\cite{energy} and MaxLogit \\cite{maxlogit}; features-based methods Residual \\cite{vim}, React \\cite{react} and Mahalanobis \\cite{mahalanobis}; and methods using both logits and features include ViM \\cite{vim}.\n    \n  \\label{fig:detailed_ablation}\n\\end{figure*}\n\n\\subsection{Reconstruction Tasks for OOD Detection} \\label{sec:pretask}\n%--------------------------------------multi-class-----------------------------------\n\\begin{table*}[tp]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{0.6mm}\n\\begin{tabular}{c|c|ccccccccccc}\n\\toprule\n\\multirow{2}{*}{Methods} & \\multirow{2}{*}{Models} & \\multicolumn{2}{c}{Texture \\cite{dtd}} & \\multicolumn{2}{c}{iNaturalist \\cite{inaturalist}} & \\multicolumn{2}{c}{ImageNet-O \\cite{imagenet_o}} & \\multicolumn{2}{c}{ OpenImage-O \\cite{openimages_o}} & \\multicolumn{2}{c}{Average} \\\\\n& & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ \\\\\n\\midrule\n\\multirow{5}{*}{MSP\\cite{baseline_ood}}\n& ViT\\cite{vit} & \\revise{71.31} & \\revise{71.31} & \\revise{90.70} & \\revise{90.70} & \\revise{60.77} & \\revise{60.77} & \\revise{84.29} & \\revise{84.29} & \\revise{76.77} & \\revise{76.77} \\\\\n& MoCov3\\cite{mocov3} & \\revise{66.85} & \\revise{66.85} & \\revise{90.68} & \\revise{90.68} & \\revise{64.80} & \\revise{64.80} & \\revise{85.42} & \\revise{85.42} & \\revise{76.94} & \\revise{76.94} \\\\\n& DINOv2\\cite{dinov2} & \\revise{47.49} & \\revise{47.49} & \\revise{62.13} & \\revise{62.13} & \\revise{44.87} & \\revise{44.87} & \\revise{52.83} & \\revise{52.83} & \\revise{51.83} & \\revise{51.83} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.61}}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.61}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.05}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.05}}} & \\cellcolor{gray!20}{\\revise{81.15}} & \\cellcolor{gray!20}{\\revise{81.15}} & \\cellcolor{gray!20}{\\revise{\\textbf{92.52}}} & \\cellcolor{gray!20}{\\revise{\\textbf{92.52}}} & \\cellcolor{gray!20}{\\revise{88.83}} & \\cellcolor{gray!20}{\\revise{88.83}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{85.05}} & \\cellcolor{gray!20}{\\revise{85.05}} & \\cellcolor{gray!20}{\\revise{95.50}} & \\cellcolor{gray!20}{\\revise{95.50}} & \\cellcolor{gray!20}{\\revise{\\textbf{83.17}}} & \\cellcolor{gray!20}{\\revise{\\textbf{83.17}}} & \\cellcolor{gray!20}{\\revise{92.28}} & \\cellcolor{gray!20}{\\revise{92.28}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.00}}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.00}}} \\\\\n\\midrule\n\\multirow{5}{*}{Energy\\cite{energy}}\n& ViT\\cite{vit} & \\revise{54.11} & \\revise{54.11} & \\revise{76.61} & \\revise{76.61} & \\revise{61.63} & \\revise{61.63} & \\revise{71.06} & \\revise{71.06} & \\revise{65.85} & \\revise{65.85} \\\\\n& MoCov3\\cite{mocov3} & \\revise{48.79} & \\revise{48.79} & \\revise{76.80} & \\revise{76.80} & \\revise{64.56} & \\revise{64.56} & \\revise{72.13} & \\revise{72.13} & \\revise{65.57} & \\revise{65.57} \\\\\n& DINOv2\\cite{dinov2} & \\revise{73.89} & \\revise{73.89} & \\revise{80.34} & \\revise{80.34} & \\revise{49.98} & \\revise{49.98} & \\revise{56.64} & \\revise{56.64} & \\revise{65.21} & \\revise{65.21} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.32}}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.32}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.95}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.95}}} & \\cellcolor{gray!20}{\\revise{85.27}} & \\cellcolor{gray!20}{\\revise{85.27}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.14}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.14}}} & \\cellcolor{gray!20}{\\revise{\\textbf{90.42}}} & \\cellcolor{gray!20}{\\revise{\\textbf{90.42}}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{83.04}} & \\cellcolor{gray!20}{\\revise{83.04}} & \\cellcolor{gray!20}{\\revise{96.48}} & \\cellcolor{gray!20}{\\revise{96.48}} & \\cellcolor{gray!20}{\\revise{\\textbf{86.36}}} & \\cellcolor{gray!20}{\\revise{\\textbf{86.36}}} & \\cellcolor{gray!20}{\\revise{93.50}} & \\cellcolor{gray!20}{\\revise{93.50}} & \\cellcolor{gray!20}{\\revise{89.85}} & \\cellcolor{gray!20}{\\revise{89.85}} \\\\\n\\midrule\n\\multirow{5}{*}{MaxLogit\\cite{maxlogit}}\n& ViT\\cite{vit} & \\revise{67.22} & \\revise{67.22} & \\revise{89.88} & \\revise{89.88} & \\revise{61.68} & \\revise{61.68} & \\revise{82.73} & \\revise{82.73} & \\revise{75.37} & \\revise{75.37} \\\\\n& MoCov3\\cite{mocov3} & \\revise{62.36} & \\revise{62.36} & \\revise{90.38} & \\revise{90.38} & \\revise{65.65} & \\revise{65.65} & \\revise{84.19} & \\revise{84.19} & \\revise{75.64} & \\revise{75.64} \\\\\n& DINOv2\\cite{dinov2} & \\revise{54.70} & \\revise{54.70} & \\revise{69.98} & \\revise{69.98} & \\revise{45.60} & \\revise{45.60} & \\revise{54.52} & \\revise{54.52} & \\revise{56.20} & \\revise{56.20} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.94}}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.94}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.90}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.90}}} & \\cellcolor{gray!20}{\\revise{83.97}} & \\cellcolor{gray!20}{\\revise{83.97}} & \\cellcolor{gray!20}{\\revise{\\textbf{93.82}}} & \\cellcolor{gray!20}{\\revise{\\textbf{93.82}}} & \\cellcolor{gray!20}{\\revise{\\textbf{90.16}}} & \\cellcolor{gray!20}{\\revise{\\textbf{90.16}}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{84.17}} & \\cellcolor{gray!20}{\\revise{84.17}} & \\cellcolor{gray!20}{\\revise{96.48}} & \\cellcolor{gray!20}{\\revise{96.48}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.34}}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.34}}} & \\cellcolor{gray!20}{\\revise{93.31}} & \\cellcolor{gray!20}{\\revise{93.31}} & \\cellcolor{gray!20}{\\revise{89.83}} & \\cellcolor{gray!20}{\\revise{89.83}} \\\\\n\\midrule\n\\multirow{5}{*}{KL-Matching\\cite{maxlogit}}\n& ViT\\cite{vit} & \\revise{82.59} & \\revise{82.59} & \\revise{87.63} & \\revise{87.63} & \\revise{66.55} & \\revise{66.55} & \\revise{84.34} & \\revise{84.34} & \\revise{80.28} & \\revise{80.28} \\\\\n& MoCov3\\cite{mocov3} & \\revise{82.35} & \\revise{82.35} & \\revise{86.24} & \\revise{86.24} & \\revise{67.80} & \\revise{67.80} & \\revise{82.73} & \\revise{82.73} & \\revise{79.78} & \\revise{79.78} \\\\\n& DINOv2\\cite{dinov2} & \\revise{80.51} & \\revise{80.51} & \\revise{56.93} & \\revise{56.93} & \\revise{69.77} & \\revise{69.77} & \\revise{62.63} & \\revise{62.63} & \\revise{67.46} & \\revise{67.46} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{87.14}} & \\cellcolor{gray!20}{\\revise{87.14}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.13}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.13}}} & \\cellcolor{gray!20}{\\revise{82.87}} & \\cellcolor{gray!20}{\\revise{82.87}} & \\cellcolor{gray!20}{\\revise{92.10}} & \\cellcolor{gray!20}{\\revise{92.10}} & \\cellcolor{gray!20}{\\revise{89.31}} & \\cellcolor{gray!20}{\\revise{89.31}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{\\textbf{87.87}}} & \\cellcolor{gray!20}{\\revise{\\textbf{87.87}}} & \\cellcolor{gray!20}{\\revise{94.82}} & \\cellcolor{gray!20}{\\revise{94.82}} & \\cellcolor{gray!20}{\\revise{\\textbf{84.56}}} & \\cellcolor{gray!20}{\\revise{\\textbf{84.56}}} & \\cellcolor{gray!20}{\\revise{\\textbf{92.48}}} & \\cellcolor{gray!20}{\\revise{\\textbf{92.48}}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.93}}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.93}}} \\\\\n\\midrule\n\\multirow{5}{*}{Residual\\cite{vim}}\n& ViT\\cite{vit} & \\revise{82.39} & \\revise{82.39} & \\revise{73.72} & \\revise{73.72} & \\revise{68.44} & \\revise{68.44} & \\revise{74.88} & \\revise{74.88} & \\revise{74.86} & \\revise{74.86} \\\\\n& MoCov3\\cite{mocov3} & \\revise{75.25} & \\revise{75.25} & \\revise{73.80} & \\revise{73.80} & \\revise{57.69} & \\revise{57.69} & \\revise{67.82} & \\revise{67.82} & \\revise{68.64} & \\revise{68.64} \\\\\n& DINOv2\\cite{dinov2} & \\revise{66.50} & \\revise{66.50} & \\revise{61.90} & \\revise{61.90} & \\revise{58.94} & \\revise{58.94} & \\revise{56.84} & \\revise{56.84} & \\revise{61.04} & \\revise{61.04} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.99}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.99}}} & \\cellcolor{gray!20}{\\revise{99.01}} & \\cellcolor{gray!20}{\\revise{99.01}} & \\cellcolor{gray!20}{\\revise{87.23}} & \\cellcolor{gray!20}{\\revise{87.23}} & \\cellcolor{gray!20}{\\revise{95.43}} & \\cellcolor{gray!20}{\\revise{95.43}} & \\cellcolor{gray!20}{\\revise{94.17}} & \\cellcolor{gray!20}{\\revise{94.17}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{94.16}} & \\cellcolor{gray!20}{\\revise{94.16}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.50}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.50}}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.35}}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.35}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.52}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.52}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.88}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.88}}} \\\\\n\\midrule\n\\multirow{5}{*}{React\\cite{react}}\n& ViT\\cite{vit} & \\revise{62.09} & \\revise{62.09} & \\revise{91.20} & \\revise{91.20} & \\revise{63.66} & \\revise{63.66} & \\revise{80.43} & \\revise{80.43} & \\revise{74.34} & \\revise{74.34} \\\\\n& MoCov3\\cite{mocov3} & \\revise{51.47} & \\revise{51.47} & \\revise{79.30} & \\revise{79.30} & \\revise{65.33} & \\revise{65.33} & \\revise{74.35} & \\revise{74.35} & \\revise{67.61} & \\revise{67.61} \\\\\n& DINOv2\\cite{dinov2} & \\revise{76.73} & \\revise{76.73} & \\revise{74.25} & \\revise{74.25} & \\revise{56.26} & \\revise{56.26} & \\revise{63.17} & \\revise{63.17} & \\revise{67.60} & \\revise{67.60} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{86.10}}} & \\cellcolor{gray!20}{\\revise{\\textbf{86.10}}} & \\cellcolor{gray!20}{\\revise{\\textbf{98.09}}} & \\cellcolor{gray!20}{\\revise{\\textbf{98.09}}} & \\cellcolor{gray!20}{\\revise{85.69}} & \\cellcolor{gray!20}{\\revise{85.69}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.96}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.96}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.21}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.21}}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{84.32}} & \\cellcolor{gray!20}{\\revise{84.32}} & \\cellcolor{gray!20}{\\revise{96.99}} & \\cellcolor{gray!20}{\\revise{96.99}} & \\cellcolor{gray!20}{\\revise{\\textbf{87.04}}} & \\cellcolor{gray!20}{\\revise{\\textbf{87.04}}} & \\cellcolor{gray!20}{\\revise{94.21}} & \\cellcolor{gray!20}{\\revise{94.21}} & \\cellcolor{gray!20}{\\revise{90.64}} & \\cellcolor{gray!20}{\\revise{90.64}} \\\\\n\\midrule\n\\multirow{5}{*}{Mahalanobis\\cite{mahalanobis}}\n& ViT\\cite{vit} & \\revise{84.93} & \\revise{84.93} & \\revise{84.90} & \\revise{84.90} & \\revise{71.53} & \\revise{71.53} & \\revise{84.16} & \\revise{84.16} & \\revise{81.38} & \\revise{81.38} \\\\\n& MoCov3\\cite{mocov3} & \\revise{84.29} & \\revise{84.29} & \\revise{86.95} & \\revise{86.95} & \\revise{70.33} & \\revise{70.33} & \\revise{83.54} & \\revise{83.54} & \\revise{81.28} & \\revise{81.28} \\\\\n& DINOv2\\cite{dinov2} & \\revise{68.58} & \\revise{68.58} & \\revise{63.14} & \\revise{63.14} & \\revise{58.86} & \\revise{58.86} & \\revise{57.57} & \\revise{57.57} & \\revise{62.04} & \\revise{62.04} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{93.01}} & \\cellcolor{gray!20}{\\revise{93.01}} & \\cellcolor{gray!20}{\\revise{98.78}} & \\cellcolor{gray!20}{\\revise{98.78}} & \\cellcolor{gray!20}{\\revise{86.78}} & \\cellcolor{gray!20}{\\revise{86.78}} & \\cellcolor{gray!20}{\\revise{95.46}} & \\cellcolor{gray!20}{\\revise{95.46}} & \\cellcolor{gray!20}{\\revise{93.51}} & \\cellcolor{gray!20}{\\revise{93.51}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{\\textbf{93.03}}} & \\cellcolor{gray!20}{\\revise{\\textbf{93.03}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.18}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.18}}} & \\cellcolor{gray!20}{\\revise{\\textbf{88.84}}} & \\cellcolor{gray!20}{\\revise{\\textbf{88.84}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.51}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.51}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.39}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.39}}} \\\\\n\\midrule\n\\multirow{5}{*}{ViM\\cite{vim}}\n& ViT\\cite{vit} & \\revise{83.51} & \\revise{83.51} & \\revise{77.75} & \\revise{77.75} & \\revise{71.04} & \\revise{71.04} & \\revise{78.31} & \\revise{78.31} & \\revise{77.65} & \\revise{77.65} \\\\\n& MoCov3\\cite{mocov3} & \\revise{76.28} & \\revise{76.28} & \\revise{78.18} & \\revise{78.18} & \\revise{61.35} & \\revise{61.35} & \\revise{72.46} & \\revise{72.46} & \\revise{72.07} & \\revise{72.07} \\\\\n& DINOv2\\cite{dinov2} & \\revise{66.90} & \\revise{66.90} & \\revise{62.53} & \\revise{62.53} & \\revise{58.93} & \\revise{58.93} & \\revise{56.93} & \\revise{56.93} & \\revise{61.32} & \\revise{61.32} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.35}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.35}}} & \\cellcolor{gray!20}{\\revise{99.31}} & \\cellcolor{gray!20}{\\revise{99.31}} & \\cellcolor{gray!20}{\\revise{90.06}} & \\cellcolor{gray!20}{\\revise{90.06}} & \\cellcolor{gray!20}{\\revise{96.96}} & \\cellcolor{gray!20}{\\revise{96.96}} & \\cellcolor{gray!20}{\\revise{95.42}} & \\cellcolor{gray!20}{\\revise{95.42}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{94.25}} & \\cellcolor{gray!20}{\\revise{94.25}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.59}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.59}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.47}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.47}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.41}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.41}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.68}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.68}}} \\\\\n\\midrule\n\\multirow{5}{*}{Average}\n& ViT\\cite{vit} & \\revise{73.52} & \\revise{73.52} & \\revise{84.05} & \\revise{84.05} & \\revise{65.66} & \\revise{65.66} & \\revise{80.02} & \\revise{80.02} & \\revise{75.81} & \\revise{75.81} \\\\\n& MoCov3\\cite{mocov3} & \\revise{68.45} & \\revise{68.45} & \\revise{82.79} & \\revise{82.79} & \\revise{64.69} & \\revise{64.69} & \\revise{77.83} & \\revise{77.83} & \\revise{73.44} & \\revise{73.44} \\\\\n& DINOv2\\cite{dinov2} & \\revise{66.91} & \\revise{66.91} & \\revise{66.40} & \\revise{66.40} & \\revise{55.40} & \\revise{55.40} & \\revise{57.64} & \\revise{57.64} & \\revise{61.59} & \\revise{61.59} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.18}}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.18}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.53}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.53}}} & \\cellcolor{gray!20}{\\revise{85.38}} & \\cellcolor{gray!20}{\\revise{85.38}} & \\cellcolor{gray!20}{\\revise{94.42}} & \\cellcolor{gray!20}{\\revise{94.42}} & \\cellcolor{gray!20}{\\revise{91.63}} & \\cellcolor{gray!20}{\\revise{91.63}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{88.24}} & \\cellcolor{gray!20}{\\revise{88.24}} & \\cellcolor{gray!20}{\\revise{97.32}} & \\cellcolor{gray!20}{\\revise{97.32}} & \\cellcolor{gray!20}{\\revise{\\textbf{87.02}}} & \\cellcolor{gray!20}{\\revise{\\textbf{87.02}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.53}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.53}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.77}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.77}}} \\\\\n\\midrule\n\\multirow{5}{*}{Best}\n& ViT\\cite{vit} & \\revise{84.93} & \\revise{84.93} & \\revise{91.20} & \\revise{91.20} & \\revise{71.53} & \\revise{71.53} & \\revise{84.34} & \\revise{84.34} & \\revise{81.38} & \\revise{81.38} \\\\\n& MoCov3\\cite{mocov3} & \\revise{84.29} & \\revise{84.29} & \\revise{90.68} & \\revise{90.68} & \\revise{70.33} & \\revise{70.33} & \\revise{85.42} & \\revise{85.42} & \\revise{81.28} & \\revise{81.28} \\\\\n& DINOv2\\cite{dinov2} & \\revise{80.51} & \\revise{80.51} & \\revise{80.34} & \\revise{80.34} & \\revise{69.77} & \\revise{69.77} & \\revise{63.17} & \\revise{63.17} & \\revise{67.60} & \\revise{67.60} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.35}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.35}}} & \\cellcolor{gray!20}{\\revise{99.31}} & \\cellcolor{gray!20}{\\revise{99.31}} & \\cellcolor{gray!20}{\\revise{90.06}} & \\cellcolor{gray!20}{\\revise{90.06}} & \\cellcolor{gray!20}{\\revise{96.96}} & \\cellcolor{gray!20}{\\revise{96.96}} & \\cellcolor{gray!20}{\\revise{95.42}} & \\cellcolor{gray!20}{\\revise{95.42}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{94.25}} & \\cellcolor{gray!20}{\\revise{94.25}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.59}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.59}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.47}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.47}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.41}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.41}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.68}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.68}}} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\n\\revise{Performance of OOD detection methods on ViT-B/16 model with $224\\times224$-pixel inputs. The pre-text tasks include classification task \\cite{vit}, contrastive learning tasks MoCov3 \\cite{mocov3} and DINOv2 \\cite{dinov2}, and masked image modeling tasks BEiT \\cite{beit} and BEiTv2 \\cite{beitv2}. All models are per-trained on ImageNet-21k and finetuned on ImageNet-1k. Both metrics AUROC and FPR95 are in percentage.\nThe best method is emphasized in bold and a gray background indicates our choice.}\n}\n\\label{tab:multi-class-imagenet-ablation}\n\\end{table*}\n\n\n\n\\revise{In this section, we offer a comprehensive analysis of these key elements in the context of OOD detection. We employ ImageNet \\cite{imagenet} as the in-distribution dataset and evaluate pre-task texts on challenging unnatural out-of-distribution datasets, including OpenImage-O \\cite{openimages_o}, Texture \\cite{dtd}, iNaturalist \\cite{inaturalist}, and ImageNet-O \\cite{imagenet_o}. Extensive validations with various pretraining methods and OOD score functions, \\re{including {MSP} \\cite{baseline_ood}, {Energy} \\cite{energy}, {MaxLogit} \\cite{maxlogit}, {KL Matching} \\cite{maxlogit}, {Residual} \\cite{vim}, {ReAct} \\cite{react}, {Mahalanobis} \\cite{mahalanobis} and {ViM} \\cite{vim}. }}\n\n\\revise{Results are shown in ~\\cref{tab:multi-class-imagenet-ablation}. The results indicate that the masked image modeling pretext task surpasses classification and contrastive learning pretext tasks when employing all included score functions. The average AUROC across these score functions exhibits an improvement of 15.96\\%  compared to the competition. Models when using the best-performing score function saw a 14.30\\% increase in performance. This remarkable achievement can be attributed to the representative ID feature space representation, thereby aiding in distinguishing between ID and OOD data. This discovery is highly significant as it enhances performance across mainstream OOD detection score functions, thus advancing the entire field. We also employ CIFAR-10 \\cite{cifar} as the ID dataset and provide results in the appendix. Our approach attains an impressive AUROC of 99.99\\% while concurrently reducing the FPR95 to a mere 0.03\\%.}\n\n\\revise{To enhance the comprehensibility of our experimental findings, we conduct a thorough statistical analysis and illustrate them in visual representations. The outcomes are depicted in \\cref{fig:detailed_ablation}. Our approach not only leads to an overall enhancement in results but also notably minimizes the variations among different methods. For instance, the ViT, MoCov3, and DINOv2 models using logit-based methods exhibited standard deviations of 4.76\\%, 5.04\\%, and 4.51\\%, respectively, while BEiT and BEiTv2 displayed significantly lower standard deviations, reaching as low as 0.13\\% and 0.01\\%. This observation underscores that even uncomplicated score functions can perform equivalently to more intricate ones when an effective ID feature representation is applied.}\n\n\\revise{In \\cref{tab:ablation-statistic}, we underscore the optimal methods for each model. On CIFAR-10, all models achieved their best results when employing the feat and logit combination approach, achieving almost 100\\% accuracy. This suggests a highly effective grasp of CIFAR-10's feature space. Conversely, with the larger ImageNet dataset, we observed variations in outcomes. Notably, the masked image modeling pretext-pretrained model achieved the best results when using the feat and logit combination method, while other models excelled in probability-based and feature-based methods. Additionally, our masked image modeling pretext demonstrated significantly superior performance compared to other pretraining methods, underscoring the limitations of classification-based pretraining strategies and their inadequacy in harnessing advanced score functions effectively. These discoveries reinforce the pivotal role of proficient feature representation in OOD detection. Furthermore, for more detailed information, we provide illustrations of the distribution curves of OOD scores for both ID and OOD datasets in the appendix.}\n\n\\subsection{Masked Image Modeling for Out-of-Distribution v2} \n\\label{sec:method}\n\\revise{To sum up, in this section, we observed that pre-trained models adeptly reconstruct ID images, yet manifest distinctive domain differences in the OOD scenario (\\cref{fig:recover}). This visual incongruity starkly highlights the prevailing domain gap in model features between ID and OOD data. Additionally, a thorough analysis of experimental outcomes reveals that the pre-task of masked image modeling not only significantly enhances overall results but also markedly diminishes disparities among score functions. These findings emphasize the crucial significance of effective feature representation in OOD detection, highlighting the enhancement of features through masked image modeling tasks.}\n\n\\revise{Finally, we propose our Masked Image Modeling for Out-of-Distribution Detection v2 (MOODv2). The algorithm of is shown in \\cref{alg1}, mainly including the following stages.}\n\n\\begin{enumerate}\n\\item \\revise{Pre-train the vision encoder with masked image modeling on the pretrain dataset. }\n\\item \\revise{Apply fine-tuning the backbone on the in-distribution dataset.}\n\\item \\revise{Extract features from the trained image encoder and calculate the OOD score distance for OOD detection.}\n\\end{enumerate}\n\n\\revise{In terms of the OOD score function, we adopt  ViM\\cite{vim} that combines features and logits, leveraging insights from the masked image modeling pre-trained model, which has demonstrated superior performance. Mathematically, the score is\n\\begin{equation}\n    \\text{s}(x) = \\frac{e^{\\alpha\\sqrt{x^T R R^Tx}}}{\\sum_{i=1}^C e^{l_i} + e^{\\alpha\\sqrt{x^TRR^Tx}}}.\n\\end{equation}\nwhere $l_i$ is the $i$-th logit of feature $x$ in the training set $X$; $\\alpha$ is a per-model constant; $R\\in\\mathbb{R}^{N\\times(N-D)}$ is the $(D+1)$-th column to the last column of the eigenvector matrix $Q$ of $X$ and $N$ is the principal dimension; $C$ is the number of classes.}\n\n\n%--------------------------------------algorithm-----------------------------------\n\\begin{algorithm}[t!]\n\\caption{MOODv2 Detection Algorithm}\n\\label{alg1}\n\\small\n\\begin{algorithmic}[1]\n\n\\Require Pre-train set $X_P$, in-distribution set $X_{\\rm ID}$, test set $X_{\\rm test}$, , required True Positive Rate $\\eta$\\%, backbone $f$.\n\\Ensure Is $x_{\\rm test}$ outlier or not? $\\forall x_{\\rm test} \\in X_{\\rm test}$.\n\n\\State Pre-train $f$ on $X_P$ by maximizing\n$$\\sum_{x\\in X_P}\\mathbb{E}_M\\left [\\sum_{i\\in M}\\log p_{\\rm MIM}(z|x^M)\\right ]$$.\n\\State Fine-tune $f$ on $X_P$ by minimizing\n$$L_{\\rm ft}=\\sum_{x_p\\in X_P}{\\rm CrossEntropy}(f(x_p), y_P(x_p))$$ \n\\State Calculate $d(x_{\\rm test})$ for $x_{\\rm test} \\in X_{\\rm test}$ and \\re{$d(x_{\\rm ID})$ for $x_{\\rm ID}\\in X_{\\rm ID}$.}\n\\State Compute threshold $T$ as the $\\eta$ percentile of \\re{$d(x_{\\rm ID})$.}\n\\If {$d(x_{\\rm test})>T$}\n\\State $x_{\\rm test}$ is an outlier.\n\\EndIf\n\\end{algorithmic}\n\\end{algorithm}\n\n%--------------------------------------multi-class-----------------------------------\n\\begin{table*}[t!]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{0.8mm}\n\\begin{tabular}{c|c|ccccccccccc}\n\\toprule\n\\multirow{2}{*}{ID data} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c}{Texture \\cite{dtd}} & \\multicolumn{2}{c}{iNaturalist \\cite{inaturalist}} & \\multicolumn{2}{c}{ImageNet-O \\cite{imagenet_o}} & \\multicolumn{2}{c}{ OpenImage-O \\cite{openimages_o}} & \\multicolumn{2}{c}{Average} \\\\\n& & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ \\\\\n\\midrule\n\\multirow{10}{*}{CIFAR-10}\n& MSP\\cite{baseline_ood} & \\revise{45.67} & \\revise{95.17} & \\revise{71.07} & \\revise{81.76} & \\revise{32.52} & \\revise{98.85} & \\revise{59.74} & \\revise{91.45} & \\revise{52.25} & \\revise{91.81} \\\\\n& Energy\\cite{energy} & \\revise{31.16} & \\revise{97.89} & \\revise{48.95} & \\revise{97.92} & \\revise{37.22} & \\revise{97.85} & \\revise{45.29} & \\revise{96.36} & \\revise{40.65} & \\revise{97.50} \\\\\n& MaxLogit\\cite{maxlogit} & \\revise{41.21} & \\revise{95.95} & \\revise{67.83} & \\revise{86.04} & \\revise{32.58} & \\revise{98.80} & \\revise{56.64} & \\revise{92.94} & \\revise{49.56} & \\revise{93.43} \\\\\n& KL-Matching\\cite{maxlogit} & \\revise{98.00} & \\revise{10.64} & \\revise{94.23} & \\revise{35.86} & \\revise{92.99} & \\revise{32.40} & \\revise{94.68} & \\revise{27.92} & \\revise{94.97} & \\revise{26.71} \\\\\n& Residual\\cite{vim} & \\revise{99.91} & \\revise{0.21} & \\revise{99.68} & \\revise{0.45} & \\revise{99.36} & \\revise{2.85} & \\revise{99.42} & \\revise{2.46} & \\revise{99.59} & \\revise{1.49} \\\\\n& React\\cite{react} & \\revise{35.97} & \\revise{96.26} & \\revise{69.01} & \\revise{87.91} & \\revise{36.65} & \\revise{97.75} & \\revise{54.14} & \\revise{93.11} & \\revise{48.94} & \\revise{93.76} \\\\\n& Mahalanobis\\cite{mahalanobis} & \\revise{99.77} & \\revise{0.60} & \\revise{99.39} & \\revise{1.11} & \\revise{98.93} & \\revise{4.90} & \\revise{99.14} & \\revise{3.26} & \\revise{99.31} & \\revise{2.47} \\\\\n& ViM\\cite{vim} & \\revise{99.91} & \\revise{0.23} & \\revise{99.72} & \\revise{0.38} & \\revise{99.38} & \\revise{2.65} & \\revise{99.49} & \\revise{2.31} & \\revise{99.63} & \\revise{1.39} \\\\\n& \\cellcolor{gray!20}{MOODv1\\cite{MOOD}} & \\cellcolor{gray!20}{\\revise{99.95}} & \\cellcolor{gray!20}{\\revise{\\textbf{0.06}}} & \\cellcolor{gray!20}{\\revise{99.99}} & \\cellcolor{gray!20}{\\revise{0.02}} & \\cellcolor{gray!20}{\\revise{99.61}} & \\cellcolor{gray!20}{\\revise{1.90}} & \\cellcolor{gray!20}{\\revise{99.82}} & \\cellcolor{gray!20}{\\revise{0.77}} & \\cellcolor{gray!20}{\\revise{99.84}} & \\cellcolor{gray!20}{\\revise{0.69}} \\\\\n& \\cellcolor{gray!20}{MOODv2 (ours)} & \\cellcolor{gray!20}{\\revise{\\textbf{99.98}}} & \\cellcolor{gray!20}{\\revise{\\textbf{0.06}}} & \\cellcolor{gray!20}{\\revise{\\textbf{100.00}}} & \\cellcolor{gray!20}{\\revise{\\textbf{0.00}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.94}}} & \\cellcolor{gray!20}{\\revise{\\textbf{0.20}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.99}}} & \\cellcolor{gray!20}{\\revise{\\textbf{0.01}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.98}}} & \\cellcolor{gray!20}{\\revise{\\textbf{0.07}}} \\\\\n\\midrule\n\\multirow{10}{*}{ImageNet}\n& MSP\\cite{baseline_ood} & \\revise{71.31} & \\revise{77.07} & \\revise{90.70} & \\revise{43.72} & \\revise{60.77} & \\revise{90.60} & \\revise{84.29} & \\revise{61.79} & \\revise{76.77} & \\revise{68.30} \\\\\n& Energy\\cite{energy} & \\revise{54.11} & \\revise{86.28} & \\revise{76.61} & \\revise{72.70} & \\revise{61.63} & \\revise{81.00} & \\revise{71.06} & \\revise{73.99} & \\revise{65.85} & \\revise{78.49} \\\\\n& MaxLogit\\cite{maxlogit} & \\revise{67.22} & \\revise{77.98} & \\revise{89.88} & \\revise{45.57} & \\revise{61.68} & \\revise{88.60} & \\revise{82.73} & \\revise{62.52} & \\revise{75.37} & \\revise{68.67} \\\\\n& KL-Matching\\cite{maxlogit} & \\revise{82.59} & \\revise{67.27} & \\revise{87.63} & \\revise{69.71} & \\revise{66.55} & \\revise{88.15} & \\revise{84.34} & \\revise{74.23} & \\revise{80.28} & \\revise{74.84} \\\\\n& Residual\\cite{vim} & \\revise{82.39} & \\revise{64.61} & \\revise{73.72} & \\revise{86.00} & \\revise{68.44} & \\revise{87.45} & \\revise{74.88} & \\revise{77.98} & \\revise{74.86} & \\revise{79.01} \\\\\n& React\\cite{react} & \\revise{62.09} & \\revise{80.47} & \\revise{91.20} & \\revise{38.74} & \\revise{63.66} & \\revise{81.00} & \\revise{80.43} & \\revise{60.41} & \\revise{74.34} & \\revise{65.15} \\\\\n& Mahalanobis\\cite{mahalanobis} & \\revise{84.93} & \\revise{66.05} & \\revise{84.90} & \\revise{81.60} & \\revise{71.53} & \\revise{88.85} & \\revise{84.16} & \\revise{74.72} & \\revise{81.38} & \\revise{77.80} \\\\\n& ViM\\cite{vim} & \\revise{83.51} & \\revise{62.71} & \\revise{77.75} & \\revise{81.72} & \\revise{71.04} & \\revise{86.60} & \\revise{78.31} & \\revise{74.55} & \\revise{77.65} & \\revise{76.40} \\\\\n& \\cellcolor{gray!20}{MOODv1\\cite{MOOD}} & \\cellcolor{gray!20}{\\revise{93.01}} & \\cellcolor{gray!20}{\\revise{30.91}} & \\cellcolor{gray!20}{\\revise{98.78}} & \\cellcolor{gray!20}{\\revise{5.89}} & \\cellcolor{gray!20}{\\revise{86.78}} & \\cellcolor{gray!20}{\\revise{63.15}} & \\cellcolor{gray!20}{\\revise{95.46}} & \\cellcolor{gray!20}{\\revise{26.46}} & \\cellcolor{gray!20}{\\revise{93.51}} & \\cellcolor{gray!20}{\\revise{31.60}} \\\\\n& \\cellcolor{gray!20}{MOODv2 (ours)} & \\cellcolor{gray!20}{\\revise{\\textbf{94.25}}} & \\cellcolor{gray!20}{\\revise{\\textbf{24.69}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.59}}} & \\cellcolor{gray!20}{\\revise{\\textbf{1.83}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.47}}} & \\cellcolor{gray!20}{\\revise{\\textbf{40.80}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.41}}} & \\cellcolor{gray!20}{\\revise{\\textbf{13.55}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.68}}} & \\cellcolor{gray!20}{\\revise{\\textbf{20.22}}} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\n\\revise{Performance of OOD detection methods on ViT-B/16 model with $224\\times224$-pixel inputs. All methods are pre-trained on ImageNet-21k and finetuned on ImageNet-1k. ID datasets include CIFAR-10 \\cite{cifar} and ImageNet-1k \\cite{imagenet}. Both metrics AUROC and FPR95 are in percentage. The best method is emphasized in bold and a gray background indicates our methods.}\n}\n\\label{tab:multi-class}\n\\end{table*}\n\n\\begin{table*}[t]\n\\small\n\\centering\n\n\\subfloat[AUROC]{\n\\setlength{\\tabcolsep}{2.8mm}\n\\begin{tabular}{c|cccccccccc|c}\n\\toprule\n\\multirow{2}{*}{Methods} & \\multicolumn{10}{c|}{ID class} & \\multirow{2}{*}{Average} \\\\\n% & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & \\\\\n& Plane & Car & Bird & Cat & Deer & Dog & Frog & Horse & Ship & Truck & \\\\\n\\midrule\nKL-Matching\\cite{maxlogit} & \\revise{95.35} & \\revise{92.04} & \\revise{95.18} & \\revise{91.26} & \\revise{88.11} & \\revise{94.66} & \\revise{94.99} & \\revise{86.52} & \\revise{93.61} & \\revise{89.37} & \\revise{92.11} \\\\\nResidual\\cite{vim} & \\revise{97.62} & \\revise{95.88} & \\revise{97.06} & \\revise{96.30} & \\revise{89.18} & \\revise{94.33} & \\revise{96.73} & \\revise{91.46} & \\revise{94.89} & \\revise{92.36} & \\revise{94.58} \\\\\nMahalanobis\\cite{mahalanobis} & \\revise{97.52} & \\revise{96.07} & \\revise{96.77} & \\revise{96.41} & \\revise{89.60} & \\revise{94.79} & \\revise{96.41} & \\revise{91.48} & \\revise{94.80} & \\revise{92.58} & \\revise{94.64} \\\\\nViM\\cite{vim} & \\revise{97.61} & \\revise{96.36} & \\revise{97.19} & \\revise{96.50} & \\revise{88.78} & \\revise{94.21} & \\revise{96.70} & \\revise{91.60} & \\revise{94.97} & \\revise{92.35} & \\revise{94.63} \\\\\n\\rowcolor{gray!20}MOODv1\\cite{MOOD} & \\revise{98.63} & \\revise{\\textbf{99.33}} & \\revise{94.31} & \\revise{93.22} & \\revise{\\textbf{98.11}} & \\revise{96.50} & \\revise{\\textbf{99.25}} & \\revise{\\textbf{98.96}} & \\revise{\\textbf{98.76}} & \\revise{\\textbf{97.82}} & \\revise{97.83} \\\\\n\\rowcolor{gray!20}MOODv2 (ours) & \\revise{\\textbf{99.14}} & \\revise{99.03} & \\revise{\\textbf{99.51}} & \\revise{\\textbf{98.37}} & \\revise{97.12} & \\revise{\\textbf{97.20}} & \\revise{98.53} & \\revise{98.07} & \\revise{98.35} & \\revise{96.68} & \\revise{\\textbf{98.20}} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\subfloat[FPR95]{\n\\setlength{\\tabcolsep}{2.8mm}\n\\begin{tabular}{c|cccccccccc|c}\n\\toprule\n\\multirow{2}{*}{Methods} & \\multicolumn{10}{c|}{ID class} & \\multirow{2}{*}{Average} \\\\\n% & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & \\\\\n& Plane & Car & Bird & Cat & Deer & Dog & Frog & Horse & Ship & Truck & \\\\\n\\midrule\nKL-Matching\\cite{maxlogit} & \\revise{23.60} & \\revise{32.60} & \\revise{22.32} & \\revise{42.92} & \\revise{46.26} & \\revise{24.30} & \\revise{24.97} & \\revise{46.74} & \\revise{25.32} & \\revise{40.53} & \\revise{32.96} \\\\\nResidual\\cite{vim} & \\revise{12.06} & \\revise{25.58} & \\revise{16.71} & \\revise{21.17} & \\revise{48.33} & \\revise{22.12} & \\revise{17.42} & \\revise{36.72} & \\revise{17.30} & \\revise{30.76} & \\revise{24.82} \\\\\nMahalanobis\\cite{mahalanobis} & \\revise{12.59} & \\revise{25.72} & \\revise{18.92} & \\revise{21.48} & \\revise{48.44} & \\revise{20.59} & \\revise{19.20} & \\revise{38.02} & \\revise{17.47} & \\revise{30.93} & \\revise{25.34} \\\\\nViM\\cite{vim} & \\revise{12.43} & \\revise{24.83} & \\revise{15.77} & \\revise{20.13} & \\revise{48.68} & \\revise{21.77} & \\revise{17.63} & \\revise{36.63} & \\revise{17.60} & \\revise{30.78} & \\revise{24.63} \\\\\n\\rowcolor{gray!20}MOODv1\\cite{MOOD} & \\revise{7.59} & \\revise{5.04} & \\revise{2.47} & \\revise{\\textbf{7.49}} & \\revise{15.63} & \\revise{\\textbf{10.96}} & \\revise{11.37} & \\revise{13.09} & \\revise{10.06} & \\revise{19.62} & \\revise{10.33} \\\\\n\\rowcolor{gray!20}MOODv2 (ours) & \\revise{\\textbf{4.82}} & \\revise{\\textbf{4.50}} & \\revise{\\textbf{1.79}} & \\revise{8.80} & \\revise{\\textbf{15.59}} & \\revise{11.00} & \\revise{\\textbf{8.46}} & \\revise{\\textbf{12.43}} & \\revise{\\textbf{8.60}} & \\revise{\\textbf{18.96}} & \\revise{\\textbf{9.49}} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\caption{\n\\revise{Performance of OOD detection methods on ViT-B/16 model with $224\\times224$-pixel inputs. All methods are pre-trained on ImageNet-21k and finetuned on ImageNet-1k. We perform each category of CIFAR-10 \\cite{cifar} as the ID dataset and other classes as OOD datasets. We report the average results across OOD classes of each ID class. Both metrics AUROC and FPR95 are in percentage. The best method is emphasized in bold and a gray background indicates our methods.}\n} % The detailed class-wize performance is in the Appendix.\n\\label{tab:one-class}\n\\end{table*}\n\n\n\n\\section{Experiments}\\label{sec:exp}\n\\label{sec:exp}\n\\revise{In this section, we conduct a thorough comparison of our algorithm with the latest OOD detection methods. We employ the ViT-B/16 model, pre-trained on ImageNet-21K \\re{with corresponding pretext tasks from different methods} and fine-tuned on ImageNet-1K at a resolution of $224\\times224$. }\n\n\\vspace{2mm}\\noindent\\revise{\\textbf{ID/OOD Datasets.} We select CIFAR-10 \\cite{cifar} and ImageNet-1K~\\cite{imagenet} as the ID datasets. Following established procedures~\\cite{vim}, for estimating the principal space of ImageNet, we randomly sample $200,000$ images from the training set. Our experiments include the following OOD datasets:\n\\begin{enumerate}\n    \\item OpenImage-O is a newly collected large-scale OOD dataset~\\cite{openimages_o}.\n    \\item Texture~\\cite{cimpoi14describing} comprises natural textural images, with four overlapping categories (\\emph{bubbly, honeycombed, cobwebbed, spiraled}) removed since they coincide with ImageNet.\n    \\item iNaturalist~\\cite{van2018inaturalist} is a fine-grained species classification dataset, and we use a specific subset from previous works ~\\cite{huang2021mos}.\n    \\item ImageNet-O~\\cite{hendrycks2021natural} contains images that are adversarially filtered to challenge OOD detectors. \n\\end{enumerate} \n}\n\n\\re{Although these OOD datasets are specifically tailored to ensure that they do not belong to any category in ImageNet, rather than being customized for CIFAR-10, each category in CIFAR-10 has a similar counterpart in ImageNet, as referenced in the appendix. Consequently, we use the same OOD datasets for CIFAR-10 as well.}\n\n\n\\vspace{2mm}\\noindent\\revise{\\textbf{Evaluation Metrics.} \nWe report two commonly used evaluation metrics AUROC and FPR95. The AUROC is a threshold-free metric, indicating the area under the receiver operating characteristic curve, with a higher value denoting better detection performance. FPR95, or FPR at TPR95, stands for the false positive rate when the true positive rate is 95\\%, and a smaller FPR95 is preferable. Both metrics are expressed as percentages.}\n\n\\vspace{2mm}\\noindent\\revise{\\textbf{Baseline Methods.} \nFollowing previous works \\cite{vim}, we compare MOODv2 with the baseline algorithms that do not require fine-tuning including {MSP} \\cite{baseline_ood}, {Energy} \\cite{energy}, {MaxLogit} \\cite{maxlogit}, {KL Matching} \\cite{maxlogit}, {Residual}, {ReAct} \\cite{react}, and {Mahalanobis} \\cite{mahalanobis}. }\n\n%--------------------------------------visualization----------------\n\\begin{figure*}[tp]\n  \\centering\n    \\includegraphics[width=\\textwidth]{figures/distribution_method.pdf}\n    \\caption{\\revise{The distribution curves of OOD score functions for ID and OOD datasets obtained using various mainstream methods, including {MSP} \\cite{baseline_ood}, {Energy} \\cite{energy}, {MaxLogit} \\cite{maxlogit}, {KL Matching} \\cite{maxlogit}, {Residual} \\cite{vim}, {ReAct} \\cite{react}, {Mahalanobis} \\cite{mahalanobis} and {ViM} \\cite{vim}. The red line indicates the ID dataset ImageNet \\cite{imagenet}; the blue line indicates Texture \\cite{dtd}; the green line indicates iNaturalist \\cite{inaturalist}; the purple line indicates ImageNet-O \\cite{imagenet_o}; the orange line indicates OpenImage-O \\cite{openimages_o}}}\n  \\label{fig:distribution}\n\\end{figure*}\n\n\\subsection{One-Class OOD Detection}\n\\label{sec:1class}\nWe start with the one-class OOD detection. For a given multi-class dataset of $N_c$ classes, we conduct $N_c$ one-class OOD tasks, where each task regards one of the classes as in-distribution and the remaining classes as out-of-distribution. We run our experiments on CIFAR-10 \\cite{cifar}. \\Cref{tab:one-class} summarizes the average results across OOD classes of each ID class and the detailed \\re{class-wise} performance is in the appendix.\n\n\\revise{It's worth noting that all methods were pre-trained on ImageNet-21k and fine-tuned on ImageNet-1k, which may have had some influence on the results to varying degrees. Nevertheless, we ensure consistent training strategies for all methods to ensure a fair comparison. \\re{Experimental results have demonstrated that MOODv2 achieves significant improvements across all ID classes.} Notably, we achieved a remarkable 3.56\\% increase in the AUROC, reaching 98.20\\%, while simultaneously reducing the FPR95 by 15.14\\% to achieve an impressive 9.49\\%.}\n\n\\subsection{Multi-Class OOD Detection} \n\\label{sec:multi-class}\n\\revise{For multi-class OOD Detection, we assume that ID samples are from a multi-class dataset, either CIFAR-10 \\cite{cifar} or ImageNet \\cite{imagenet}. They are tested on external datasets as out-of-distribution, including OpenImage-O \\cite{openimages_o}, Texture \\cite{cimpoi14describing}, iNaturalist \\cite{van2018inaturalist} and ImageNet-O \\cite{hendrycks2021natural}.}\n\n\\revise{Results are shown in \\cref{tab:multi-class}. MOODv2 delivers outstanding results on CIFAR-10, achieving an impressive AUROC of 99.98\\% (0.35\\% enhancement) and the FPR95 reaches an astonishingly low rate of 0.07\\%, marking a substantial 95\\% reduction compared to the prior SOTA (1.39\\%). On ImageNet, MOODv2 also exhibited significant improvements, showcasing a remarkable 14.30\\% increase in AUROC, resulting in 95.68\\%. Additionally, the FPR95 saw a substantial reduction of 44.93\\%, reaching 20.22\\%.}\n\n\\revise{In \\cref{fig:distribution}, we illustrate the distribution curves of OOD scores for ID and OOD datasets using various mainstream methods. A smaller overlap between ID and OOD data indicates superior OOD detection performance, while a larger overlap signifies weaker detection results. The ID curve (in red) for MOODv2 features a distinct peak at a higher position, resulting in minimal overlap with other OOD data, indicating a notable OOD detection capability. This success can be attributed to the high-quality ID feature representation.}\n\n\\section{Conclusion}\\label{sec:conclusion}\n\\revise{In our work, we focus on the critical aspect of effective out-of-distribution (OOD) detection, which involves acquiring a robust in-distribution (ID) representation that distinguishes it from OOD samples. We conduct comprehensive experiments with distinct pretraining tasks and employ various OOD score functions. The findings indicate that feature representations pre-trained through reconstruction significantly enhance performance and reduce the performance gap among different score functions. This implies that even simple score functions can perform as well as complex ones when utilizing reconstruction-based pretext tasks. These findings hold promise for further development in OOD detection. Ultimately, we introduce the MOODv2 OOD detection framework, employing the masked image modeling pretext task, which achieves a remarkable 14.30\\% increase in AUROC, reaching 95.68\\% on ImageNet, and substantially improving CIFAR-10 to 99.98\\%.}\n. The second paper ends.", "input": "What is the connection between the papers?", "answer": ""}
{"context": "The first paper begins. \\begin{abstract}\n\nThe core of out-of-distribution (OOD) detection is to learn the in-distribution (ID) representation, which is distinguishable from OOD samples. Previous work applied recognition-based methods to learn the ID features, which tend to learn shortcuts instead of comprehensive representations. In this work, we find surprisingly that simply using reconstruction-based methods could boost the performance of OOD detection significantly. We deeply explore the main contributors of OOD detection and find that reconstruction-based pretext tasks have the potential to provide a generally applicable and efficacious prior, which benefits the model in learning intrinsic data distributions of the ID dataset. Specifically, we take Masked Image Modeling as a pretext task for our OOD detection framework (MOOD). Without bells and whistles, MOOD outperforms previous SOTA of one-class OOD detection by 5.7\\%, multi-class OOD detection by 3.0\\%, and near-distribution OOD detection by 2.1\\%. It even defeats the 10-shot-per-class outlier exposure OOD detection, although we do not include any OOD samples for our detection. Codes are available at \\href{}{https://github.com/JulietLJY/MOOD}.\n\n% achieve new state-of-the-art results on one-class and multi-class tasks with the AUROCs reaching 94.9\\% and 97.6\\% respectively, which are 5.7\\% and 3.0\\% higher than the existing works. \n\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\n\nA reliable visual recognition system not only provides correct predictions on known context (also known as in-distribution data) but also detects unknown out-of-distribution (OOD) samples and rejects (or transfers) them to human intervention for safe handling. This motivates applications of outlier detectors before feeding input to the downstream networks, which is the main task of OOD detection, also referred to as novelty or anomaly detection. OOD detection is the task of identifying whether a test sample is drawn far from the in-distribution (ID) data or not. It is at the cornerstone of various safety-critical applications, including medical diagnosis \\cite{caruana2015intelligible}, fraud detection \\cite{phua2010comprehensive}, autonomous driving \\cite{eykholt2018robust}, etc.\n% When deploying, the OOD network is supposed to solve problems with a large amount of data. \n\n\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=0.99\\linewidth]{figures/performance.pdf}\n    \\caption{Performance of MOOD compared with current SOTA (indicated by `*') on four OOD detection tasks: (a) one-class OOD detection; (b) multi-class detection; (c) near-distribution detection; and (d) few-shot outlier exposure OOD detection.}\n  \\label{fig:performance}\n\\end{figure}\n\n\nMany previous OOD detection approaches depend on outlier exposure \\cite{ssd, oodlimits} to improve the performance of OOD detection, which turns OOD detection into a simple binary classification problem. We claim that the core of OOD detection is, instead, to learn the effective ID representation to discover OOD samples without any known outlier exposure. \n\nIn this paper, we first present our surprising finding -- that is, {\\it simply using reconstruction-based methods can {\\it notably} boost the performance on various OOD detection tasks}. Our pioneer work along this line even outperforms previous few-shot outlier exposure OOD detection, albeit we do not include any OOD samples.\n\nExisting methods perform contrastive learning \\cite{csi, ssd} or pretrain classification on a large dataset \\cite{oodlimits} to detect OOD samples. The former methods classify images according to the pseudo labels while the latter classifies images based on ground truth, whose core tasks are both to fulfill the classification target. However, research on backdoor attack \\cite{backdoor_attack, frog_attack} shows that when learning is represented by classifying data, networks tend to take a shortcut to classify images. % by only learning specific patterns between categories. \n\nIn a typical backdoor attack scene  \\cite{frog_attack}, the attacker adds secret triggers on original training images with the visibly correct label. During the course of testing, the victim model classifies images with secret triggers into the wrong category. Research in this area demonstrates that networks only learn specific distinguishable patterns of different categories because it is a shortcut to fulfill the classification requirement. \n\nNonetheless, learning these patterns is ineffective for OOD detection since the network does not understand the intrinsic data distribution of the ID images. Thus, learning representations by classifying ID data for OOD detection may not be satisfying. For example, when the patterns similar to some ID categories appear in OOD samples, the network could easily interpret these OOD samples as the ID data and classify them into the wrong ID categories. \n\nTo remedy this issue, we introduce the reconstruction-based pretext task. Different from contrastive learning in existing OOD detection approaches \\cite{csi, ssd}, our method forces the network to achieve the training purpose of reconstructing the image and thus makes it learn pixel-level data distribution. \n\nSpecifically, we adopt the masked image modeling (MIM) \\cite{bert, beit, mae} as our self-supervised pretext task, which has been demonstrated to have great potential in both natural language processing \\cite{bert} and computer vision \\cite{beit, mae}. In the MIM task, we split images into patches and randomly mask a proportion of image patches before feeding the corrupted input to the vision transformer. Then we use the tokens from discrete VAE \\cite{tokenzier} as labels to supervise the network during training. With its procedure, the network learns information from remaining patches to speculate the masked patches and restore tokens of the original image. The reconstruction process enables the model to learn from the prior based on the intrinsic data distribution of images rather than just learning different patterns among categories in the classification process. \n\nIn our extensive experiments, it is noteworthy that masked image modeling for OOD detection (MOOD) outperforms the current SOTA on all four tasks of one-class OOD detection, multi-class OOD detection, near-distribution OOD detection, and even few-shot outlier exposure OOD detection, as shown in \\cref{fig:performance}. A few statistics are the following.\n\n\\begin{enumerate}\n\\item For one-class OOD detection (\\cref{tab:one-class}), MOOD boosts the AUROC of current SOTA, i.e., CSI \\cite{csi}, by \\textbf{5.7\\%} to \\textbf{94.9\\%}. \n\\item For multi-class OOD detection (\\cref{tab:multi-class}), MOOD outperforms current SOTA of SSD+ \\cite{ssd} by \\textbf{3.0\\%} and reaches \\textbf{97.6\\%}. \n\\item For near-distribution OOD detection (\\cref{tab:structure}), AUROC of MOOD achieves \\textbf{98.3\\%}, which is \\textbf{2.1\\%} higher than the current SOTA of R50+ViT \\cite{oodlimits}.\n\\item For few-shot outlier exposure OOD detection (\\cref{tab:exposure}), MOOD (\\textbf{99.41\\%}) surprisingly defeats current SOTA of R50+ViT \\cite{oodlimits} (with \\textbf{99.29\\%}), which makes use of 10 OOD samples per class. It is notable that we do not even include any OOD samples in MOOD.\n\\end{enumerate}\n\n%====================================relatex==================================\n\\section{Related Work}\n\\subsection{Out-of-distribution Detection}\nA straightforward out-of-distribution (OOD) approach is to estimate the in-distribution (ID) density \\cite{density_1,density_2,density_3,density_4} and reject test samples that deviate from the estimated distribution. Alternative methods base on the image reconstruction \\cite{reconstruct_1, reconstruct_2, reconstruct_3}, learn the decision boundary between in- and out-of-distribution data \\cite{boundary_1, boundary_2, boundary_3}, compute the distance between train and test features \\cite{distance_1, distance_2, distance_3, csi, ssd}, etc..\n\nIn comparison, our work focuses on distance-based methods and yet includes the reconstruction-based methods as a pretext task. The key idea of distance-based approaches is that the OOD samples are supposedly far from the center of the in-distribution (ID) data \\cite{ood_survey} in the feature space. Representative methods include K-nearest Neighbors \\cite{distance_1}, prototype-based methods \\cite{distance_2, distance_3}, etc.. We will explain the difference between our work and previous OOD detection methods later in this paper.\n\n\\subsection{Vision Transformer}\nTransformer has achieved promising performance in computer vision \\cite{beit, mae} and natural language processing \\cite{bert}. Existing OOD detection research \\cite{oodlimits} performs vision transformer (ViT  \\cite{vit}) with classification pre-train on ImageNet-21k \\cite{imagenet}. It mainly explores the impact of different structures on OOD detection tasks while we deeply explore the effect from four dimensions for OOD detection, including various pretext tasks, architectures, fine-tune processes, and OOD detection metrics. \n\nIt is notable that extra OOD samples are utilized in various previous methods \\cite{ssd, oodlimits} to further improve performance. In contrast, we argue that the exposure of OOD samples violates the original intention of OOD detection. In fact, a sufficient pretext task can achieve comparable or even superior results. Therefore, in our work, we focus on exploring an appropriate pretext task for OOD detection without including any OOD samples.\n\n%----------------------------------------pretask---------------------------------------\n\\begin{table*}[t]\n\\centering\n\\small\n\\setlength{\\tabcolsep}{3.5mm}\n\\begin{tabular}{c|cccccccccccccc}\n\\toprule\nIn-Distribution          & \\multicolumn{4}{c|}{CIFAR-10 $\\longrightarrow $}                                    & \\multicolumn{4}{c}{CIFAR-100 $\\longrightarrow $}              \\\\\nOut-of-Distribution         & SVHN          & CIFAR-100     & LSUN           & \\multicolumn{1}{c|}{Avg}           & SVHN          & CIFAR-10      & LSUN          & Avg           \\\\\n\\midrule\n%Cls(SM) & \\textbf{99.8} & 91.5          & 95.4           & \\multicolumn{1}{c|}{95.6}          & 90.8          & 90.9          & 79.2          & 87.0          \\\\\nClassification & 98.3          & 98.6          & 98.6           & \\multicolumn{1}{c|}{98.5}          & 78.0          & 93.5          & 88.6          & 86.7          \\\\\nMoCov3    & 98.6          & 92.4          & 89.8           & \\multicolumn{1}{c|}{93.6}          & 78.8          & 72.8          & 75.8          & 75.8          \\\\\nMIM         & \\textbf{99.8} & \\textbf{99.4} & \\textbf{99.9}  & \\multicolumn{1}{c|}{\\textbf{99.7}} & \\textbf{96.5} & \\textbf{98.3} & \\textbf{96.3} & \\textbf{97.0} \\\\\n\\midrule\nIn-Distribution          & \\multicolumn{8}{c}{ImageNet-30 $\\longrightarrow $}                                                                                                  \\\\\nOut-of-Distribution         & Dogs          & Places365     & Flowers102     & Pets                               & Food          & Dtd           & Caltech256    & Avg           \\\\\n\\midrule\n%Cls(SM) & 83.5          & 83.8          & 99.3           & 77.7                               & 69.6          & 91.8          & 89.4          & 85.0          \\\\\nClassification & \\textbf{99.7} & 98.4          & 99.9           & \\textbf{99.6}                      & \\textbf{98.3} & 98.6          & 96.8          & 98.8          \\\\\nMoCov3      & 88.2          & 82.0          & 99.3           & 81.1                               & 71.4          & 91.3          & 88.5          & 86.0          \\\\\nMIM         & 99.4          & \\textbf{98.9} & \\textbf{100.0} & 99.1                               & 96.6          & \\textbf{99.5} & \\textbf{98.9} & \\textbf{98.9} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Pretext Task}. AUROC (\\%) of OOD detection on ViT with different pretext tasks on ImageNet22k. }\n\\label{tab:pretask}\n\\end{table*}\n\n\n\\subsection{Self-Supervised Pretext Task}\nIt has been long in the community to pre-train vision networks in various self-supervised manners, including generative learning \\cite{pixelcnn, gpt, bert, beit}, contrastive learning \\cite{moco, supcon, simclr, simsiam} and adversarial learning \\cite{colorization, gan, adversial_ae}.\nAmong them, representative generative approaches include auto-regressive \\cite{pixelcnn, gpt}, flow-based \\cite{nice,glow}, auto-encoding \\cite{bert, beit}, and hybrid generative methods \\cite{graphaf, xlnet}. \n\nThe self-supervised pretext task in our framework is Masked Image Modeling (MIM). It generally belongs to auto-encoding generative approaches. MIM was first proposed in natural language processing \\cite{beit}. Its language modeling task randomly masks varying percentages of tokens of text and recovers the masked tokens from encoding results of the rest of text. Follow-up research \\cite{bert, mae} transfers the similar idea from natural language processing to computer vision, masking different proportions of the image patches to recover results.\n\nMultiple existing methods take advantage of self-supervised tasks to guide learning of representation for OOD detection. The latest work \\cite{csi, ssd} presents contrastive learning models as feature extractors. However, existing approaches of classifying transformed images according to contrastive learning possess similar limitations -- that is, the model tends to learn the specific patterns of categories, which are beneficial for classification but do not help understand intrinsic data  distributions of ID images. \n\nResearch of \\cite{oodlimits} also mentioned this problem. However, the introduced large-scale pre-trained transformers \\cite{oodlimits} may not jump out of the loop, in our observation, because the pretext task remained to be classification. In our work, we address this issue by performing the masked image modeling task for OOD detection.\n\n% By contrast, our framework performs the masked image modeling task to force the network to reconstruct the image and thus learn the pixel-level data distribution. Benefiting from the prior based on such data distribution, our OOD detection network can learn more distinguishable representations, which enlarge the divergence between in- and out-of-distribution data. The performance of MOOD compared with methods based on contrastive learning is vividly shown in the distances from ID and OOD testing samples to the training data in \\cref{fig:distribution}.\n\n\n\n%=================================Method========================================\n\\section{Method}\nIn this section, we first explain the main factors to help OOD detection and finally propose our framework to achieve this goal. \n\nWe first define the notations. For a given dataset $X_{\\rm ID}$, the goal of out-of-distribution (OOD) detection is to model a detector that identifies whether an input image $x \\in X_{\\rm ID}$ or $x \\notin X_{\\rm ID}$ (that is, $x \\in X_{\\rm OOD}$). A majority of existing methods for OOD detection define an OOD score function $s(x)$. Its abnormal high or low value represents that $x$ is from out-of-distribution. \n\n\\subsection{Choosing the Pretext Task} \n\\label{sec:pretask}\n\nIn this section, we choose the pretext task that can provide the intrinsic prior to suit the OOD detection task. Most previous OOD methods learn the ID representation through classification \\cite{baseline_ood, oodlimits} or contrastive learning \\cite{csi, ssd} on ID samples, which take advantage of either the ground truth or pseudo labels to supervise the classification networks. \n\nOn the other hand, work of \\cite{backdoor_attack, frog_attack} shows that classification networks only learn different patterns among training categories because it is a shortcut to fulfill classification. It is indicated that the network actually does not understand the intrinsic data distribution of the ID images.\n\nIn comparison, the reconstruction-based pretext task forces the network to learn the real data distribution of the ID images during training to reconstruct the image instead of the patterns for classification. Benefiting from these priors, the network can learn a more representative feature of the ID dataset. It enlarges the divergence between the OOD and ID samples. \n\nIn our method, we pre-train the model with Masked Image Modeling (MIM) pretext \\cite{bert} on a large dataset and fine-tune it on the ID dataset. We compare the performance of MIM and contrastive learning pretext task MoCov3 \\cite{mocov3} in ~\\cref{tab:pretask}. It shows that the performance of MIM is much increased by 13.3\\% to 98.66\\%. \n\n\\subsection{Exploring Architecture} \n\\label{sec:arch}\n%----------------------------------------structure------------------------------------------\n\\begin{table}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{3.5mm}\n\\begin{tabular}{c|ccccccccccc}\n\\toprule\n\\multirow{2}{*}{Model} & Fine-tuned          & \\multirow{2}{*}{AUROC(\\%)} \\\\\n                       & Test Acc(\\%)        &                            \\\\\n\\midrule\nBiT R50 \\cite{oodlimits} & 87.01               & 81.71                      \\\\\nBiT R101$\\times$3 \\cite{oodlimits}& 91.55               & 90.10                      \\\\\nViT \\cite{oodlimits} & 90.95               & 95.53                      \\\\\nMLP-Mixer \\cite{oodlimits} & 90.40               & 95.31                      \\\\\nR50 + ViT (SOTA) \\cite{oodlimits} & \\textbf{91.71}               & \\textbf{96.23}  \\\\\n% \\midrule\n% MOOD (ours)          & \\textbf{95.73}      & \\textbf{98.30}             \\\\\n%(Difference)              & {\\color{teal}+0.16} & {\\color{teal}+0.70}        \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Architecture}. AUROC (\\%) of OOD detection with various architectures. The last line shows our improvement. The ID and OOD datasets are CIFAR-100 and CIFAR-10, respectively. }\n\\label{tab:structure}\n\\end{table}\n\nTo explore an effective architecture \\cite{oodlimits}, we evaluate OOD detection performance on BiT (Big Transfer \\cite{bit}) and MLP-Mixer, in comparison with ViT. We adopt CIFAR-100 and CIFAR-10 \\cite{cifar} as the ID-OOD pair. They have close distributions because of their similar semantics and construction. Results are in \\cref{tab:structure}.\n\nR50 + ViT \\cite{vit, resnet} is the current SOTA on near-distribution OOD detection \\cite{oodlimits}, which doubles the model size and testing time but achieves only 96.23\\% (0.70\\% higher than ViT). However, MIM on a single ViT significantly improves its AUROC to 98.30\\% (2.07\\% higher), without any additional source assumption. It manifests that efficient pretext itself is sufficient for producing distinguishable representation -- {\\it there is no need to use a larger model or combination of multiple models} in this regard.\n\n\n\n\n\\subsection{About Fine-Tuning} \n\\label{sec:fine-tune}\n\n\\begin{table}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{2.1mm}\n\\begin{tabular}{c|ccc|ccccccc}\n\\toprule\nOne-Class                    & \\multicolumn{3}{c|}{fine-tune}         & \\multirow{2}{*}{AUROC(\\%)} \\\\\nDataset                      & MIM-pt     & inter-ft   & fine-tune         & \\\\\n\\midrule\n\\multirow{2}{*}{CIFAR-10}    & \\checkmark &            &            & 72.2 \\\\\n                             & \\checkmark & \\checkmark &            & \\textbf{97.9} \\\\\n\\midrule\n\\multirow{2}{*}{CIFAR-100}   & \\checkmark &            &            & 66.3  \\\\\n                             & \\checkmark & \\checkmark &            & \\textbf{96.5} \\\\\n\\midrule\n\\multirow{2}{*}{ImageNet-30} & \\checkmark &            &            & 75.2   \\\\\n                             & \\checkmark &            & \\checkmark & \\textbf{92.0} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Fine-tuning} (One-class). AUROC (\\%) of OOD detection with different fine-tuning processes on one-class CIFAR-10, CIFAR-100 (super-classes) and ImageNet-30.}\n\\label{tab:1class-fine-tune}\n\\end{table}\n\n%----------------------------------------metric--------------------------------------\n\\begin{table*}[t]\n\\centering\n\\small\n\\setlength{\\tabcolsep}{3.2mm}\n\\begin{tabular}{ccc|ccccccccccccc}\n\\toprule\n\\multicolumn{3}{c}{finetune}                   & \\multicolumn{4}{|c}{CIFAR-10 $\\longrightarrow $}                             & \\multicolumn{4}{|c}{CIFAR-100 $\\longrightarrow $}                                   \\\\\nMIM-pt               & inter ft   & ft         & SVHN           & CIFAR-100     & LSUN           & Avg                       & \\multicolumn{1}{|c}{SVHN}          & CIFAR-10             & LSUN                 & Avg                  \\\\\n\\midrule\n\\checkmark           &            &            & 62.2           & 62.9          & 98.5           & \\multicolumn{1}{c|}{74.5} & 48.4          & 42.2                 & 96.0                 & 62.2                 \\\\\n\\checkmark           & \\checkmark &            & 89.5           & 90.0          & 99.8           & \\multicolumn{1}{c|}{93.1} & 74.3          & 62.0                 & \\textbf{98.3}        & 68.2                 \\\\\n\\checkmark           &            & \\checkmark & 99.1           & 94.6          & 97.4           & \\multicolumn{1}{c|}{97.0}   & 93.7          & 83.7                 & 91.4                 & 89.6                 \\\\\n\\checkmark           & \\checkmark & \\checkmark & \\textbf{99.8}  & \\textbf{99.4} & \\textbf{99.9}  & \\multicolumn{1}{c|}{\\textbf{99.7}} & \\textbf{96.5} & \\textbf{98.3}        & 96.3                 & \\textbf{97.0}        \\\\\n\\midrule\n\\multicolumn{3}{c}{finetune}                   & \\multicolumn{8}{|c}{ImageNet30 $\\longrightarrow $}                                                                                                                \\\\\nMIM-pt               & inter-ft   & ft         & Dogs           & Places365     & Flowers102     & Pets                      & Food          & Caltech256           & Dtd                  & Avg                  \\\\\n\\midrule\n\\checkmark           &            &            & 60.2           & 82.7          & 28.6           & 41.9                      & 72.5          & 42.2                 & 29.4                 & 51.1                 \\\\\n\\checkmark           & \\checkmark &            & \\textbf{100.0} & 97.9          & 99.9           & \\textbf{99.6}             & \\textbf{97.1} & 96.9                 & 98.2                 & 98.2                 \\\\\n\\checkmark           &            & \\checkmark & 91.3           & 97.0          & 95.1           & 93.8                      & 99.3          & 84.0                 & 95.4                 & 92.9                 \\\\\n\\checkmark           & \\checkmark & \\checkmark & 99.4           & \\textbf{98.9} & \\textbf{100.0} & 99.1                      & 96.6          & \\textbf{99.5}        & \\textbf{98.9}        & \\textbf{98.9}       \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Fine-tuning} (Multi-class). AUROC (\\%) of OOD detection with different fine-tuning processes on multi-class CIFAR-10, CIFAR-100 and ImageNet-30. }\n\\label{tab:metric}\n\\label{tab:fine-tune}\n\\end{table*}\n\n%----------------------------------------metric--------------------------------------\n\\begin{table*}[t]\n\\centering\n\\small\n\\setlength{\\tabcolsep}{3.5mm}\n\\begin{tabular}{c|ccccccccccccccc}\n\\toprule\nIn-Distribution                           & \\multicolumn{4}{c|}{CIFAR-10 $\\longrightarrow $}                                                        & \\multicolumn{4}{c}{CIFAR-100 $\\longrightarrow $}                                          \\\\\nOut-of-Distribution                          & SVHN                 & CIFAR-100            & LSUN                 & \\multicolumn{1}{c|}{Avg}           & SVHN                 & CIFAR-10             & LSUN                 & Avg                  \\\\\n\\midrule\nSoftmax  \\cite{baseline_ood} & 88.6                 & 85.8                 & 90.7                 & \\multicolumn{1}{c|}{88.4}          & 81.9                 & 81.1                 & 86.6                 & 83.2                 \\\\\nEntropy  \\cite{baseline_ood} & \\textbf{99.9}        & 97.1                 & 98.1                 & \\multicolumn{1}{c|}{98.4}          & 93.7                 & 94.1                 & 88.7                 & 92.2                 \\\\\nEnergy   \\cite{energy}       & \\textbf{99.9}        & 97.0                 & 97.6                 & \\multicolumn{1}{c|}{98.2}          & 92.8                 & 93.5                 & 86.1                 & 90.8                 \\\\\nGradNorm  \\cite{gradnorm}    & 99.6                 & 94.3                 & 87.8                 & \\multicolumn{1}{c|}{93.9}          & 61.6                 & 87.7                 & 38.4                 & 62.6                 \\\\\nDistance  \\cite{mahalanobis} & 99.8                 & \\textbf{99.4}        & \\textbf{99.9}        & \\multicolumn{1}{c|}{\\textbf{99.7}} & \\textbf{96.5}        & \\textbf{98.3}        & \\textbf{96.3}        & \\textbf{97.0}        \\\\\n\\midrule\nIn-Distribution                           & \\multicolumn{8}{c}{ImageNet-30 $\\longrightarrow $}                                                                                                                                                  \\\\\nOut-of-Distribution                          & Dogs                 & Places365            & Flowers102           & Pets                               & Food                 & Dtd                  & Caltech256           & Avg                  \\\\\n\\midrule\nSoftmax  \\cite{baseline_ood} & 96.7                 & 90.5                 & 89.7                 & 95.0                               & 79.8                 & 90.6                 & 90.1                 & 90.3                 \\\\\nEntropy  \\cite{baseline_ood} & 92.5                 & 87.2                 & 97.5                 & 90.6                               & 69.6                 & 94.9                 & 85.7                 & 88.3                 \\\\\nEnergy   \\cite{energy}       & 89.7                 & 82.1                 & 95.8                 & 88.1                               & 67.8                 & 93.1                 & 82.3                 & 85.6                 \\\\\nGradNorm  \\cite{gradnorm}    & 74.8                 & 78.7                 & 92.0                 & 70.6                               & 61.5                 & 90.3                 & 74.3                 & 77.5                 \\\\\nDistance  \\cite{mahalanobis} & \\textbf{99.4}        & \\textbf{98.9}        & \\textbf{100.0}       & \\textbf{99.1}                      & \\textbf{96.6}        & \\textbf{99.5}        & \\textbf{98.9}        & \\textbf{98.9}       \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Metric}. AUROC (\\%) of OOD detection with different metrics on multi-class CIFAR-10, CIFAR-100 and ImageNet-30. }\n\\label{tab:metric}\n\\label{tab:fine-tune}\n\\end{table*}\n\n\n\\noindent\\textbf{One-class Fine-tuning.} For one-class OOD detection, we pre-train the MIM model and finely tune it on ImageNet-21k \\cite{imagenet}, as recommended by BEiT \\cite{beit}. In particular, when performing one-class OOD detection on ImageNet-30, since we do not include the OOD labels during training, we only pre-train it on ImageNet-21k without intermediate fine-tuning. Therefore, we utilize the label smoothing \\cite{ls} to help the model learn from the one-class fine-tune task on the ID dataset as\n\\begin{equation}\\label{equ:ls}\n    y_c^{LS} = y_c(1-\\alpha)+\\alpha/N_c, \\quad\\quad c=1, 2, \\dots, N_c\n\\end{equation}\nwhere $c$ is the index of category; $N_c$ is the number of classes; and $\\alpha$ is the hyperparameter that determines smoothing level. If $\\alpha=0$, we obtain the original one-hot encoded $y_c$ and if $\\alpha=1$, we get the uniform distribution.\n\nLabel smoothing was used to address overfitting and overconfidence in normal fine-tuning process. We, instead, find that it can be utilized in one-class fine-tuning. The performance of the model before and after one-class fine-tune is illustrated in \\cref{tab:1class-fine-tune}. It is clear that the model actually learns information from the one-class fine-tuning operation. This may be counter-intuitive because the labels are equal. The reason is, due to label smoothing, the loss is larger than 0 and persuades the model to update parameters, although the accuracy reaches 1.\n\n\\vspace{2mm}\\noindent\\textbf{Multi-class Fine-tuning.}\nFor multi-class OOD detection, we pre-train the MIM model, intermediately use fine-tuning on ImageNet-21k \\cite{imagenet}, and apply fine-tuning again on the ID dataset. We perform experiments to validate the effectiveness of each stage in \\cref{tab:fine-tune}. It proves that all stages contribute well to the performance of OOD detection.\n\n%------------------------------------OOD------------------------------------\n\n\\subsection{OOD Detection Metric is Important}\n\\label{sec:metric}\n\nHere, we compare the performance of several commonly-used OOD detection metrics, including Softmax \\cite{baseline_ood}, Entropy \\cite{baseline_ood}, Energy \\cite{energy}, GradNorm  \\cite{gradnorm} and Mahalanobis distance \\cite{mahalanobis}. We perform OOD detection with MIM pretext task with each metric -- the results are shown in \\cref{tab:metric}. They prove that the Mahalanobis distance is a better metric for MOOD.\n\n\\subsection{Final Algorithm of MOOD} \n\\label{sec:alg}\n\nTo sum up, in this section, we have explored the effect of contributors to OOD detection, including various pretext tasks, architectures, fine-tuning processes, and OOD detection metrics. In general, we find that the finely tuned MOOD on ViT with Mahalanobis distances achieves the best result. The outstanding performance of MOOD demonstrates that an efficient pretext task itself is sufficient for producing distinguishable representation, and there is no need for a larger model or multi-models. \n\nIn \\cref{sec:exp}, we will show that few-shot outlier exposure utilized in multiple existing OOD detection approaches \\cite{ssd, oodlimits} is also unnecessary. The algorithm of MOOD is shown in the Appendix. It mainly includes the following stages.\n\n\\begin{enumerate}\n% \\renewcommand{\\labelenumi}{Step \\theenumi.}\n\\item Pre-train the Masked Image Modeling ViT on ImageNet-21k.\n\\item Apply intermediate fine-tuning ViT on ImageNet-21k. %(Not for one-class detection on ImageNet-30). \n\\item Apply fine-tuning of pre-trained ViT on the ID dataset. % (Not for one-class detection on CIFAR-10 and CIFAR-100). \n\\item Extract features from the trained ViT and calculate the Mahalanobis distance metric for OOD detection.\n\\end{enumerate}\n\n%--------------------------------------one-class-----------------------------------\n\\begin{table*}[t]\n\\small\n\\centering\n\\begin{subtable}{0.99\\linewidth}\n\\centering\n\\setlength{\\tabcolsep}{1.5mm}\n\\begin{tabular}{c|ccccccccccccccc}\n\\toprule\n%Method & Network   & Airplane & Automobile & Bird & Cat  & Dear & Dog  & Frog & Horse & Ship & Truck & Average \\\\\nMethod   & Plane & Car   & Bird & Cat & Dear& Dog & Frog& Horse & Ship& Truck & Average  \\\\\n\\midrule\nOC-SVM\\cite{goad} & 65.6 & 40.9 & 65.3 & 50.1 & 75.2 & 51.2 & 71.8 & 51.2 & 67.9 & 48.5 & 58.8   \\\\\nDeepSVDD\\cite{deepsvdd}  & 61.7 & 65.9 & 50.8 & 59.1 & 60.9 & 65.7 & 67.7 & 67.3 & 75.9 & 73.1 & 64.8   \\\\\nAnoGAN\\cite{anogan} & 67.1 & 54.7 & 52.9 & 54.5 & 65.1 & 60.3 & 58.5 & 62.5 & 75.8 & 66.5 & 61.8   \\\\\nOCGANOCGAN\\cite{ocgan} & 75.7 & 53.1 & 64.0 & 62.0 & 72.3 & 62.0 & 72.3 & 57.5 & 82.0 & 55.4 & 65.7   \\\\\nGeom\\cite{geom}  & 74.7 & 95.7 & 78.1 & 72.4 & 87.8 & 87.8 & 83.4 & 95.5 & 93.3 & 91.3 & 86.0   \\\\\nRot\\cite{rot}   & 71.9 & 94.5 & 78.4 & 70.0 & 77.2 & 86.6 & 81.6 & 93.7 & 90.7 & 88.8 & 83.3   \\\\\nRot+Trans\\cite{rot} & 77.5 & 96.9 & 87.3 & 80.9 & 92.7 & 90.2 & 90.9 & 96.5 & 95.2 & 93.3 & 90.1   \\\\\nGOAD\\cite{goad}  & 77.2 & 96.7 & 83.3 & 77.7 & 87.8 & 87.8 & 90.0 & 96.1 & 93.8 & 92.0 & 88.2   \\\\\nCSI (SOTA)\\cite{csi}   & 89.9 & 99.1 & 93.1 & 86.4 & 93.9 & 93.2 & 95.1 & 98.7 & 97.9 & 95.5 & 94.3   \\\\\nours  & \\textbf{98.6}\\tiny{$\\pm$0.4} & \\textbf{99.3}\\tiny{$\\pm$0.5} & \\textbf{94.3}\\tiny{$\\pm$0.6} & \\textbf{93.2}\\tiny{$\\pm$0.5} & \\textbf{98.1}\\tiny{$\\pm$0.6} & \\textbf{96.5}\\tiny{$\\pm$0.4} & \\textbf{99.3}\\tiny{$\\pm$0.2} & \\textbf{99.0}\\tiny{$\\pm$0.1} & \\textbf{98.8}\\tiny{$\\pm$0.1} & \\textbf{97.8}\\tiny{$\\pm$0.4} & \\textbf{97.8}\\tiny{$\\pm$0.4} \\\\\n(improve)   & {\\color{teal}+8.7}& {\\color{teal}+0.2}& {\\color{teal}+1.2}& {\\color{teal}+6.8}& {\\color{teal}+4.2}& {\\color{teal}+3.3}& {\\color{teal}+4.2}& {\\color{teal}+0.3}& {\\color{teal}+0.9}& {\\color{teal}+2.3}& {\\color{teal}+3.5} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{CIFAR-10}\n\\label{tab:one-class-cifar10}\n\\end{subtable}\n\\\\\n\\begin{subtable}{0.48\\linewidth}\n\\centering\n\\setlength{\\tabcolsep}{12mm}\n\\begin{tabular}{c|ccccccccccc}\n\\toprule\nMethod & AUROC   \\\\\n\\midrule\nOC-SVM\\cite{goad} & 63.1  \\\\\nGeom\\cite{geom}  & 78.7  \\\\\nRot\\cite{rot}   & 77.7  \\\\\nRot+Trans\\cite{rot} & 79.8  \\\\\nGOAD\\cite{goad}  & 74.5  \\\\\nCSI (SOTA)\\cite{csi}    & 89.6  \\\\\nours  & \\textbf{94.8}   \\\\\n(improve)   & {\\color{teal}+5.2}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{CIFAR-100}\n\\label{tab:one-class-cifar100}\n\\end{subtable}\n\\hspace{2.2mm}\n\\begin{subtable}{0.48\\linewidth}\n\\centering\n\\setlength{\\tabcolsep}{8mm}\n\\begin{tabular}{c|ccccccccccc}\n\\toprule\nMethod   & AUROC   \\\\\n\\midrule\nRot\\cite{rot}  & 65.3  \\\\\nRot+Trans\\cite{rot} & 77.9  \\\\\nRot+Attn\\cite{rot} & 81.6  \\\\\nRot+Trans+Attn\\cite{rot} & 84.8  \\\\\nRot+Trans+Attn+Resize\\cite{rot} & 85.7  \\\\\nCSI (SOTA) \\cite{csi}  & 91.6  \\\\\nours & \\textbf{92.0}   \\\\\n(improve)  & {\\color{teal}+0.4}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{ImageNet-30}\n\\label{tab:one-class-imagenet30}\n\\end{subtable}\n\\caption{\\textbf{One-class OOD detection.} AUROC (\\%) of OOD methods on one-class (a) CIFAR-10, (b) CIFAR-100 (super-classes) and (c) ImageNet-30. The reported results on CIFAR-10 are averaged over 3 trials. Subscripts denote standard deviation, and bold ones denote the best results. The last line lists improvement of MOOD over the current SOTA.}\n\\label{tab:one-class}\n\\end{table*}\n\n%--------------------------------------multi-class-----------------------------------\n\\begin{table*}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{3.2mm}\n\\begin{subtable}{0.99\\linewidth}\n\\centering\n\\begin{tabular}{c|cccccccccc}\n\\toprule\nIn-Distribution & \\multicolumn{4}{c|}{CIFAR-10 $\\longrightarrow $} &\\multicolumn{4}{c}{CIFAR-100 $\\longrightarrow $} \\\\\nOut-of-Distribution & SVHN  & CIFAR-100  & LSUN & \\multicolumn{1}{c|}{Average} & SVHN  & CIFAR-10   & LSUN & Average \\\\\n\\midrule\nBaseline OOD\\cite{baseline_ood} & 88.6  & 85.8 &90.7 &\\multicolumn{1}{c|}{88.4} & 81.9  & 81.1 &86.6 &83.2   \\\\\nODIN\\cite{odin}   & 96.4  & 89.6 &-   &\\multicolumn{1}{c|}{93.0} & 60.9  & 77.9 &-   &69.4   \\\\\nMahalanobis\\cite{mahalanobis}  & 99.4  & 90.5 &-   &\\multicolumn{1}{c|}{95.0} & 94.5  & 55.3 &-   &74.9   \\\\\nResidual Flows\\cite{residual_flows}   & 99.1  & 89.4 &-   &\\multicolumn{1}{c|}{94.3} & 97.5  & 77.1 &-   &87.3   \\\\\nGram Matrix\\cite{gram_matrix}  & 99.5  & 79.0 &-   &\\multicolumn{1}{c|}{89.3} & 96.0  & 67.9 &-   &82.0   \\\\\nOutlier exposure\\cite{outlier_exposure} & 98.4  & 93.3 &-   &\\multicolumn{1}{c|}{95.9} & 86.9  & 75.7 &-   &81.3   \\\\\nRotation loss\\cite{rot} & 98.9  & 90.9 &\u2013   &\\multicolumn{1}{c|}{94.9} & - & -   &-   &-   \\\\\nContrastive loss\\cite{supcon} & 97.3  & 88.6 &92.8 &\\multicolumn{1}{c|}{92.9} & 95.6  & 78.3 &-   &87.0   \\\\\nCSI\\cite{csi} & 97.9  & 92.2 &97.7 &\\multicolumn{1}{c|}{95.9} & - & -   &-   &-   \\\\\nSSD+ (SOTA) \\cite{ssd}   & \\textbf{99.9}   & 93.4 &98.4 &\\multicolumn{1}{c|}{97.2} & \\textbf{98.2}   & 78.3 &79.8 &85.4   \\\\\nours   & 99.8\\tiny{$\\pm$0.0} & \\textbf{99.4}\\tiny{$\\pm$0.0} & \\textbf{99.9}\\tiny{$\\pm$0.0} & \\multicolumn{1}{c|}{\\textbf{99.7}} & 96.5\\tiny{$\\pm$0.6} & \\textbf{98.3}\\tiny{$\\pm$0.1} & \\textbf{96.3}\\tiny{$\\pm$0.6} & \\textbf{97.0}   \\\\\n(improve) & {\\color{gray}-0.1}   & {\\color{teal}+6.0}  & {\\color{teal}+1.5}  & \\multicolumn{1}{c|}{{\\color{teal}+2.5}} & {\\color{gray}-1.7}   & {\\color{teal}+20.0}   & {\\color{teal}+16.5}   & {\\color{teal}+11.6}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{CIFAR}\n\\end{subtable}\n\n\\begin{subtable}{0.99\\linewidth}\n\\centering\n\\setlength{\\tabcolsep}{3.8mm}\n\\begin{tabular}{c|ccccccccccccccc}\n\\toprule\nIn-Distribution & \\multicolumn{8}{c}{ImageNet-30 $\\longrightarrow $}   \\\\\nOut-of-Distribution & Dogs  & Places365 & Flowers102 & Pets  & Food  & Caltech256 & DTD   & Average  \\\\\n\\midrule\nBaseline OOD\\cite{baseline_ood} & 96.7  & 90.5  & 89.7  & 95.0  & 79.8  & 90.6  & 90.1  & 90.3  \\\\\nContrastive loss\\cite{supcon} & 95.6  & 89.7  & 92.2  & 94.2  & 81.2  & 90.2  & 92.1  & 90.7  \\\\\nCSI (SOTA)\\cite{csi}   & 98.3  & 94.0  & 96.2  & 97.4  & 87.0  & 93.2  & 97.4  & 94.8  \\\\\nours  & \\textbf{99.4} & \\textbf{98.9} & \\textbf{100.0} & \\textbf{99.1} & \\textbf{96.6} & \\textbf{99.5} & \\textbf{98.9} & \\textbf{98.9}  \\\\\n(improve)   & {\\color{teal}+0.9} & {\\color{teal}+4.9} & {\\color{teal}+3.8} & {\\color{teal}+1.7} & {\\color{teal}+9.6} & {\\color{teal}+6.3} & {\\color{teal}+1.5} & {\\color{teal}+4.1}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{ImageNet-30}\n\\end{subtable}\n\n\\begin{subtable}{0.99\\linewidth}\n\\centering\n\\setlength{\\tabcolsep}{7.7mm}\n\\begin{tabular}{c|ccccccccccccccc}\n\\toprule\nIn-Distribution                              & \\multicolumn{5}{c}{ImageNet-1k $\\longrightarrow $}                                                       \\\\\nOut-of-Distribution                             & iNaturalist        & SUN                & Places             & Textures            & Average            \\\\\n\\midrule\nBaseline OOD \\cite{baseline_ood}         & 87.6               & 78.3               & 76.8               & 74.5                & 79.3               \\\\\nODIN \\cite{odin}                & 89.4               & 83.9               & 80.7               & 76.3                & 82.6               \\\\\nEnergy \\cite{energy}            & 88.5               & 85.3               & 81.4               & 75.8                & 82.7               \\\\\nMahalanobis \\cite{mahalanobis}  & 46.3               & 65.2               & 64.5               & 72.1                & 62.0               \\\\\nGradNorm (SOTA) \\cite{gradnorm} & \\textbf{90.3}      & 89.0               & 84.8               & 81.1                & 86.3               \\\\\nours                     & 86.9               & \\textbf{89.8}      & \\textbf{88.5}      & \\textbf{91.3}       & \\textbf{89.1}      \\\\\n(improve)                       & {\\color{gray}-3.4} & {\\color{teal}+0.8} & {\\color{teal}+3.7} & {\\color{teal}+10.2} & {\\color{teal}+2.8}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{ImageNet-1k}\n\\end{subtable}\n\\caption{\\textbf{Multi-class OOD detection.} AUROC (\\%) of OOD detection methods on multi-class CIFAR-10, CIFAR-100, ImageNet-30 and ImageNet-1k. The reported results on CIFAR-10 and CIFAR-100 are averaged over 3 trials. Subscripts denote standard deviation, and bold ones stand for the best results. The last line lists improvement of MOOD over the current SOTA approach.}\n\\label{tab:multi-class}\n\\end{table*}\n\n\n\n\n%====================================exp===============================================\n\n\\section{Experiments}\n\\label{sec:exp}\nIn this section, we compare Masked Image Modeling for OOD detection (MOOD) with current SOTA approaches in one-class OOD detection (\\cref{sec:1class}), multi-class OOD detection (\\cref{sec:multi-class}), near-distribution OOD detection (\\cref{sec:near-distribution}) and OOD detection with few-shot outlier exposure (\\cref{sec:exposure}). Our MOOD outperforms all previous approaches on all four OOD detection tasks significantly. % In \\cref{sec:discuss}, we visualize the distribution of ID and OOD samples and discuss the performance for near-distribution OOD detection. \n\n\\vspace{2mm}\\noindent\\textbf{Experimental Configuration.} We report the commonly-used Area Under the Receiver Operating Characteristic Curve (AUROC) as a threshold-free evaluation metric for detecting OOD score. We perform experiments on (i) CIFAR-10 \\cite{cifar}, which consists of 50,000 training and 10,000 testing images with 10 image classes, (ii) CIFAR-100 \\cite{cifar} and CIFAR-100 (super-classes) \\cite{cifar}, which consists of 50,000 training and 10,000 testing images with 100 and 20 (super-classes) image classes. respectively, (iii) ImageNet-30 \\cite{imagenet}, which contains 39,000 training and 3,000 testing images with 30 image classes, and (iv) ImageNet-1k \\cite{imagenet}, which contains around 120k and 50k testing images with 1k image classes.\nMore details of training settings are given in the Appendix.\n\n\\subsection{One-Class OOD Detection}\n\\label{sec:1class}\nWe start with the one-class OOD detection. For a given multi-class dataset of $N_c$ classes, we conduct $N_c$ one-class OOD tasks, where each task regards one of the classes as in-distribution and the remaining classes as out-of-distribution. We run our experiments on three datasets, following prior work \\cite{geom, rot, goad}, of CIFAR-10, CIFAR-100 (super-classes), and ImageNet-30. \n\n\\Cref{tab:one-class} summarizes the results, showing that MOOD outperforms current SOTA of CSI \\cite{csi} on all tested cases significantly. The improvement is of 5.7\\% to 94.9\\% on average. The improvement is comparatively smaller on ImageNet-30 \\cref{tab:one-class-imagenet30}. It is because we do not apply intermediate fine-tuning of the model on ImageNet-30. More details are shown in \\cref{sec:fine-tune}. We provide the class-wise AUROC in the Appendix for detailed exhibition.\n\n%  with OC-SVM\\cite{goad}, DeepSVDD\\cite{deepsvdd}, AnoGAN\\cite{anogan}, OCGAN\\cite{ocgan}, Geom\\cite{geom}, Rot\\cite{rot}, Rot+Trans\\cite{rot}, GOAD\\cite{goad} and CSI\\cite{csi}\n\n%------------------------------multi-class-------------------------------------------\n\n\\subsection{Multi-Class OOD Detection} \n\\label{sec:multi-class}\nFor multi-class OOD Detection, we assume that ID samples are from a specific multi-class dataset. They are tested on various external datasets as out-of-distribution. We perform MOOD on CIFAR-10, CIFAR-100, ImageNet-30 and ImageNet-1k. For CIFAR-10, We consider CIFAR-100 \\cite{cifar}, SVHN \\cite{svhn} and LSUN \\cite{lsun} as OOD datasets. For CIFAR-100, We consider CIFAR-10 \\cite{cifar}, SVHN \\cite{svhn} and LSUN \\cite{lsun} as OOD datasets. For ImageNet-30, OOD samples are from CUB-200 \\cite{cub}, Stanford Dogs \\cite{dogs}, Oxford Pets \\cite{pets}, Oxford Flowers \\cite{flowers}, Food-101 \\cite{food}, Places-365 \\cite{places}, Caltech-256 \\cite{caltech}, and Describable Textures Dataset (DTD) \\cite{dtd}. For ImageNet-1k, we utilize non-natural images as OOD datasets, which includes iNatualist \\cite{inaturalist}, SUN \\cite{sun}, places \\cite{places}, Textures \\cite{dtd}.\n\nAs shown in \\cref{tab:multi-class}, MOOD boosts performance of current SOTA of SSD+ \\cite{ssd} by 3.0\\% to 97.6\\% and SOTA of GradNorm \\cite{gradnorm} by 2.8\\% to 89.1\\% on ImageNet-1k. We remark that when detecting hard (i.e., near-distribution) OOD samples on ImageNet30 and Food, MOOD still yields decent performance, while previous methods often fail. \n\n\\vspace{2mm}\\noindent\\textbf{Visualization.} In \\cref{fig:distribution}, we illustrate the probability distribution of the test samples according to metrics of three OOD detection approaches: baseline OOD detection \\cite{baseline_ood}, SSD+ \\cite{ssd}, and MOOD. The baseline OOD detection performs softmax as its OOD detection metric, where ID samples tend to have greater value than OOD samples. MOOD and SSD+ perform the Mahalanobis distance as their metrics. \n\nAs shown in the figure, the distance of a majority of testing ID samples to the training data is close to zero, demonstrating a similar representation of training and testing ID samples. In contrast, the distances from most OOD samples to the training data are much larger, especially on CIFAR-10 and ImageNet-30. \n\nAlso, in \\cref{fig:distribution}, we reveal that the difference in the distribution of ID and OOD samples according to MOOD is significantly larger compared with other approaches \\cite{baseline_ood, ssd}. It demonstrates that MOOD can separate ID and OOD samples more clearly. In order to illustrate the appearance of images in each ID and OOD dataset, we plot several images as examples with their corresponding distances in the Appendix.\n\n% We compare MOOD with Baseline OOD\\cite{baseline_ood}, ODIN\\cite{odin}, Mahalanobis\\cite{mahalanobis}, Residual Flows\\cite{residual_flows}, Gram Matrix\\cite{gram_matrix}, Outlier exposure\\cite{outlier_exposure}, SupCon\\cite{supcon} and SSD+\\cite{ssd}. \n\n\n%--------------------------------------visualization-----------------------------------\n\\begin{figure}[t]\n  \\centering\n  \\begin{subfigure}{\\linewidth}\n    \\includegraphics[width=0.99\\linewidth, trim=0 0 0 0, clip]{figures/distances_cifar10.pdf}\n    \\caption{ID: CIFAR-10}\n    \\label{fig:distribution_cifar100}\n  \\end{subfigure}\n  \\\\\n  \\begin{subfigure}{\\linewidth}\n    \\includegraphics[width=0.99\\linewidth, trim=0 0 0 0, clip]{figures/distances_cifar100.pdf}\n    \\caption{ID: CIFAR-100}\n    \\label{fig:distribution_cifar100}\n  \\end{subfigure}\n   \\\\\n  \\begin{subfigure}{\\linewidth}\n    \\includegraphics[width=0.99\\linewidth, trim=0 0 0 0, clip]{figures/distances_imagenet30.pdf}\n    \\caption{ID: ImageNet-30}\n    \\label{fig:distribution_imagenet30}\n    \\end{subfigure}\n    \\caption{Line chart to illustrate the relation between the probability distribution of test samples and OOD detection metrics on (a) CIFAR-10, (b) CIFAR-100, and (c) ImageNet-30. Each line in the sub-figures represents an OOD or ID dataset. We compare three OOD detection approaches, including baseline OOD detection, SSD+ (current SOTA, \\cite{ssd} ), and our proposed MOOD. The baseline OOD detection takes the maximum softmax probabilities as its OOD detection metric, while SSD+ and MOOD both use the Mahalanobis distance as their metrics.} %  The greater the difference in the distribution of ID and OOD samples according to a specific metric, the larger the degree of separation between ID and OOD samples, representing the better performance of OOD detection. As can be seen from the probability distribution, our proposed MOOD illustrates obvious priority compared with other OOD detection approaches\n  \\label{fig:distribution}\n\\end{figure}\n\n\n\n%------------------------hard samples---------------------\n\\iffalse\n\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=0.99\\linewidth]{figures/one-class-near-distribution.pdf}\n    \\caption{AUROC (\\%) of near-distribution pairs in one-class detection on CIFAR-10, compared with current SOTA (CSI \\cite{csi}).}\n  \\label{fig:hardood-oneclass}\n\\end{figure}\n\\fi\n\n\\subsection{Near-Distribution OOD Detection} \n\\label{sec:near-distribution}\nCompared with existing approaches on normal OOD detection tasks, SOTA results of near-distribution OOD detection is much worse -- AUROC of some ID-OOD pairs \\cite{csi, ssd} is even lower than 70\\%. Therefore, improving SOTA for near-OOD detection is essential for the application to work on real-world data. \n\n\\begin{table}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{2.8mm}\n\\begin{tabular}{cc|cccccc|cccc}\n\\toprule\nID & OOD  & \\multicolumn{3}{c}{AUROC (\\%)} \\\\\nclass & class  & CSI \\cite{csi} & ours  & (improve)           \\\\\n\\midrule\nPlane   & Automobile   & 74.1 & 99.0  & {\\color{teal}+24.9} \\\\\nPlane   & Ship  & 79.6 & 99.4  & {\\color{teal}+19.8} \\\\\nPlane   & Truck & 82.8 & 98.5  & {\\color{teal}+15.7} \\\\\nBird    & Horse & 83.2 & 94.3  & {\\color{teal}+11.1} \\\\\nCat     & Deer  & 83.3 & 92.6  & {\\color{teal}+9.3}  \\\\\nCat     & Dog   & 67.0 & 75.5  & {\\color{teal}+8.5}  \\\\\nCat     & Frog  & 89.6 & 92.5  & {\\color{teal}+2.9}  \\\\\nCat     & Horse & 79.0 & 95.5  & {\\color{teal}+16.5} \\\\\nDeer    & Horse & 69.0 & 100.0 & {\\color{teal}+31.0}   \\\\\nDog     & Deer  & 88.1 & 96.4  & {\\color{teal}+8.3}  \\\\\nDog     & Horse & 76.6 & 95.5  & {\\color{teal}+18.9} \\\\\nTrunk   & Automobile   & 72.3 & 87.8  & {\\color{teal}+15.5}\\\\\n\\midrule\n\\multicolumn{2}{c|}{Average} & 78.7 & 93.9 & {\\color{teal}+15.2} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Near-distribution OOD detection} (one-class). AUROC (\\%) of near-distribution pairs in one-class detection on CIFAR-10, compared with current SOTA (CSI \\cite{csi}).}\n\\label{tab:hardood-oneclass}\n\\end{table}\n\n\n\n\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=0.99\\linewidth, trim=0 15 0 10 clip]{figures/multi-class-near-distribution.pdf}\n    \\caption{\\textbf{Near-distribution OOD detection} (multi-class). Number of some mistakenly-classified OOD samples (when TPR = 95\\%). These samples are wrongly taken as ID samples by the current SOTA of SSD+ \\cite{ssd} in multi-class detection on CIFAR-10. `*' indicates SOTA.}\n  \\label{fig:hardood-multiclass}\n\\end{figure}\n\nIn \\cref{tab:structure}, we have compared MOOD with the current SOTA on near-distribution CIFAR10-CIFAR100 (ID-OOD) pair, R50+ViT \\cite{oodlimits}, and MOOD outperforms the latter significantly by 2.07\\% to 98.30\\%. In this section, we focus on the hard-detected pairs with similar semantics from \\cref{sec:1class} and \\cref{sec:multi-class}. \n\nFor one-class OOD detection, we adopt 12 hard-detected ID-OOD pairs (AUROC under 90\\%) from the confusion matrix of current one-class OOD detection SOTA of CSI \\cite{csi}. The semantics of these ID-OOD pairs are more similar than normal ID-OOD combinations, such as trunk and car, deer and horse, etc., leading to their poor OOD detection performance. As shown in \\cref{tab:hardood-oneclass}, MOOD significantly boosts the AUROC of current SOTA from 78.7\\% to 93.9\\%. \n\nFor multi-class OOD detection, we examine the large mistakenly-classified value in the OOD-ID confusion matrix, which represents the number of classifying the OOD image to the category in the ID dataset. For example, when the True-Positive Rate (TPR) is 95\\%, 48 testing tiger images from CIFAR-100 are classified as cat by the current multi-class OOD detection SOTA method of SSD+ \\cite{ssd}, while only 2 of them are wrongly classified by MOOD. More results are shown in \\cref{fig:hardood-multiclass}. For the listed 12 ID-OOD pairs, MOOD averagely reduces the number of mistakenly-classified OOD samples notably by 79\\%. \n\n\n\n\n\\begin{table}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{3.1mm}\n\\begin{tabular}{c|ccccccccccc}\n\\toprule\n\\multirow{2}{*}{Method}  & \\# OOD samples & \\multirow{2}{*}{AUROC(\\%)} \\\\\n                         & per class  &                           \\\\\n\\midrule\n\\multirow{5}{*}{R50+ViT (SOTA) \\cite{oodlimits}} & 0          & 98.52                     \\\\\n                         & 1          & 98.96                     \\\\\n                         & 2          & 99.11                     \\\\\n                         & 3          & 99.17                     \\\\\n                         & 10         & 99.29                     \\\\\n\\midrule\nours           & 0          & \\textbf{99.41}            \\\\\n(improve)                & -          & {\\color{teal}+0.12}      \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Outlier Exposure OOD detection.} AUROC (\\%) of current SOTA of R50+ViT \\cite{oodlimits} for near-distribution OOD detection and MOOD. SOTA utilizes up to 10 known OOD samples per class for detection, while ours do not include any OOD samples.}\n\\label{tab:exposure}\n\\end{table}\n\n\\subsection{OOD Detection with Outlier Exposure} \n\\label{sec:exposure}\nSeveral representative OOD detection methods \\cite{ssd, oodlimits} utilize OOD samples to improve the performance in extra stages. We note they are not included in our work because we generally believe that exposure of OOD samples violates the original intention of OOD detection. \n\nIn \\cref{tab:exposure}, we compare MOOD with current SOTA \\cite{oodlimits} for near-distribution OOD detection with up to 10 OOD samples per class. We surprisingly find that MOOD works better in terms of AUROC than current SOTA \\cite{oodlimits}, even though we do not include any OOD samples for detection. The outstanding performance of MOOD demonstrates that an effective pretext task is already sufficient for producing a distinguishable representation that OOD detection requires. Thus, there is no need to include extra OOD samples.\n\n\n%==================================Conclusion=========================================\n\n\n\\section{Conclusion}\nIn this paper, we have extensively explored the effect of multiple contributors for OOD detection and observed that reconstruction-based pretext tasks have the potential to provide effective priors for OOD detection to learn the real data distribution of the ID dataset. Specifically, we take the Masked Image Modeling pretext task for our OOD detection framework (MOOD). We perform MOOD on one-class OOD detection, multi-class OOD detection, near-distribution OOD detection, and few-shot outlier exposure OOD detection -- MOOD all achieve new SOTA results, although we do not include any OOD samples for detection.\n\n\\section{Acknowledgement}\nThis work is partially supported by Shenzhen Science and Technology Program KQTD20210811090149095.\n. The first paper ends. The second paper begins. \\IEEEtitleabstractindextext{%\n\\begin{abstract}\n\\revise{The crux of effective out-of-distribution (OOD) detection lies in acquiring a robust in-distribution (ID) representation, distinct from OOD samples. While previous methods predominantly leaned on recognition-based techniques for this purpose, they often resulted in shortcut learning, lacking comprehensive representations. In our study, we conducted a comprehensive analysis, exploring distinct pretraining tasks and employing various OOD score functions. The results highlight that the feature representations pre-trained through reconstruction yield a notable enhancement and narrow the performance gap among various score functions. This suggests that even simple score functions can rival complex ones when leveraging reconstruction-based pretext tasks. Reconstruction-based pretext tasks adapt well to various score functions. As such, it holds promising potential for further expansion. Our OOD detection framework, MOODv2, employs the masked image modeling pretext task. Without bells and whistles, MOODv2 impressively enhances 14.30\\% AUROC to 95.68\\% on ImageNet and achieves 99.98\\% on CIFAR-10.}\n\\end{abstract}\n\n% Note that keywords are not normally used for peerreview papers.\n\\begin{IEEEkeywords}\nComputer Vision, Out-of-Distribution Detection, Outlier Detection, Masked Image Modeling\n\\end{IEEEkeywords}\n\n% Codes are available at \\href{}{https://github.com/JulietLJY/MOOD}\n}\n\n\\maketitle\n\\IEEEdisplaynontitleabstractindextext\n\\IEEEpeerreviewmaketitle\n\n\\IEEEraisesectionheading{\\section{Introduction}\\label{sec:intro}}\n\\IEEEPARstart{A} reliable visual recognition system not only provides correct predictions on known context (also known as in-distribution data) but also detects unknown out-of-distribution (OOD) samples and rejects (or transfers) them to human intervention for safe handling. This motivates the applications of outlier detectors before feeding input to the downstream networks, which is the main task of OOD detection, also referred to as novelty or anomaly detection. OOD detection is the task of identifying whether a test sample is drawn far from the in-distribution (ID) data or not. It is at the cornerstone of various safety-critical applications, including medical diagnosis \\cite{caruana2015intelligible}, fraud detection \\cite{phua2010comprehensive}, autonomous driving \\cite{eykholt2018robust}, etc~\\cite{tagclip, motcoder, bal}. A representative in-distribution feature space representation is crucial for out-of-distribution detection. A well-crafted feature representation significantly enhances the performance via most mainstream OOD detection score functions. Our research is dedicated to refining feature representations tailored for OOD detection, with the aim of advancing the entire field.\n\nExisting methods perform contrastive learning \\cite{csi, ssd} or pretrain classification on a large dataset \\cite{oodlimits, vim, yang2021generalized, sariyildiz2023fake} to detect OOD samples. The former methods classify images according to the pseudo labels while the latter classifies images based on ground truth, whose core tasks are both to fulfill the classification target. However, research on backdoor attack \\cite{backdoor_attack, frog_attack} shows that when learning is represented by classifying data, networks tend to take a shortcut to classify images. In a typical backdoor attack scene \\cite{frog_attack}, the attacker adds secret triggers on original training images with the visibly correct label. During the course of testing, the victim model classifies images with secret triggers into the wrong category. Research in this area demonstrates that networks only learn specific distinguishable patterns of different categories because it is a shortcut to fulfill the classification requirement. Nonetheless, learning these patterns is ineffective for OOD detection. Thus, learning representations by classifying ID data for OOD detection may not be satisfying. For example, when patterns similar to some ID categories appear in OOD samples, the network could easily interpret these OOD samples as the ID data and classify them into the wrong ID categories\\revise{, as shown in \\cref{fig:intro}}. \n\n\n\\begin{figure}[t]\n  \\centering    \n    \\includegraphics[width=0.8\\linewidth]{figures/ablation_imagenet.pdf}\n    \\caption{\\re{The average AUROC (\\%) tested on four OOD datasets applied to a ViT model with different pre-text tasks. \n    Methods in blue use the feature space;\n    methods in green use logits;\n    methods in yellow use the softmax probability;\n    and methods in red use both features and logits. The stars show the average performance of a category of methods.}}\n  \\label{fig:ablation}\n\\end{figure}\n\n\n\\begin{figure*}[t]\n  \\centering\n    \\includegraphics[width=0.9\\linewidth, trim=75 225 380 120, clip]{figures/mood.pdf}\n    \\caption{\\revise{Comparison of reconstruction-based and classification-based methods. In the context of image classification, networks often take a shortcut when categorizing images \\cite{backdoor_attack, frog_attack}. For example, ears are a distinctive feature for distinguishing between cats and dogs, and a classification model typically assumes that animals with pointed ears are cats, while those without are dogs. Consequently, when the network encounters an out-of-distribution animal, such as a fox with pointed ears, it readily misclassifies it as a cat. In contrast, reconstruction-based tasks effectively mitigate this issue. By randomly masking portions of images, the model avoids learning localized, stereotypical features (e.g., masked ears), thus preventing shortcuts and instead acquiring effective pixel-level representations for ID data. This significantly improves the model's ability to detect OOD instances.}}\n  \\label{fig:intro}\n\\end{figure*}\n\n\n\\iffalse\n\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=0.99\\linewidth]{figures/impression.pdf}\n    \\caption{\\revise{The AUROC (in percentage) of eight OOD detection algorithms applied to a ViT model with five pre-text tasks.The OOD datasets are ImageNet-O ($x$-axis) and OpenImage-O ($y$-axis).}}\n  \\label{fig:impression}\n\\end{figure}\n\\fi\n\n\\revise{To remedy this issue, we introduce the reconstruction-based pretext task. Different from contrastive learning in existing OOD detection approaches \\cite{csi, ssd}, our method forces the network to achieve the training purpose of reconstructing the image and thus makes it learn pixel-level feature representation. Specifically, we adopt the masked image modeling (MIM) \\cite{beit} as our self-supervised pretext task, which has been demonstrated to have great potential in both natural language processing \\cite{bert} and computer vision \\cite{beit, mae}. In the MIM task, a proportion of image patches are randomly masked. The network learns information from the remaining patches to speculate the masked patches and restore tokens of the original image. The reconstruction process enables the model to learn from the prior effective ID feature representation rather than just learning different patterns among categories in the classification process. In our work, we observed that the pre-trained models effectively reconstruct ID images, whereas they exhibit distinct domain differences when it comes to the OOD domain (\\cref{fig:recover}). This visual discrepancy clearly underscores the existing domain gap in model features between ID and OOD data, offering valuable insights for OOD detection.}\n\n\\revise{To validate the effectiveness of our ID feature representation, we conduct experiments to test its performance with various mainstream OOD detection score functions. We employed OOD score functions encompassing probability-based \\cite{baseline_ood, maxlogit}, logits-based \\cite{energy, maxlogit}, features-based \\cite{vim, react, mahalanobis}, and hybrid methods utilizing both logits and features \\cite{vim}. In the context of a comparative analysis spanning classic classification \\cite{vit}, contrastive learning \\cite{mocov3, dinov2}, and masked image modeling pretext tasks \\cite{beit, beitv2}, our findings underscore the dominant role of reconstruction-based strategies in the field of OOD detection, as illustrated in \\cref{fig:ablation}.}\n\n\\revise{Furthermore, we conduct a comprehensive analysis of the experimental results and observe that our approach not only significantly improves the overall results but also substantially reduces the disparities among score functions. This observation underscores that even simple score functions can perform on par with more complex ones when a representative ID feature representation is utilized. These findings further emphasize the critical importance of effective feature representation in OOD detection. More details are in \\cref{sec:pretask}.} \\revise{Ultimately, MOODv2 demonstrates remarkable enhancements, achieving a substantial 14.30\\% increase, reaching 95.68\\% AUROC on ImageNet. On CIFAR-10, our results significantly improved to an impressive 99.98\\%, marking a notable 0.35\\% enhancement compared to the previous state-of-the-art.}\n\n\n\n\\section{Related Works}\\label{sec:related}\n\n\\subsection{Out-of-distribution Detection}\n\\revise{Many scoring functions have been developed by researchers to distinguish between in-distribution and out-of-distribution examples. These functions are designed to exploit properties that are typically exhibited by ID examples but violated by OOD examples, and vice versa. These scores are primarily derived from three sources:}\n\\begin{enumerate}\n\n    \\item \\revise{\\textbf{Probability-based}: This category includes measures like the maximum softmax probabilities~\\cite{baseline_ood} and the minimum KL-divergence between the softmax and the mean class-conditional distributions~\\cite{maxlogit}, etc.}\n\n    \\item \\revise{\\textbf{Logit-based}: These functions rely on maximum logits~\\cite{maxlogit} and the \\(\\mathrm{logsumexp}\\) function computed over logits~\\cite{energy}, etc.}\n\n    \\item \\revise{\\textbf{Feature-based}: These functions involve the norm of the residual between a feature and the pre-image of its low-dimensional embedding~\\cite{ndiour2020out} and the minimum Mahalanobis distance between a feature and the class centroids~\\cite{mahalanobis}, among others.}\n\n\\end{enumerate}\n\\revise{After a thorough analysis of the performance and their correlations with various score functions and pretext tasks, our work follows the hybrid methods combining logit and feature \\cite{vim}} and includes the reconstruction-based methods as a pretext task. We will explain the implementation details later in this paper.\n\n\\subsection{Self-Supervised Pretext Task}\n\\revise{In the ever-evolving landscape of computer vision and deep learning, a multitude of strategies and techniques have been devised to enhance the capacity of models to understand and process visual data\\re{:}}\n\n\\begin{enumerate}\n\n    \\item \\revise{\\textbf{Classification task:} Vision models are pre-trained via classical classification task \\cite{vit}.}\n\n    \\item \\revise{\\textbf{Contrastive Learning tasks: }MOCOv3 \\cite{mocov3} and DINOv2 \\cite{dinov2} are advanced contrastive learning methods used for self-supervised representation learning. These methods focus on learning representations by contrasting positive pairs (e.g., different augmentations of the same image) with negative pairs (e.g., augmentations from different images). MOCOv3 extends the MOCO framework \\cite{moco} with a momentum encoder and dynamic queues for improved performance. DINOv2 introduces a clustered teacher network and an asymmetric loss to learn efficient representations.}\n\n    \\item \\revise{\\textbf{Masked Image Modeling Tasks: } Data-Efficient Image Transformer (BEiT series \\cite{beit, beitv2}) are self-supervised learning tasks that involve masked image modeling. In these tasks, a portion of an image is randomly masked, and the model's objective is to predict the masked pixels, effectively filling in the blanks.}\n    \n\\end{enumerate}\n\n\\revise{These methods and tasks represent cutting-edge approaches in the field of computer vision and deep learning. They have led to substantial improvements in the ability of models to learn useful visual representations from unlabeled data, enabling better performance on various downstream vision tasks.}\n\nMultiple existing methods take advantage of self-supervised tasks to guide the learning of representation for OOD detection. Previous work \\cite{csi, ssd} presents contrastive learning models as feature extractors. However, existing approaches of classifying transformed images according to contrastive learning possess similar limitations -- that is, the model tends to learn the specific patterns of categories \\cite{backdoor_attack, backdoor_survey}, which are beneficial for classification but do not help understand the intrinsic ID representation. In our work, we address this issue by performing the masked image modeling task for OOD detection.\n\n\\subsection{Training Strategy}\n\\revise{Numerous approaches have been developed to address OOD-awareness in training loss~\\cite{confbranch2018arxiv}. These methods often involve the introduction of regularization terms aimed at encouraging a clearer separation between ID and OOD features~\\cite{onedim21cvpr,huang2021mos}. In some cases, networks are augmented with confidence estimation branches, utilizing misclassified in-distribution examples as proxies for out-of-distribution ones~\\cite{confbranch2018arxiv}. MOS~\\cite{huang2021mos} adapts the loss function by incorporating a predefined group structure, enabling the minimum group-wise ``else\" class probability to serve as an indicator of OOD classification. An alternative approach~\\cite{onedim21cvpr} focuses on compelling ID samples to embed into a union of 1-dimensional subspaces during training, and it evaluates the minimum angular distance between the feature and class-wise subspaces. }\n\n\\revise{In contrast to these approaches, our method belongs to the lightweight training-free methods \\cite{vim, MOOD}, which doesn't necessitate retraining the model. Therefore, it not only offers a more straightforward application but also preserves the accuracy of in-distribution classification.}\n\n\n\\begin{figure}[t]\n  \\centering\n    \\subfloat[\\re{ID: ImageNet}]{\\includegraphics[width=0.99\\linewidth]{figures/compare_moodv1_imagenet.pdf}}\n    \n    \\subfloat[\\re{ID: CIFAR-10}]{\\includegraphics[width=0.99\\linewidth]{figures/compare_moodv1_cifar10.pdf}}\n    % \\includegraphics[width=0.99\\linewidth]{figures/compare_moodv1_imagenet.pdf}\n    \\caption{\\re{The AUROC (\\%) of MOODv2 and MOODv1 tested on ID datasets (a) ImageNet and (b) CIFAR-10. OOD datasets including OpenImage-O \\cite{openimages_o}, Texture \\cite{dtd}, iNaturalist \\cite{inaturalist}, and ImageNet-O \\cite{imagenet_o}. }}\n  \\label{fig:moodv1}\n\\end{figure}\n\n\\begin{figure*}[t]\n  \\centering\n    \\includegraphics[width=0.99\\linewidth, trim=20 10 20 10]{figures/plot_recover_images.pdf}\n    \\caption{\\revise{Each image pair consists of the original image (left) and reconstructed image (right). The rows of images are sourced from ImageNet \\cite{imagenet}, Texture \\cite{cimpoi14describing}, iNaturalist \\cite{van2018inaturalist}, ImageNet-O \\cite{hendrycks2021natural}, and OpenImage-O \\cite{openimages_o}. The number in the top left corner of each image pair represents the Euclidean distance between the two images.}}\n  \\label{fig:recover}\n\\end{figure*}\n\n\\begin{table}[t]\n\\small\n\\centering\n\n\\subfloat[ID: CIFAR-10]{\n\\setlength{\\tabcolsep}{1.1mm}\n\\begin{tabular}{c|ccccccccc}\n\\toprule\nMethods & prob & feat & logit & feat+logit \\\\\n\\midrule\nViT\\cite{vit} & \\revise{73.61\\scriptsize{$\\pm$21.36}} & \\revise{82.61\\scriptsize{$\\pm$23.81}} & \\revise{45.11\\scriptsize{$\\pm$4.45}} & \\revise{\\textbf{99.63}} \\\\\nMoCov3\\cite{mocov3} & \\revise{70.96\\scriptsize{$\\pm$23.68}} & \\revise{79.17\\scriptsize{$\\pm$28.75}} & \\revise{41.42\\scriptsize{$\\pm$3.50}} & \\revise{\\textbf{99.73}} \\\\\nDINOv2\\cite{dinov2} & \\revise{87.20\\scriptsize{$\\pm$10.62}} & \\revise{84.73\\scriptsize{$\\pm$21.57}} & \\revise{80.30\\scriptsize{$\\pm$0.10}} & \\revise{\\textbf{99.98}} \\\\\n\\rowcolor{gray!20}BEiTv2\\cite{beitv2} & \\revise{79.96\\scriptsize{$\\pm$13.71}} & \\revise{91.77\\scriptsize{$\\pm$11.47}} & \\revise{72.87\\scriptsize{$\\pm$2.08}} & \\revise{\\textbf{99.87}} \\\\\n\\rowcolor{gray!20}BEiT\\cite{beit} & \\revise{77.51\\scriptsize{$\\pm$17.83}} & \\revise{89.05\\scriptsize{$\\pm$15.46}} & \\revise{65.05\\scriptsize{$\\pm$2.06}} & \\revise{\\textbf{99.98}} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\subfloat[ID: ImageNet]{\n\\setlength{\\tabcolsep}{1.3mm}\n\\begin{tabular}{c|ccccccccc}\n\\toprule\nMethods & prob & feat & logit & feat+logit \\\\\n\\midrule\nViT\\cite{vit} & \\revise{\\textbf{78.52}\\scriptsize{$\\pm$1.76}} & \\revise{76.86\\scriptsize{$\\pm$3.20}} & \\revise{70.61\\scriptsize{$\\pm$4.76}} & \\revise{77.65} \\\\\nMoCov3\\cite{mocov3} & \\revise{\\textbf{78.36}\\scriptsize{$\\pm$1.42}} & \\revise{72.51\\scriptsize{$\\pm$6.21}} & \\revise{70.61\\scriptsize{$\\pm$5.04}} & \\revise{72.07} \\\\\nDINOv2\\cite{dinov2} & \\revise{59.64\\scriptsize{$\\pm$7.82}} & \\revise{\\textbf{63.56}\\scriptsize{$\\pm$2.89}} & \\revise{60.70\\scriptsize{$\\pm$4.51}} & \\revise{61.32} \\\\\n\\rowcolor{gray!20}BEiTv2\\cite{beitv2} & \\revise{89.07\\scriptsize{$\\pm$0.24}} & \\revise{92.96\\scriptsize{$\\pm$1.27}} & \\revise{90.29\\scriptsize{$\\pm$0.13}} & \\revise{\\textbf{95.42}} \\\\\n\\rowcolor{gray!20}BEiT\\cite{beit} & \\revise{89.47\\scriptsize{$\\pm$0.47}} & \\revise{93.30\\scriptsize{$\\pm$1.89}} & \\revise{89.84\\scriptsize{$\\pm$0.01}} & \\revise{\\textbf{95.68}} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\caption{\\revise{The AUROC (\\%) of four types of methods: probability-based methods MSP \\cite{baseline_ood} and KL-Matching \\cite{maxlogit}; logits-based methods Energy \\cite{energy} and MaxLogit \\cite{maxlogit}; features-based methods Residual \\cite{vim}, React \\cite{react} and Mahalanobis \\cite{mahalanobis}; and methods using both logits and features include ViM \\cite{vim}. The best method for each model is emphasized in bold.}\n}\n\\label{tab:ablation-statistic}\n\\end{table}\n\n\n\\subsection{MOODv1}\n\\revise{Our previous version MOODv1 \\cite{MOOD} has introduced masked image modeling pretraining strategy into the OOD detection (MOOD) and achieved promising results. However, there are still concerns:}\n\n\\revise{Firstly, previous studies \\cite{MOOD, csi, ssd} have typically necessitated fine-tuning a model on each in-distribution dataset. The expense of training becomes notably high when dealing with a substantial number of ID datasets to be assessed, such as in one-class OOD detection \\cite{csi, MOOD}. However, through experimental validation, we have discovered that a well-prepared masked image modeling model doesn't require additional fine-tuning to achieve outstanding detection performance, conserving substantial fine-tuning resource consumption when dealing with a plethora of ID datasets that require evaluation.}\n\n\\revise{Secondly, as the field has seen the emergence of more advanced OOD score functions \\cite{maxlogit, vim, react, baseline_ood, energy, mahalanobis} and pretraining techniques \\cite{beitv2, dinov2, mocov3, beit, vit}, it raises the question of whether masked image modeling continues to maintain its leading role. In MOODv2, we integrate the latest advancements in pretraining methods and conduct experiments with an array of state-of-the-art OOD score functions. This broader spectrum of pretraining methods and score functions allows for a more comprehensive assessment of the MOODv2's performance, better aligning MOODv2 with the increasingly intricate challenges of OOD detection. }\n\n\\revise{Lastly, it is well known that if the network has seen similar samples in training, regardless of pre-training or fine-tuning, the OOD performance will be more or less trivial \\cite{openimages_o}. Previous works \\cite{oodlimits, MOOD} rely on pre-training on ImageNet-21K, so that the benchmark OOD dataset such as CIFAR \\cite{cifar}, Places \\cite{places}, etc., is unlikely to be untouched by the ImageNet-21K \\cite{imagenet} dataset. In this work, MOODv2 introduces the latest unnatural datasets as OOD, which rules out the possibility of overlap between the OOD test set and the training set \\cite{openimages_o, imagenet_o}.}\n\n\n\\revise{In summary, MOODv2 incorporates improved score functions, advanced pretraining techniques, a wider range of unnatural OOD datasets, and a streamlined general framework. The performance improvement of MOODv2 compared to MOODv1 is depicted in Fig. \\ref{fig:moodv1}. On ImageNet, MOODv2 exhibits a noteworthy 2.17\\% improvement in AUROC compared to MOODv1. Furthermore, on CIFAR-10, MOODv2, without finetuning on the ID dataset, achieves an exceptional AUROC score of up to 99.98\\%. }\n\n\\section{Methods}\\label{sec:methods}\n\n\n\n% \\iffalse\n\\begin{table}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{0.5mm}\n\\begin{tabular}{c|cccccccccc|c}\n\\toprule\nDatasets & ImageNet & Texture & iNaturalist & ImageNet-O & OpenImage-O \\\\\n\\midrule\nDistance & 18.09 & 32.89 & 38.96 & 30.76 & 45.56 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Difference.} .}\n\\label{tab:difference}\n\\end{table}\n\\fi \n\n\\begin{table}[t]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{3mm}\n\\begin{tabular}{c|cc}\n\\toprule\nID/OOD & Datasets & Recovery Distance \\\\\n\\midrule\nID & ImageNet (ID) \\cite{imagenet} & \\revise{18.09} \\\\\n\\midrule\n\\multirow{4}{*}{OOD} & Texture \\cite{dtd} & \\revise{32.89} \\\\\n& INaturalist \\cite{inaturalist}& \\revise{38.96} \\\\\n& ImageNet-O \\cite{imagenet_o} & \\revise{30.76} \\\\\n& OpenImage-O \\cite{openimages_o} & \\revise{45.56} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\revise{The average recovery distance (L2 norm between the original images and their corresponding reconstructions) for ID and OOD datasets. For each dataset, the number of sampled images is the minimum of 5000 and the dataset size.}}\n\\label{tab:difference}\n\\end{table}\n\n% We first define the notations. For a given dataset $X_{\\rm ID}$, the goal of out-of-distribution (OOD) detection is to model a detector that identifies whether an input image $x \\in X_{\\rm ID}$ or $x \\notin X_{\\rm ID}$ (that is, $x \\in X_{\\rm OOD}$). A majority of existing methods for OOD detection define an OOD score function $s(x)$. Its abnormal high or low value represents that $x$ is from out-of-distribution. \n\n\\revise{In this section, we initiate our exploration of reconstruction tasks for OOD detection by presenting the underlying motivation in \\cref{sec:motivation}. Following that, in \\cref{sec:pretask}, we delve into a comprehensive analysis of the essential attributes that play a pivotal role in OOD detection.}\n\n\\subsection{Motivation: seeking for effective ID representation} \n\\label{sec:motivation}\n\n% In this section, we choose the pretext task that can provide the intrinsic prior to suit the OOD detection task. \nMost previous OOD methods learn the ID representation through classification \\cite{baseline_ood, oodlimits} or contrastive learning \\cite{csi, ssd} on ID samples, which take advantage of either the ground truth or pseudo labels to supervise the classification networks. On the other hand, work of \\cite{backdoor_attack, frog_attack} shows that classification networks only learn different patterns among training categories because it is a shortcut to fulfill classification. \\revise{It is indicated that the network actually does not learn the effective in-distribution representation. In comparison, the reconstruction-based pretext task forces the network to learn the pixel-level image representation of the ID images during training to reconstruct the image instead of the patterns for classification. In this way, the network can learn a more representative feature of the ID dataset.}\n\n\\iffalse\n\\begin{equation}\nx_{r} = f_{e}\\cdot f_{d}(x),\n\\end{equation}\nwhere $x$ and $x_{r}$ represent the original and reconstructed images. $f_{e}$ and $f_{d}$ correspond to the image encoder and decoder, and $f_{e}$ needs to learn representative features from the training dataset to reconstruct images. To assess the benefits of using the pretask features, we compute the recovery distance between the original and reconstructed images as follows:\n\\begin{equation}\nd_r = \\left\\lVert x - x_{r} \\right\\rVert.\n\\end{equation}\n\\fi\n\\revise{To verify this, we reconstruct ID and OOD data and compute the Euclidean distance between the original and reconstructed images. A greater distance indicates a larger deviation of the reconstructed image from the original image. We collect recovery distances for ID and OOD data. Examples of the reconstruction are depicted in \\cref{fig:recover}. In the first row, for ID images, pre-trained models reconstruct the images effectively. Instead, for unnatural OOD images in the following rows, clear domain discrepancies emerge. For instance, in the case of textured images, the models still apply lighting and shadows reminiscent of natural images. In the case of sketch images,  the models render the images smoother and brighter. This discrepancy visually highlights the domain gap in model features between ID and OOD data, which can be leveraged for OOD detection.}\n\n\n\n\\begin{figure*}[t]\n  \\centering\n    \\subfloat[\\re{ID: CIFAR-10}]{\\includegraphics[width=0.99\\linewidth, trim=10 15 5 15]{figures/detailed_ablation_cifar10.pdf}}\n    \n    \\subfloat[\\re{ID: ImageNet}]{\\includegraphics[width=0.99\\linewidth, trim=10 15 5 15]{figures/detailed_ablation_imagenet.pdf}}\n\n    \\caption{\\revise{The AUROC (\\%) tested on unnatural OOD datasets of various OOD detection algorithms applied to a ViT model. \n    The pre-text tasks include classification task \\cite{vit}, contrastive learning tasks MoCov3 \\cite{mocov3} and DINOv2 \\cite{dinov2}, and masked image modeling tasks BEiT series \\cite{beit,beitv2}.\n    Methods in blue utilize the feature space;\n    methods in green use logits;\n    methods in yellow make use of the softmax probability.\n    and methods in red leverage both features and logits.\n    Stars represent the average AUROC for methods in the corresponding colors; light vertical lines represent the standard deviation.\n    }}\n    \n    % probability-based methods MSP \\cite{baseline_ood} and KL-Matching \\cite{maxlogit}; logits-based methods Energy \\cite{energy} and MaxLogit \\cite{maxlogit}; features-based methods Residual \\cite{vim}, React \\cite{react} and Mahalanobis \\cite{mahalanobis}; and methods using both logits and features include ViM \\cite{vim}.\n    \n  \\label{fig:detailed_ablation}\n\\end{figure*}\n\n\\subsection{Reconstruction Tasks for OOD Detection} \\label{sec:pretask}\n%--------------------------------------multi-class-----------------------------------\n\\begin{table*}[tp]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{0.6mm}\n\\begin{tabular}{c|c|ccccccccccc}\n\\toprule\n\\multirow{2}{*}{Methods} & \\multirow{2}{*}{Models} & \\multicolumn{2}{c}{Texture \\cite{dtd}} & \\multicolumn{2}{c}{iNaturalist \\cite{inaturalist}} & \\multicolumn{2}{c}{ImageNet-O \\cite{imagenet_o}} & \\multicolumn{2}{c}{ OpenImage-O \\cite{openimages_o}} & \\multicolumn{2}{c}{Average} \\\\\n& & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ \\\\\n\\midrule\n\\multirow{5}{*}{MSP\\cite{baseline_ood}}\n& ViT\\cite{vit} & \\revise{71.31} & \\revise{71.31} & \\revise{90.70} & \\revise{90.70} & \\revise{60.77} & \\revise{60.77} & \\revise{84.29} & \\revise{84.29} & \\revise{76.77} & \\revise{76.77} \\\\\n& MoCov3\\cite{mocov3} & \\revise{66.85} & \\revise{66.85} & \\revise{90.68} & \\revise{90.68} & \\revise{64.80} & \\revise{64.80} & \\revise{85.42} & \\revise{85.42} & \\revise{76.94} & \\revise{76.94} \\\\\n& DINOv2\\cite{dinov2} & \\revise{47.49} & \\revise{47.49} & \\revise{62.13} & \\revise{62.13} & \\revise{44.87} & \\revise{44.87} & \\revise{52.83} & \\revise{52.83} & \\revise{51.83} & \\revise{51.83} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.61}}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.61}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.05}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.05}}} & \\cellcolor{gray!20}{\\revise{81.15}} & \\cellcolor{gray!20}{\\revise{81.15}} & \\cellcolor{gray!20}{\\revise{\\textbf{92.52}}} & \\cellcolor{gray!20}{\\revise{\\textbf{92.52}}} & \\cellcolor{gray!20}{\\revise{88.83}} & \\cellcolor{gray!20}{\\revise{88.83}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{85.05}} & \\cellcolor{gray!20}{\\revise{85.05}} & \\cellcolor{gray!20}{\\revise{95.50}} & \\cellcolor{gray!20}{\\revise{95.50}} & \\cellcolor{gray!20}{\\revise{\\textbf{83.17}}} & \\cellcolor{gray!20}{\\revise{\\textbf{83.17}}} & \\cellcolor{gray!20}{\\revise{92.28}} & \\cellcolor{gray!20}{\\revise{92.28}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.00}}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.00}}} \\\\\n\\midrule\n\\multirow{5}{*}{Energy\\cite{energy}}\n& ViT\\cite{vit} & \\revise{54.11} & \\revise{54.11} & \\revise{76.61} & \\revise{76.61} & \\revise{61.63} & \\revise{61.63} & \\revise{71.06} & \\revise{71.06} & \\revise{65.85} & \\revise{65.85} \\\\\n& MoCov3\\cite{mocov3} & \\revise{48.79} & \\revise{48.79} & \\revise{76.80} & \\revise{76.80} & \\revise{64.56} & \\revise{64.56} & \\revise{72.13} & \\revise{72.13} & \\revise{65.57} & \\revise{65.57} \\\\\n& DINOv2\\cite{dinov2} & \\revise{73.89} & \\revise{73.89} & \\revise{80.34} & \\revise{80.34} & \\revise{49.98} & \\revise{49.98} & \\revise{56.64} & \\revise{56.64} & \\revise{65.21} & \\revise{65.21} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.32}}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.32}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.95}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.95}}} & \\cellcolor{gray!20}{\\revise{85.27}} & \\cellcolor{gray!20}{\\revise{85.27}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.14}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.14}}} & \\cellcolor{gray!20}{\\revise{\\textbf{90.42}}} & \\cellcolor{gray!20}{\\revise{\\textbf{90.42}}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{83.04}} & \\cellcolor{gray!20}{\\revise{83.04}} & \\cellcolor{gray!20}{\\revise{96.48}} & \\cellcolor{gray!20}{\\revise{96.48}} & \\cellcolor{gray!20}{\\revise{\\textbf{86.36}}} & \\cellcolor{gray!20}{\\revise{\\textbf{86.36}}} & \\cellcolor{gray!20}{\\revise{93.50}} & \\cellcolor{gray!20}{\\revise{93.50}} & \\cellcolor{gray!20}{\\revise{89.85}} & \\cellcolor{gray!20}{\\revise{89.85}} \\\\\n\\midrule\n\\multirow{5}{*}{MaxLogit\\cite{maxlogit}}\n& ViT\\cite{vit} & \\revise{67.22} & \\revise{67.22} & \\revise{89.88} & \\revise{89.88} & \\revise{61.68} & \\revise{61.68} & \\revise{82.73} & \\revise{82.73} & \\revise{75.37} & \\revise{75.37} \\\\\n& MoCov3\\cite{mocov3} & \\revise{62.36} & \\revise{62.36} & \\revise{90.38} & \\revise{90.38} & \\revise{65.65} & \\revise{65.65} & \\revise{84.19} & \\revise{84.19} & \\revise{75.64} & \\revise{75.64} \\\\\n& DINOv2\\cite{dinov2} & \\revise{54.70} & \\revise{54.70} & \\revise{69.98} & \\revise{69.98} & \\revise{45.60} & \\revise{45.60} & \\revise{54.52} & \\revise{54.52} & \\revise{56.20} & \\revise{56.20} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.94}}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.94}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.90}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.90}}} & \\cellcolor{gray!20}{\\revise{83.97}} & \\cellcolor{gray!20}{\\revise{83.97}} & \\cellcolor{gray!20}{\\revise{\\textbf{93.82}}} & \\cellcolor{gray!20}{\\revise{\\textbf{93.82}}} & \\cellcolor{gray!20}{\\revise{\\textbf{90.16}}} & \\cellcolor{gray!20}{\\revise{\\textbf{90.16}}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{84.17}} & \\cellcolor{gray!20}{\\revise{84.17}} & \\cellcolor{gray!20}{\\revise{96.48}} & \\cellcolor{gray!20}{\\revise{96.48}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.34}}} & \\cellcolor{gray!20}{\\revise{\\textbf{85.34}}} & \\cellcolor{gray!20}{\\revise{93.31}} & \\cellcolor{gray!20}{\\revise{93.31}} & \\cellcolor{gray!20}{\\revise{89.83}} & \\cellcolor{gray!20}{\\revise{89.83}} \\\\\n\\midrule\n\\multirow{5}{*}{KL-Matching\\cite{maxlogit}}\n& ViT\\cite{vit} & \\revise{82.59} & \\revise{82.59} & \\revise{87.63} & \\revise{87.63} & \\revise{66.55} & \\revise{66.55} & \\revise{84.34} & \\revise{84.34} & \\revise{80.28} & \\revise{80.28} \\\\\n& MoCov3\\cite{mocov3} & \\revise{82.35} & \\revise{82.35} & \\revise{86.24} & \\revise{86.24} & \\revise{67.80} & \\revise{67.80} & \\revise{82.73} & \\revise{82.73} & \\revise{79.78} & \\revise{79.78} \\\\\n& DINOv2\\cite{dinov2} & \\revise{80.51} & \\revise{80.51} & \\revise{56.93} & \\revise{56.93} & \\revise{69.77} & \\revise{69.77} & \\revise{62.63} & \\revise{62.63} & \\revise{67.46} & \\revise{67.46} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{87.14}} & \\cellcolor{gray!20}{\\revise{87.14}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.13}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.13}}} & \\cellcolor{gray!20}{\\revise{82.87}} & \\cellcolor{gray!20}{\\revise{82.87}} & \\cellcolor{gray!20}{\\revise{92.10}} & \\cellcolor{gray!20}{\\revise{92.10}} & \\cellcolor{gray!20}{\\revise{89.31}} & \\cellcolor{gray!20}{\\revise{89.31}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{\\textbf{87.87}}} & \\cellcolor{gray!20}{\\revise{\\textbf{87.87}}} & \\cellcolor{gray!20}{\\revise{94.82}} & \\cellcolor{gray!20}{\\revise{94.82}} & \\cellcolor{gray!20}{\\revise{\\textbf{84.56}}} & \\cellcolor{gray!20}{\\revise{\\textbf{84.56}}} & \\cellcolor{gray!20}{\\revise{\\textbf{92.48}}} & \\cellcolor{gray!20}{\\revise{\\textbf{92.48}}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.93}}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.93}}} \\\\\n\\midrule\n\\multirow{5}{*}{Residual\\cite{vim}}\n& ViT\\cite{vit} & \\revise{82.39} & \\revise{82.39} & \\revise{73.72} & \\revise{73.72} & \\revise{68.44} & \\revise{68.44} & \\revise{74.88} & \\revise{74.88} & \\revise{74.86} & \\revise{74.86} \\\\\n& MoCov3\\cite{mocov3} & \\revise{75.25} & \\revise{75.25} & \\revise{73.80} & \\revise{73.80} & \\revise{57.69} & \\revise{57.69} & \\revise{67.82} & \\revise{67.82} & \\revise{68.64} & \\revise{68.64} \\\\\n& DINOv2\\cite{dinov2} & \\revise{66.50} & \\revise{66.50} & \\revise{61.90} & \\revise{61.90} & \\revise{58.94} & \\revise{58.94} & \\revise{56.84} & \\revise{56.84} & \\revise{61.04} & \\revise{61.04} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.99}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.99}}} & \\cellcolor{gray!20}{\\revise{99.01}} & \\cellcolor{gray!20}{\\revise{99.01}} & \\cellcolor{gray!20}{\\revise{87.23}} & \\cellcolor{gray!20}{\\revise{87.23}} & \\cellcolor{gray!20}{\\revise{95.43}} & \\cellcolor{gray!20}{\\revise{95.43}} & \\cellcolor{gray!20}{\\revise{94.17}} & \\cellcolor{gray!20}{\\revise{94.17}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{94.16}} & \\cellcolor{gray!20}{\\revise{94.16}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.50}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.50}}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.35}}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.35}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.52}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.52}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.88}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.88}}} \\\\\n\\midrule\n\\multirow{5}{*}{React\\cite{react}}\n& ViT\\cite{vit} & \\revise{62.09} & \\revise{62.09} & \\revise{91.20} & \\revise{91.20} & \\revise{63.66} & \\revise{63.66} & \\revise{80.43} & \\revise{80.43} & \\revise{74.34} & \\revise{74.34} \\\\\n& MoCov3\\cite{mocov3} & \\revise{51.47} & \\revise{51.47} & \\revise{79.30} & \\revise{79.30} & \\revise{65.33} & \\revise{65.33} & \\revise{74.35} & \\revise{74.35} & \\revise{67.61} & \\revise{67.61} \\\\\n& DINOv2\\cite{dinov2} & \\revise{76.73} & \\revise{76.73} & \\revise{74.25} & \\revise{74.25} & \\revise{56.26} & \\revise{56.26} & \\revise{63.17} & \\revise{63.17} & \\revise{67.60} & \\revise{67.60} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{86.10}}} & \\cellcolor{gray!20}{\\revise{\\textbf{86.10}}} & \\cellcolor{gray!20}{\\revise{\\textbf{98.09}}} & \\cellcolor{gray!20}{\\revise{\\textbf{98.09}}} & \\cellcolor{gray!20}{\\revise{85.69}} & \\cellcolor{gray!20}{\\revise{85.69}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.96}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.96}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.21}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.21}}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{84.32}} & \\cellcolor{gray!20}{\\revise{84.32}} & \\cellcolor{gray!20}{\\revise{96.99}} & \\cellcolor{gray!20}{\\revise{96.99}} & \\cellcolor{gray!20}{\\revise{\\textbf{87.04}}} & \\cellcolor{gray!20}{\\revise{\\textbf{87.04}}} & \\cellcolor{gray!20}{\\revise{94.21}} & \\cellcolor{gray!20}{\\revise{94.21}} & \\cellcolor{gray!20}{\\revise{90.64}} & \\cellcolor{gray!20}{\\revise{90.64}} \\\\\n\\midrule\n\\multirow{5}{*}{Mahalanobis\\cite{mahalanobis}}\n& ViT\\cite{vit} & \\revise{84.93} & \\revise{84.93} & \\revise{84.90} & \\revise{84.90} & \\revise{71.53} & \\revise{71.53} & \\revise{84.16} & \\revise{84.16} & \\revise{81.38} & \\revise{81.38} \\\\\n& MoCov3\\cite{mocov3} & \\revise{84.29} & \\revise{84.29} & \\revise{86.95} & \\revise{86.95} & \\revise{70.33} & \\revise{70.33} & \\revise{83.54} & \\revise{83.54} & \\revise{81.28} & \\revise{81.28} \\\\\n& DINOv2\\cite{dinov2} & \\revise{68.58} & \\revise{68.58} & \\revise{63.14} & \\revise{63.14} & \\revise{58.86} & \\revise{58.86} & \\revise{57.57} & \\revise{57.57} & \\revise{62.04} & \\revise{62.04} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{93.01}} & \\cellcolor{gray!20}{\\revise{93.01}} & \\cellcolor{gray!20}{\\revise{98.78}} & \\cellcolor{gray!20}{\\revise{98.78}} & \\cellcolor{gray!20}{\\revise{86.78}} & \\cellcolor{gray!20}{\\revise{86.78}} & \\cellcolor{gray!20}{\\revise{95.46}} & \\cellcolor{gray!20}{\\revise{95.46}} & \\cellcolor{gray!20}{\\revise{93.51}} & \\cellcolor{gray!20}{\\revise{93.51}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{\\textbf{93.03}}} & \\cellcolor{gray!20}{\\revise{\\textbf{93.03}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.18}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.18}}} & \\cellcolor{gray!20}{\\revise{\\textbf{88.84}}} & \\cellcolor{gray!20}{\\revise{\\textbf{88.84}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.51}}} & \\cellcolor{gray!20}{\\revise{\\textbf{96.51}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.39}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.39}}} \\\\\n\\midrule\n\\multirow{5}{*}{ViM\\cite{vim}}\n& ViT\\cite{vit} & \\revise{83.51} & \\revise{83.51} & \\revise{77.75} & \\revise{77.75} & \\revise{71.04} & \\revise{71.04} & \\revise{78.31} & \\revise{78.31} & \\revise{77.65} & \\revise{77.65} \\\\\n& MoCov3\\cite{mocov3} & \\revise{76.28} & \\revise{76.28} & \\revise{78.18} & \\revise{78.18} & \\revise{61.35} & \\revise{61.35} & \\revise{72.46} & \\revise{72.46} & \\revise{72.07} & \\revise{72.07} \\\\\n& DINOv2\\cite{dinov2} & \\revise{66.90} & \\revise{66.90} & \\revise{62.53} & \\revise{62.53} & \\revise{58.93} & \\revise{58.93} & \\revise{56.93} & \\revise{56.93} & \\revise{61.32} & \\revise{61.32} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.35}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.35}}} & \\cellcolor{gray!20}{\\revise{99.31}} & \\cellcolor{gray!20}{\\revise{99.31}} & \\cellcolor{gray!20}{\\revise{90.06}} & \\cellcolor{gray!20}{\\revise{90.06}} & \\cellcolor{gray!20}{\\revise{96.96}} & \\cellcolor{gray!20}{\\revise{96.96}} & \\cellcolor{gray!20}{\\revise{95.42}} & \\cellcolor{gray!20}{\\revise{95.42}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{94.25}} & \\cellcolor{gray!20}{\\revise{94.25}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.59}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.59}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.47}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.47}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.41}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.41}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.68}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.68}}} \\\\\n\\midrule\n\\multirow{5}{*}{Average}\n& ViT\\cite{vit} & \\revise{73.52} & \\revise{73.52} & \\revise{84.05} & \\revise{84.05} & \\revise{65.66} & \\revise{65.66} & \\revise{80.02} & \\revise{80.02} & \\revise{75.81} & \\revise{75.81} \\\\\n& MoCov3\\cite{mocov3} & \\revise{68.45} & \\revise{68.45} & \\revise{82.79} & \\revise{82.79} & \\revise{64.69} & \\revise{64.69} & \\revise{77.83} & \\revise{77.83} & \\revise{73.44} & \\revise{73.44} \\\\\n& DINOv2\\cite{dinov2} & \\revise{66.91} & \\revise{66.91} & \\revise{66.40} & \\revise{66.40} & \\revise{55.40} & \\revise{55.40} & \\revise{57.64} & \\revise{57.64} & \\revise{61.59} & \\revise{61.59} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.18}}} & \\cellcolor{gray!20}{\\revise{\\textbf{89.18}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.53}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.53}}} & \\cellcolor{gray!20}{\\revise{85.38}} & \\cellcolor{gray!20}{\\revise{85.38}} & \\cellcolor{gray!20}{\\revise{94.42}} & \\cellcolor{gray!20}{\\revise{94.42}} & \\cellcolor{gray!20}{\\revise{91.63}} & \\cellcolor{gray!20}{\\revise{91.63}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{88.24}} & \\cellcolor{gray!20}{\\revise{88.24}} & \\cellcolor{gray!20}{\\revise{97.32}} & \\cellcolor{gray!20}{\\revise{97.32}} & \\cellcolor{gray!20}{\\revise{\\textbf{87.02}}} & \\cellcolor{gray!20}{\\revise{\\textbf{87.02}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.53}}} & \\cellcolor{gray!20}{\\revise{\\textbf{94.53}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.77}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.77}}} \\\\\n\\midrule\n\\multirow{5}{*}{Best}\n& ViT\\cite{vit} & \\revise{84.93} & \\revise{84.93} & \\revise{91.20} & \\revise{91.20} & \\revise{71.53} & \\revise{71.53} & \\revise{84.34} & \\revise{84.34} & \\revise{81.38} & \\revise{81.38} \\\\\n& MoCov3\\cite{mocov3} & \\revise{84.29} & \\revise{84.29} & \\revise{90.68} & \\revise{90.68} & \\revise{70.33} & \\revise{70.33} & \\revise{85.42} & \\revise{85.42} & \\revise{81.28} & \\revise{81.28} \\\\\n& DINOv2\\cite{dinov2} & \\revise{80.51} & \\revise{80.51} & \\revise{80.34} & \\revise{80.34} & \\revise{69.77} & \\revise{69.77} & \\revise{63.17} & \\revise{63.17} & \\revise{67.60} & \\revise{67.60} \\\\\n& \\cellcolor{gray!20}{BEiTv2\\cite{beitv2}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.35}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.35}}} & \\cellcolor{gray!20}{\\revise{99.31}} & \\cellcolor{gray!20}{\\revise{99.31}} & \\cellcolor{gray!20}{\\revise{90.06}} & \\cellcolor{gray!20}{\\revise{90.06}} & \\cellcolor{gray!20}{\\revise{96.96}} & \\cellcolor{gray!20}{\\revise{96.96}} & \\cellcolor{gray!20}{\\revise{95.42}} & \\cellcolor{gray!20}{\\revise{95.42}} \\\\\n& \\cellcolor{gray!20}{BEiT\\cite{beit}} & \\cellcolor{gray!20}{\\revise{94.25}} & \\cellcolor{gray!20}{\\revise{94.25}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.59}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.59}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.47}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.47}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.41}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.41}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.68}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.68}}} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\n\\revise{Performance of OOD detection methods on ViT-B/16 model with $224\\times224$-pixel inputs. The pre-text tasks include classification task \\cite{vit}, contrastive learning tasks MoCov3 \\cite{mocov3} and DINOv2 \\cite{dinov2}, and masked image modeling tasks BEiT \\cite{beit} and BEiTv2 \\cite{beitv2}. All models are per-trained on ImageNet-21k and finetuned on ImageNet-1k. Both metrics AUROC and FPR95 are in percentage.\nThe best method is emphasized in bold and a gray background indicates our choice.}\n}\n\\label{tab:multi-class-imagenet-ablation}\n\\end{table*}\n\n\n\n\\revise{In this section, we offer a comprehensive analysis of these key elements in the context of OOD detection. We employ ImageNet \\cite{imagenet} as the in-distribution dataset and evaluate pre-task texts on challenging unnatural out-of-distribution datasets, including OpenImage-O \\cite{openimages_o}, Texture \\cite{dtd}, iNaturalist \\cite{inaturalist}, and ImageNet-O \\cite{imagenet_o}. Extensive validations with various pretraining methods and OOD score functions, \\re{including {MSP} \\cite{baseline_ood}, {Energy} \\cite{energy}, {MaxLogit} \\cite{maxlogit}, {KL Matching} \\cite{maxlogit}, {Residual} \\cite{vim}, {ReAct} \\cite{react}, {Mahalanobis} \\cite{mahalanobis} and {ViM} \\cite{vim}. }}\n\n\\revise{Results are shown in ~\\cref{tab:multi-class-imagenet-ablation}. The results indicate that the masked image modeling pretext task surpasses classification and contrastive learning pretext tasks when employing all included score functions. The average AUROC across these score functions exhibits an improvement of 15.96\\%  compared to the competition. Models when using the best-performing score function saw a 14.30\\% increase in performance. This remarkable achievement can be attributed to the representative ID feature space representation, thereby aiding in distinguishing between ID and OOD data. This discovery is highly significant as it enhances performance across mainstream OOD detection score functions, thus advancing the entire field. We also employ CIFAR-10 \\cite{cifar} as the ID dataset and provide results in the appendix. Our approach attains an impressive AUROC of 99.99\\% while concurrently reducing the FPR95 to a mere 0.03\\%.}\n\n\\revise{To enhance the comprehensibility of our experimental findings, we conduct a thorough statistical analysis and illustrate them in visual representations. The outcomes are depicted in \\cref{fig:detailed_ablation}. Our approach not only leads to an overall enhancement in results but also notably minimizes the variations among different methods. For instance, the ViT, MoCov3, and DINOv2 models using logit-based methods exhibited standard deviations of 4.76\\%, 5.04\\%, and 4.51\\%, respectively, while BEiT and BEiTv2 displayed significantly lower standard deviations, reaching as low as 0.13\\% and 0.01\\%. This observation underscores that even uncomplicated score functions can perform equivalently to more intricate ones when an effective ID feature representation is applied.}\n\n\\revise{In \\cref{tab:ablation-statistic}, we underscore the optimal methods for each model. On CIFAR-10, all models achieved their best results when employing the feat and logit combination approach, achieving almost 100\\% accuracy. This suggests a highly effective grasp of CIFAR-10's feature space. Conversely, with the larger ImageNet dataset, we observed variations in outcomes. Notably, the masked image modeling pretext-pretrained model achieved the best results when using the feat and logit combination method, while other models excelled in probability-based and feature-based methods. Additionally, our masked image modeling pretext demonstrated significantly superior performance compared to other pretraining methods, underscoring the limitations of classification-based pretraining strategies and their inadequacy in harnessing advanced score functions effectively. These discoveries reinforce the pivotal role of proficient feature representation in OOD detection. Furthermore, for more detailed information, we provide illustrations of the distribution curves of OOD scores for both ID and OOD datasets in the appendix.}\n\n\\subsection{Masked Image Modeling for Out-of-Distribution v2} \n\\label{sec:method}\n\\revise{To sum up, in this section, we observed that pre-trained models adeptly reconstruct ID images, yet manifest distinctive domain differences in the OOD scenario (\\cref{fig:recover}). This visual incongruity starkly highlights the prevailing domain gap in model features between ID and OOD data. Additionally, a thorough analysis of experimental outcomes reveals that the pre-task of masked image modeling not only significantly enhances overall results but also markedly diminishes disparities among score functions. These findings emphasize the crucial significance of effective feature representation in OOD detection, highlighting the enhancement of features through masked image modeling tasks.}\n\n\\revise{Finally, we propose our Masked Image Modeling for Out-of-Distribution Detection v2 (MOODv2). The algorithm of is shown in \\cref{alg1}, mainly including the following stages.}\n\n\\begin{enumerate}\n\\item \\revise{Pre-train the vision encoder with masked image modeling on the pretrain dataset. }\n\\item \\revise{Apply fine-tuning the backbone on the in-distribution dataset.}\n\\item \\revise{Extract features from the trained image encoder and calculate the OOD score distance for OOD detection.}\n\\end{enumerate}\n\n\\revise{In terms of the OOD score function, we adopt  ViM\\cite{vim} that combines features and logits, leveraging insights from the masked image modeling pre-trained model, which has demonstrated superior performance. Mathematically, the score is\n\\begin{equation}\n    \\text{s}(x) = \\frac{e^{\\alpha\\sqrt{x^T R R^Tx}}}{\\sum_{i=1}^C e^{l_i} + e^{\\alpha\\sqrt{x^TRR^Tx}}}.\n\\end{equation}\nwhere $l_i$ is the $i$-th logit of feature $x$ in the training set $X$; $\\alpha$ is a per-model constant; $R\\in\\mathbb{R}^{N\\times(N-D)}$ is the $(D+1)$-th column to the last column of the eigenvector matrix $Q$ of $X$ and $N$ is the principal dimension; $C$ is the number of classes.}\n\n\n%--------------------------------------algorithm-----------------------------------\n\\begin{algorithm}[t!]\n\\caption{MOODv2 Detection Algorithm}\n\\label{alg1}\n\\small\n\\begin{algorithmic}[1]\n\n\\Require Pre-train set $X_P$, in-distribution set $X_{\\rm ID}$, test set $X_{\\rm test}$, , required True Positive Rate $\\eta$\\%, backbone $f$.\n\\Ensure Is $x_{\\rm test}$ outlier or not? $\\forall x_{\\rm test} \\in X_{\\rm test}$.\n\n\\State Pre-train $f$ on $X_P$ by maximizing\n$$\\sum_{x\\in X_P}\\mathbb{E}_M\\left [\\sum_{i\\in M}\\log p_{\\rm MIM}(z|x^M)\\right ]$$.\n\\State Fine-tune $f$ on $X_P$ by minimizing\n$$L_{\\rm ft}=\\sum_{x_p\\in X_P}{\\rm CrossEntropy}(f(x_p), y_P(x_p))$$ \n\\State Calculate $d(x_{\\rm test})$ for $x_{\\rm test} \\in X_{\\rm test}$ and \\re{$d(x_{\\rm ID})$ for $x_{\\rm ID}\\in X_{\\rm ID}$.}\n\\State Compute threshold $T$ as the $\\eta$ percentile of \\re{$d(x_{\\rm ID})$.}\n\\If {$d(x_{\\rm test})>T$}\n\\State $x_{\\rm test}$ is an outlier.\n\\EndIf\n\\end{algorithmic}\n\\end{algorithm}\n\n%--------------------------------------multi-class-----------------------------------\n\\begin{table*}[t!]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{0.8mm}\n\\begin{tabular}{c|c|ccccccccccc}\n\\toprule\n\\multirow{2}{*}{ID data} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c}{Texture \\cite{dtd}} & \\multicolumn{2}{c}{iNaturalist \\cite{inaturalist}} & \\multicolumn{2}{c}{ImageNet-O \\cite{imagenet_o}} & \\multicolumn{2}{c}{ OpenImage-O \\cite{openimages_o}} & \\multicolumn{2}{c}{Average} \\\\\n& & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ & AUROC$\\uparrow$ & FPR95$\\downarrow$ \\\\\n\\midrule\n\\multirow{10}{*}{CIFAR-10}\n& MSP\\cite{baseline_ood} & \\revise{45.67} & \\revise{95.17} & \\revise{71.07} & \\revise{81.76} & \\revise{32.52} & \\revise{98.85} & \\revise{59.74} & \\revise{91.45} & \\revise{52.25} & \\revise{91.81} \\\\\n& Energy\\cite{energy} & \\revise{31.16} & \\revise{97.89} & \\revise{48.95} & \\revise{97.92} & \\revise{37.22} & \\revise{97.85} & \\revise{45.29} & \\revise{96.36} & \\revise{40.65} & \\revise{97.50} \\\\\n& MaxLogit\\cite{maxlogit} & \\revise{41.21} & \\revise{95.95} & \\revise{67.83} & \\revise{86.04} & \\revise{32.58} & \\revise{98.80} & \\revise{56.64} & \\revise{92.94} & \\revise{49.56} & \\revise{93.43} \\\\\n& KL-Matching\\cite{maxlogit} & \\revise{98.00} & \\revise{10.64} & \\revise{94.23} & \\revise{35.86} & \\revise{92.99} & \\revise{32.40} & \\revise{94.68} & \\revise{27.92} & \\revise{94.97} & \\revise{26.71} \\\\\n& Residual\\cite{vim} & \\revise{99.91} & \\revise{0.21} & \\revise{99.68} & \\revise{0.45} & \\revise{99.36} & \\revise{2.85} & \\revise{99.42} & \\revise{2.46} & \\revise{99.59} & \\revise{1.49} \\\\\n& React\\cite{react} & \\revise{35.97} & \\revise{96.26} & \\revise{69.01} & \\revise{87.91} & \\revise{36.65} & \\revise{97.75} & \\revise{54.14} & \\revise{93.11} & \\revise{48.94} & \\revise{93.76} \\\\\n& Mahalanobis\\cite{mahalanobis} & \\revise{99.77} & \\revise{0.60} & \\revise{99.39} & \\revise{1.11} & \\revise{98.93} & \\revise{4.90} & \\revise{99.14} & \\revise{3.26} & \\revise{99.31} & \\revise{2.47} \\\\\n& ViM\\cite{vim} & \\revise{99.91} & \\revise{0.23} & \\revise{99.72} & \\revise{0.38} & \\revise{99.38} & \\revise{2.65} & \\revise{99.49} & \\revise{2.31} & \\revise{99.63} & \\revise{1.39} \\\\\n& \\cellcolor{gray!20}{MOODv1\\cite{MOOD}} & \\cellcolor{gray!20}{\\revise{99.95}} & \\cellcolor{gray!20}{\\revise{\\textbf{0.06}}} & \\cellcolor{gray!20}{\\revise{99.99}} & \\cellcolor{gray!20}{\\revise{0.02}} & \\cellcolor{gray!20}{\\revise{99.61}} & \\cellcolor{gray!20}{\\revise{1.90}} & \\cellcolor{gray!20}{\\revise{99.82}} & \\cellcolor{gray!20}{\\revise{0.77}} & \\cellcolor{gray!20}{\\revise{99.84}} & \\cellcolor{gray!20}{\\revise{0.69}} \\\\\n& \\cellcolor{gray!20}{MOODv2 (ours)} & \\cellcolor{gray!20}{\\revise{\\textbf{99.98}}} & \\cellcolor{gray!20}{\\revise{\\textbf{0.06}}} & \\cellcolor{gray!20}{\\revise{\\textbf{100.00}}} & \\cellcolor{gray!20}{\\revise{\\textbf{0.00}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.94}}} & \\cellcolor{gray!20}{\\revise{\\textbf{0.20}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.99}}} & \\cellcolor{gray!20}{\\revise{\\textbf{0.01}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.98}}} & \\cellcolor{gray!20}{\\revise{\\textbf{0.07}}} \\\\\n\\midrule\n\\multirow{10}{*}{ImageNet}\n& MSP\\cite{baseline_ood} & \\revise{71.31} & \\revise{77.07} & \\revise{90.70} & \\revise{43.72} & \\revise{60.77} & \\revise{90.60} & \\revise{84.29} & \\revise{61.79} & \\revise{76.77} & \\revise{68.30} \\\\\n& Energy\\cite{energy} & \\revise{54.11} & \\revise{86.28} & \\revise{76.61} & \\revise{72.70} & \\revise{61.63} & \\revise{81.00} & \\revise{71.06} & \\revise{73.99} & \\revise{65.85} & \\revise{78.49} \\\\\n& MaxLogit\\cite{maxlogit} & \\revise{67.22} & \\revise{77.98} & \\revise{89.88} & \\revise{45.57} & \\revise{61.68} & \\revise{88.60} & \\revise{82.73} & \\revise{62.52} & \\revise{75.37} & \\revise{68.67} \\\\\n& KL-Matching\\cite{maxlogit} & \\revise{82.59} & \\revise{67.27} & \\revise{87.63} & \\revise{69.71} & \\revise{66.55} & \\revise{88.15} & \\revise{84.34} & \\revise{74.23} & \\revise{80.28} & \\revise{74.84} \\\\\n& Residual\\cite{vim} & \\revise{82.39} & \\revise{64.61} & \\revise{73.72} & \\revise{86.00} & \\revise{68.44} & \\revise{87.45} & \\revise{74.88} & \\revise{77.98} & \\revise{74.86} & \\revise{79.01} \\\\\n& React\\cite{react} & \\revise{62.09} & \\revise{80.47} & \\revise{91.20} & \\revise{38.74} & \\revise{63.66} & \\revise{81.00} & \\revise{80.43} & \\revise{60.41} & \\revise{74.34} & \\revise{65.15} \\\\\n& Mahalanobis\\cite{mahalanobis} & \\revise{84.93} & \\revise{66.05} & \\revise{84.90} & \\revise{81.60} & \\revise{71.53} & \\revise{88.85} & \\revise{84.16} & \\revise{74.72} & \\revise{81.38} & \\revise{77.80} \\\\\n& ViM\\cite{vim} & \\revise{83.51} & \\revise{62.71} & \\revise{77.75} & \\revise{81.72} & \\revise{71.04} & \\revise{86.60} & \\revise{78.31} & \\revise{74.55} & \\revise{77.65} & \\revise{76.40} \\\\\n& \\cellcolor{gray!20}{MOODv1\\cite{MOOD}} & \\cellcolor{gray!20}{\\revise{93.01}} & \\cellcolor{gray!20}{\\revise{30.91}} & \\cellcolor{gray!20}{\\revise{98.78}} & \\cellcolor{gray!20}{\\revise{5.89}} & \\cellcolor{gray!20}{\\revise{86.78}} & \\cellcolor{gray!20}{\\revise{63.15}} & \\cellcolor{gray!20}{\\revise{95.46}} & \\cellcolor{gray!20}{\\revise{26.46}} & \\cellcolor{gray!20}{\\revise{93.51}} & \\cellcolor{gray!20}{\\revise{31.60}} \\\\\n& \\cellcolor{gray!20}{MOODv2 (ours)} & \\cellcolor{gray!20}{\\revise{\\textbf{94.25}}} & \\cellcolor{gray!20}{\\revise{\\textbf{24.69}}} & \\cellcolor{gray!20}{\\revise{\\textbf{99.59}}} & \\cellcolor{gray!20}{\\revise{\\textbf{1.83}}} & \\cellcolor{gray!20}{\\revise{\\textbf{91.47}}} & \\cellcolor{gray!20}{\\revise{\\textbf{40.80}}} & \\cellcolor{gray!20}{\\revise{\\textbf{97.41}}} & \\cellcolor{gray!20}{\\revise{\\textbf{13.55}}} & \\cellcolor{gray!20}{\\revise{\\textbf{95.68}}} & \\cellcolor{gray!20}{\\revise{\\textbf{20.22}}} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\n\\revise{Performance of OOD detection methods on ViT-B/16 model with $224\\times224$-pixel inputs. All methods are pre-trained on ImageNet-21k and finetuned on ImageNet-1k. ID datasets include CIFAR-10 \\cite{cifar} and ImageNet-1k \\cite{imagenet}. Both metrics AUROC and FPR95 are in percentage. The best method is emphasized in bold and a gray background indicates our methods.}\n}\n\\label{tab:multi-class}\n\\end{table*}\n\n\\begin{table*}[t]\n\\small\n\\centering\n\n\\subfloat[AUROC]{\n\\setlength{\\tabcolsep}{2.8mm}\n\\begin{tabular}{c|cccccccccc|c}\n\\toprule\n\\multirow{2}{*}{Methods} & \\multicolumn{10}{c|}{ID class} & \\multirow{2}{*}{Average} \\\\\n% & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & \\\\\n& Plane & Car & Bird & Cat & Deer & Dog & Frog & Horse & Ship & Truck & \\\\\n\\midrule\nKL-Matching\\cite{maxlogit} & \\revise{95.35} & \\revise{92.04} & \\revise{95.18} & \\revise{91.26} & \\revise{88.11} & \\revise{94.66} & \\revise{94.99} & \\revise{86.52} & \\revise{93.61} & \\revise{89.37} & \\revise{92.11} \\\\\nResidual\\cite{vim} & \\revise{97.62} & \\revise{95.88} & \\revise{97.06} & \\revise{96.30} & \\revise{89.18} & \\revise{94.33} & \\revise{96.73} & \\revise{91.46} & \\revise{94.89} & \\revise{92.36} & \\revise{94.58} \\\\\nMahalanobis\\cite{mahalanobis} & \\revise{97.52} & \\revise{96.07} & \\revise{96.77} & \\revise{96.41} & \\revise{89.60} & \\revise{94.79} & \\revise{96.41} & \\revise{91.48} & \\revise{94.80} & \\revise{92.58} & \\revise{94.64} \\\\\nViM\\cite{vim} & \\revise{97.61} & \\revise{96.36} & \\revise{97.19} & \\revise{96.50} & \\revise{88.78} & \\revise{94.21} & \\revise{96.70} & \\revise{91.60} & \\revise{94.97} & \\revise{92.35} & \\revise{94.63} \\\\\n\\rowcolor{gray!20}MOODv1\\cite{MOOD} & \\revise{98.63} & \\revise{\\textbf{99.33}} & \\revise{94.31} & \\revise{93.22} & \\revise{\\textbf{98.11}} & \\revise{96.50} & \\revise{\\textbf{99.25}} & \\revise{\\textbf{98.96}} & \\revise{\\textbf{98.76}} & \\revise{\\textbf{97.82}} & \\revise{97.83} \\\\\n\\rowcolor{gray!20}MOODv2 (ours) & \\revise{\\textbf{99.14}} & \\revise{99.03} & \\revise{\\textbf{99.51}} & \\revise{\\textbf{98.37}} & \\revise{97.12} & \\revise{\\textbf{97.20}} & \\revise{98.53} & \\revise{98.07} & \\revise{98.35} & \\revise{96.68} & \\revise{\\textbf{98.20}} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\subfloat[FPR95]{\n\\setlength{\\tabcolsep}{2.8mm}\n\\begin{tabular}{c|cccccccccc|c}\n\\toprule\n\\multirow{2}{*}{Methods} & \\multicolumn{10}{c|}{ID class} & \\multirow{2}{*}{Average} \\\\\n% & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & \\\\\n& Plane & Car & Bird & Cat & Deer & Dog & Frog & Horse & Ship & Truck & \\\\\n\\midrule\nKL-Matching\\cite{maxlogit} & \\revise{23.60} & \\revise{32.60} & \\revise{22.32} & \\revise{42.92} & \\revise{46.26} & \\revise{24.30} & \\revise{24.97} & \\revise{46.74} & \\revise{25.32} & \\revise{40.53} & \\revise{32.96} \\\\\nResidual\\cite{vim} & \\revise{12.06} & \\revise{25.58} & \\revise{16.71} & \\revise{21.17} & \\revise{48.33} & \\revise{22.12} & \\revise{17.42} & \\revise{36.72} & \\revise{17.30} & \\revise{30.76} & \\revise{24.82} \\\\\nMahalanobis\\cite{mahalanobis} & \\revise{12.59} & \\revise{25.72} & \\revise{18.92} & \\revise{21.48} & \\revise{48.44} & \\revise{20.59} & \\revise{19.20} & \\revise{38.02} & \\revise{17.47} & \\revise{30.93} & \\revise{25.34} \\\\\nViM\\cite{vim} & \\revise{12.43} & \\revise{24.83} & \\revise{15.77} & \\revise{20.13} & \\revise{48.68} & \\revise{21.77} & \\revise{17.63} & \\revise{36.63} & \\revise{17.60} & \\revise{30.78} & \\revise{24.63} \\\\\n\\rowcolor{gray!20}MOODv1\\cite{MOOD} & \\revise{7.59} & \\revise{5.04} & \\revise{2.47} & \\revise{\\textbf{7.49}} & \\revise{15.63} & \\revise{\\textbf{10.96}} & \\revise{11.37} & \\revise{13.09} & \\revise{10.06} & \\revise{19.62} & \\revise{10.33} \\\\\n\\rowcolor{gray!20}MOODv2 (ours) & \\revise{\\textbf{4.82}} & \\revise{\\textbf{4.50}} & \\revise{\\textbf{1.79}} & \\revise{8.80} & \\revise{\\textbf{15.59}} & \\revise{11.00} & \\revise{\\textbf{8.46}} & \\revise{\\textbf{12.43}} & \\revise{\\textbf{8.60}} & \\revise{\\textbf{18.96}} & \\revise{\\textbf{9.49}} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\caption{\n\\revise{Performance of OOD detection methods on ViT-B/16 model with $224\\times224$-pixel inputs. All methods are pre-trained on ImageNet-21k and finetuned on ImageNet-1k. We perform each category of CIFAR-10 \\cite{cifar} as the ID dataset and other classes as OOD datasets. We report the average results across OOD classes of each ID class. Both metrics AUROC and FPR95 are in percentage. The best method is emphasized in bold and a gray background indicates our methods.}\n} % The detailed class-wize performance is in the Appendix.\n\\label{tab:one-class}\n\\end{table*}\n\n\n\n\\section{Experiments}\\label{sec:exp}\n\\label{sec:exp}\n\\revise{In this section, we conduct a thorough comparison of our algorithm with the latest OOD detection methods. We employ the ViT-B/16 model, pre-trained on ImageNet-21K \\re{with corresponding pretext tasks from different methods} and fine-tuned on ImageNet-1K at a resolution of $224\\times224$. }\n\n\\vspace{2mm}\\noindent\\revise{\\textbf{ID/OOD Datasets.} We select CIFAR-10 \\cite{cifar} and ImageNet-1K~\\cite{imagenet} as the ID datasets. Following established procedures~\\cite{vim}, for estimating the principal space of ImageNet, we randomly sample $200,000$ images from the training set. Our experiments include the following OOD datasets:\n\\begin{enumerate}\n    \\item OpenImage-O is a newly collected large-scale OOD dataset~\\cite{openimages_o}.\n    \\item Texture~\\cite{cimpoi14describing} comprises natural textural images, with four overlapping categories (\\emph{bubbly, honeycombed, cobwebbed, spiraled}) removed since they coincide with ImageNet.\n    \\item iNaturalist~\\cite{van2018inaturalist} is a fine-grained species classification dataset, and we use a specific subset from previous works ~\\cite{huang2021mos}.\n    \\item ImageNet-O~\\cite{hendrycks2021natural} contains images that are adversarially filtered to challenge OOD detectors. \n\\end{enumerate} \n}\n\n\\re{Although these OOD datasets are specifically tailored to ensure that they do not belong to any category in ImageNet, rather than being customized for CIFAR-10, each category in CIFAR-10 has a similar counterpart in ImageNet, as referenced in the appendix. Consequently, we use the same OOD datasets for CIFAR-10 as well.}\n\n\n\\vspace{2mm}\\noindent\\revise{\\textbf{Evaluation Metrics.} \nWe report two commonly used evaluation metrics AUROC and FPR95. The AUROC is a threshold-free metric, indicating the area under the receiver operating characteristic curve, with a higher value denoting better detection performance. FPR95, or FPR at TPR95, stands for the false positive rate when the true positive rate is 95\\%, and a smaller FPR95 is preferable. Both metrics are expressed as percentages.}\n\n\\vspace{2mm}\\noindent\\revise{\\textbf{Baseline Methods.} \nFollowing previous works \\cite{vim}, we compare MOODv2 with the baseline algorithms that do not require fine-tuning including {MSP} \\cite{baseline_ood}, {Energy} \\cite{energy}, {MaxLogit} \\cite{maxlogit}, {KL Matching} \\cite{maxlogit}, {Residual}, {ReAct} \\cite{react}, and {Mahalanobis} \\cite{mahalanobis}. }\n\n%--------------------------------------visualization----------------\n\\begin{figure*}[tp]\n  \\centering\n    \\includegraphics[width=\\textwidth]{figures/distribution_method.pdf}\n    \\caption{\\revise{The distribution curves of OOD score functions for ID and OOD datasets obtained using various mainstream methods, including {MSP} \\cite{baseline_ood}, {Energy} \\cite{energy}, {MaxLogit} \\cite{maxlogit}, {KL Matching} \\cite{maxlogit}, {Residual} \\cite{vim}, {ReAct} \\cite{react}, {Mahalanobis} \\cite{mahalanobis} and {ViM} \\cite{vim}. The red line indicates the ID dataset ImageNet \\cite{imagenet}; the blue line indicates Texture \\cite{dtd}; the green line indicates iNaturalist \\cite{inaturalist}; the purple line indicates ImageNet-O \\cite{imagenet_o}; the orange line indicates OpenImage-O \\cite{openimages_o}}}\n  \\label{fig:distribution}\n\\end{figure*}\n\n\\subsection{One-Class OOD Detection}\n\\label{sec:1class}\nWe start with the one-class OOD detection. For a given multi-class dataset of $N_c$ classes, we conduct $N_c$ one-class OOD tasks, where each task regards one of the classes as in-distribution and the remaining classes as out-of-distribution. We run our experiments on CIFAR-10 \\cite{cifar}. \\Cref{tab:one-class} summarizes the average results across OOD classes of each ID class and the detailed \\re{class-wise} performance is in the appendix.\n\n\\revise{It's worth noting that all methods were pre-trained on ImageNet-21k and fine-tuned on ImageNet-1k, which may have had some influence on the results to varying degrees. Nevertheless, we ensure consistent training strategies for all methods to ensure a fair comparison. \\re{Experimental results have demonstrated that MOODv2 achieves significant improvements across all ID classes.} Notably, we achieved a remarkable 3.56\\% increase in the AUROC, reaching 98.20\\%, while simultaneously reducing the FPR95 by 15.14\\% to achieve an impressive 9.49\\%.}\n\n\\subsection{Multi-Class OOD Detection} \n\\label{sec:multi-class}\n\\revise{For multi-class OOD Detection, we assume that ID samples are from a multi-class dataset, either CIFAR-10 \\cite{cifar} or ImageNet \\cite{imagenet}. They are tested on external datasets as out-of-distribution, including OpenImage-O \\cite{openimages_o}, Texture \\cite{cimpoi14describing}, iNaturalist \\cite{van2018inaturalist} and ImageNet-O \\cite{hendrycks2021natural}.}\n\n\\revise{Results are shown in \\cref{tab:multi-class}. MOODv2 delivers outstanding results on CIFAR-10, achieving an impressive AUROC of 99.98\\% (0.35\\% enhancement) and the FPR95 reaches an astonishingly low rate of 0.07\\%, marking a substantial 95\\% reduction compared to the prior SOTA (1.39\\%). On ImageNet, MOODv2 also exhibited significant improvements, showcasing a remarkable 14.30\\% increase in AUROC, resulting in 95.68\\%. Additionally, the FPR95 saw a substantial reduction of 44.93\\%, reaching 20.22\\%.}\n\n\\revise{In \\cref{fig:distribution}, we illustrate the distribution curves of OOD scores for ID and OOD datasets using various mainstream methods. A smaller overlap between ID and OOD data indicates superior OOD detection performance, while a larger overlap signifies weaker detection results. The ID curve (in red) for MOODv2 features a distinct peak at a higher position, resulting in minimal overlap with other OOD data, indicating a notable OOD detection capability. This success can be attributed to the high-quality ID feature representation.}\n\n\\section{Conclusion}\\label{sec:conclusion}\n\\revise{In our work, we focus on the critical aspect of effective out-of-distribution (OOD) detection, which involves acquiring a robust in-distribution (ID) representation that distinguishes it from OOD samples. We conduct comprehensive experiments with distinct pretraining tasks and employ various OOD score functions. The findings indicate that feature representations pre-trained through reconstruction significantly enhance performance and reduce the performance gap among different score functions. This implies that even simple score functions can perform as well as complex ones when utilizing reconstruction-based pretext tasks. These findings hold promise for further development in OOD detection. Ultimately, we introduce the MOODv2 OOD detection framework, employing the masked image modeling pretext task, which achieves a remarkable 14.30\\% increase in AUROC, reaching 95.68\\% on ImageNet, and substantially improving CIFAR-10 to 99.98\\%.}\n. The second paper ends.", "input": "Please compare the papers and summarize the improvement of the second paper from the first paper.", "answer": ""}
