model:
  type: qllm
  path: TinyLlama/TinyLlama-1.1B-Chat-v1.0
  block_size: 128
  n_init: 64
  n_local: 512
  topk: 4
  repr_topk: 2
  max_cached_block: 16
  exc_block_size: 256
  fattn: false
  base: 1000000
  distance_scale: 1.0
  question_weight: 1.0

max_len: 100000
chunk_size: 4096
conv_type: mistral-inst
